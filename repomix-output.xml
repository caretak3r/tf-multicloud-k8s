This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
docs/
  ECS_EC2_SUPPORT.md
environments/
  dev-aws.tfvars
  dev-azure.tfvars
  perf-aws.tfvars
  perf-azure.tfvars
  prod-aws.tfvars
  prod-azure.tfvars
examples/
  aws-ecs-dual-ecr-services.tfvars
  aws-ecs-dual-service-concept.tfvars
  aws-ecs-full-stack.tfvars
  aws-ecs-lightweight-service.tfvars
  aws-ecs-multi-service-setup.md
  aws-ecs-only.tfvars
  aws-ecs-secure-deployment.tfvars
  aws-eks-only.tfvars
  aws-full-stack.tfvars
  aws-vpc-only.tfvars
  create-keypair.sh
  ecs-ec2.tfvars
  ecs-fargate.tfvars
  ecs-http-secrets-example.tfvars
  ECS-HTTP-SECRETS-README.md
  generate-certs.sh
  README.md
  sandbox-ecs-existing-vpc.tfvars
  SECURE-DEPLOYMENT-README.md
  upload-secrets.sh
modules/
  aws/
    bastion/
      main.tf
      outputs.tf
      user_data.sh
      variables.tf
    eks/
      main.tf
      outputs.tf
      variables.tf
    vpc/
      main.tf
      outputs.tf
      variables.tf
    main.tf
    outputs.tf
    README.md
    variables.tf
  azure/
    aks/
      main.tf
      outputs.tf
      variables.tf
    vpc/
      main.tf
      outputs.tf
      variables.tf
    main.tf
    outputs.tf
    variables.tf
  gcp/
    gke/
      main.tf
      outputs.tf
      variables.tf
    provider.tf
scripts/
  generate-certificates.sh
  setup-secrets.sh
.gitignore
main.tf
README.md
size-mapping.md
terraform.tfvars.example
variables.tf
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(mkdir:*)",
      "Bash(terraform:*)",
      "Bash(chmod:*)",
      "Bash(find:*)",
      "Bash(sed:*)"
    ],
    "deny": []
  }
}
</file>

<file path="docs/ECS_EC2_SUPPORT.md">
# ECS EC2 Support Documentation

This document describes the EC2 support for ECS in the multicloud-tf module.

## Overview

The ECS module now supports both Fargate and EC2 launch types. You can choose between them using the `ecs_launch_type` variable.

## Launch Types

### Fargate (Default)
- Serverless container hosting
- No EC2 instance management required
- Pay for vCPU and memory resources used by your tasks
- Suitable for workloads that don't require persistent storage or specific instance types

### EC2
- Container hosting on EC2 instances that you manage
- Full control over the underlying infrastructure
- More cost-effective for consistently running workloads
- Supports instance storage, custom AMIs, and specific instance types
- Requires managing the underlying EC2 infrastructure

## Configuration Variables

### Core ECS Variables
- `ecs_launch_type`: Choose between "FARGATE" (default) or "EC2"
- `ecs_container_image`: Docker image for your application
- `ecs_instance_type`: EC2 instance type (EC2 only, default: "t3.medium")
- `ecs_key_name`: EC2 key pair for SSH access (EC2 only, optional)

### EC2-Specific Variables
- `ecs_min_size`: Minimum number of EC2 instances (default: 1)
- `ecs_max_size`: Maximum number of EC2 instances (default: 3)
- `ecs_desired_capacity`: Desired number of EC2 instances (default: 2)
- `ecs_ec2_spot_price`: Maximum price for spot instances (optional)
- `ecs_ebs_volume_size`: EBS volume size in GB (default: 30)
- `ecs_ebs_volume_type`: EBS volume type (default: "gp3")
- `ecs_enable_container_insights`: Enable CloudWatch Container Insights (default: true)

## Features

### EC2 Launch Type Features
1. **Auto Scaling Group**: Automatically manages EC2 instance capacity
2. **Capacity Provider**: ECS-managed scaling based on task requirements
3. **Dynamic Port Mapping**: Allows multiple tasks on the same instance
4. **ECS-Optimized AMI**: Uses the latest Amazon ECS-optimized AMI
5. **Security Groups**: Separate security groups for EC2 instances and tasks
6. **IAM Roles**: Proper IAM roles for EC2 instances and ECS tasks
7. **CloudWatch Monitoring**: Instance and container-level monitoring
8. **Encrypted EBS Volumes**: All EBS volumes are encrypted by default

### Network Configuration
- **Fargate**: Uses `awsvpc` network mode with ENI per task
- **EC2**: Uses `bridge` network mode with dynamic port mapping

### Security
- **Fargate**: Security groups applied directly to ENIs
- **EC2**: Security groups applied to EC2 instances with port range 32768-65535 for ALB access
- **SSH Access**: Optional SSH access to EC2 instances when key pair is provided

## Example Configurations

### Fargate Configuration
```hcl
cloud_provider = "aws"
enable_ecs = true
ecs_launch_type = "FARGATE"
ecs_container_image = "nginx:latest"
```

### EC2 Configuration
```hcl
cloud_provider = "aws"
enable_ecs = true
ecs_launch_type = "EC2"
ecs_container_image = "nginx:latest"
ecs_instance_type = "t3.medium"
ecs_key_name = "my-keypair"  # Optional for SSH access
```

## Migration from Fargate to EC2

1. Update your `terraform.tfvars` or variable configuration:
   - Change `ecs_launch_type` from "FARGATE" to "EC2"
   - Add EC2-specific variables as needed
   - Optionally specify `ecs_key_name` for SSH access

2. Plan and apply the changes:
   ```bash
   terraform plan
   terraform apply
   ```

3. The migration will:
   - Create EC2 Auto Scaling Group and Launch Template
   - Create ECS Capacity Provider for EC2 instances
   - Update ECS Cluster capacity providers
   - Update ECS Service configuration
   - Create additional security groups for EC2 instances

## Cost Considerations

- **Fargate**: Pay-per-use pricing based on vCPU and memory
- **EC2**: Pay for EC2 instances whether tasks are running or not
- **EC2 with Spot**: Use spot pricing for significant cost savings (up to 70% off)

For consistently running workloads, EC2 is typically more cost-effective, especially with Reserved Instances or Spot pricing.

## Monitoring and Troubleshooting

### CloudWatch Logs
Both launch types use the same CloudWatch log groups:
- `/ecs/{cluster_name}` for ECS task logs

### CloudWatch Metrics
- Container Insights provides detailed metrics for both launch types
- EC2 launch type also provides EC2 instance metrics

### Troubleshooting EC2 Launch Type
1. Check Auto Scaling Group health
2. Verify ECS agent is running on EC2 instances
3. Check security group rules for port ranges
4. Verify IAM roles and policies
5. Check ECS capacity provider settings

## Limitations

### EC2 Launch Type Limitations
- Requires managing EC2 infrastructure
- Less isolation between tasks compared to Fargate
- Network configuration is more complex
- Requires careful security group management

### Fargate Limitations
- Higher per-vCPU/memory costs for consistent workloads
- Less flexibility in compute configuration
- No persistent instance storage

## Best Practices

1. **Use Fargate for**:
   - Development and testing environments
   - Batch jobs and sporadic workloads
   - Microservices with variable load
   - Workloads requiring strong isolation

2. **Use EC2 for**:
   - Production workloads with consistent resource usage
   - Applications requiring specific instance types
   - Workloads needing persistent instance storage
   - Cost-sensitive applications with predictable usage

3. **Security**:
   - Always use encrypted EBS volumes (enabled by default)
   - Limit SSH access with appropriate CIDR blocks
   - Use IAM roles instead of hardcoded credentials
   - Enable VPC Flow Logs for network monitoring

4. **Scaling**:
   - Configure appropriate min/max/desired capacity for EC2 Auto Scaling
   - Monitor ECS capacity provider metrics
   - Use spot instances for non-critical workloads to reduce costs
</file>

<file path="modules/aws/bastion/outputs.tf">
output "bastion_instance_id" {
  description = "ID of the bastion host instance"
  value       = aws_instance.bastion.id
}

output "bastion_public_ip" {
  description = "Public IP address of the bastion host"
  value       = aws_instance.bastion.public_ip
}

output "bastion_private_ip" {
  description = "Private IP address of the bastion host"
  value       = aws_instance.bastion.private_ip
}

output "bastion_security_group_id" {
  description = "Security group ID of the bastion host"
  value       = aws_security_group.bastion.id
}

output "bastion_iam_role_arn" {
  description = "IAM role ARN of the bastion host"
  value       = aws_iam_role.bastion.arn
}

output "ssh_command" {
  description = "SSH command to connect to bastion host"
  value       = "ssh -i ~/.ssh/${var.key_name}.pem ec2-user@${aws_instance.bastion.public_ip}"
}

output "session_manager_command" {
  description = "AWS CLI command to connect via Session Manager"
  value       = "aws ssm start-session --target ${aws_instance.bastion.id}"
}

output "eks_connect_command" {
  description = "Command to run on bastion host to configure kubectl for EKS"
  value       = "./eks-connect.sh"
}
</file>

<file path="modules/aws/bastion/user_data.sh">
#!/bin/bash
yum update -y

# Install required packages
yum install -y awscli curl wget jq git unzip

# Install kubectl
curl -o kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.28.3/2023-11-14/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin
kubectl version --client

# Install helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Install session manager plugin
yum install -y https://s3.amazonaws.com/session-manager-downloads/plugin/latest/linux_64bit/session-manager-plugin.rpm

# Install CloudWatch agent
wget https://s3.amazonaws.com/amazoncloudwatch-agent/amazon_linux/amd64/latest/amazon-cloudwatch-agent.rpm
rpm -U ./amazon-cloudwatch-agent.rpm

# Configure AWS CLI and kubectl for EKS
if [ -n "${cluster_name}" ]; then
    aws eks update-kubeconfig --region ${region} --name ${cluster_name}
fi

# Configure CloudWatch agent
cat > /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json << EOF
{
    "logs": {
        "logs_collected": {
            "files": {
                "collect_list": [
                    {
                        "file_path": "/var/log/messages",
                        "log_group_name": "/aws/ec2/bastion/${cluster_name}",
                        "log_stream_name": "{instance_id}/var/log/messages"
                    },
                    {
                        "file_path": "/var/log/secure",
                        "log_group_name": "/aws/ec2/bastion/${cluster_name}",
                        "log_stream_name": "{instance_id}/var/log/secure"
                    }
                ]
            }
        }
    }
}
EOF

# Start CloudWatch agent
/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json

# Create a helper script for EKS access
cat > /home/ec2-user/eks-connect.sh << 'EOF'
#!/bin/bash
echo "Connecting to EKS cluster: ${cluster_name}"
aws eks update-kubeconfig --region ${region} --name ${cluster_name}
kubectl get nodes
EOF

chmod +x /home/ec2-user/eks-connect.sh
chown ec2-user:ec2-user /home/ec2-user/eks-connect.sh

echo "Bastion host setup completed!"
</file>

<file path="modules/aws/bastion/variables.tf">
variable "name_prefix" {
  description = "Prefix for resource names"
  type        = string
}

variable "vpc_id" {
  description = "VPC ID where bastion host will be created"
  type        = string
}

variable "vpc_cidr_block" {
  description = "CIDR block of the VPC"
  type        = string
}

variable "public_subnet_id" {
  description = "Public subnet ID for bastion host"
  type        = string
}

variable "cluster_name" {
  description = "Name of the EKS cluster for kubectl configuration"
  type        = string
}

variable "region" {
  description = "AWS region"
  type        = string
}

variable "key_name" {
  description = "EC2 Key Pair name for SSH access"
  type        = string
}

variable "instance_type" {
  description = "Instance type for bastion host"
  type        = string
  default     = "t3.micro"
}

variable "ami_id" {
  description = "AMI ID for bastion host (defaults to latest Amazon Linux 2)"
  type        = string
  default     = null
}

variable "root_volume_size" {
  description = "Size of root volume in GB"
  type        = number
  default     = 8
}

variable "allowed_ssh_cidr_blocks" {
  description = "List of CIDR blocks allowed to SSH to bastion host"
  type        = list(string)
  validation {
    condition = length(var.allowed_ssh_cidr_blocks) > 0 && alltrue([
      for cidr in var.allowed_ssh_cidr_blocks : can(cidrhost(cidr, 0))
    ])
    error_message = "At least one valid CIDR block must be provided for SSH access."
  }
}

variable "log_retention_in_days" {
  description = "CloudWatch log retention in days"
  type        = number
  default     = 7
}

variable "tags" {
  description = "Tags to apply to resources"
  type        = map(string)
  default     = {}
}
</file>

<file path="modules/aws/eks/outputs.tf">
output "cluster_id" {
  description = "EKS cluster ID"
  value       = aws_eks_cluster.main.id
}

output "cluster_name" {
  description = "EKS cluster name"
  value       = aws_eks_cluster.main.name
}

output "cluster_arn" {
  description = "EKS cluster ARN"
  value       = aws_eks_cluster.main.arn
}

output "cluster_endpoint" {
  description = "EKS cluster endpoint"
  value       = aws_eks_cluster.main.endpoint
}

output "cluster_version" {
  description = "EKS cluster version"
  value       = aws_eks_cluster.main.version
}

output "cluster_ca_certificate" {
  description = "EKS cluster CA certificate"
  value       = base64decode(aws_eks_cluster.main.certificate_authority[0].data)
  sensitive   = true
}

output "cluster_security_group_id" {
  description = "Security group ID attached to the EKS cluster"
  value       = aws_security_group.cluster.id
}

output "node_security_group_id" {
  description = "Security group ID attached to the EKS worker nodes"
  value       = aws_security_group.nodes.id
}

output "cluster_iam_role_arn" {
  description = "IAM role ARN of the EKS cluster"
  value       = aws_iam_role.cluster.arn
}

output "node_iam_role_arn" {
  description = "IAM role ARN of the EKS worker nodes"
  value       = aws_iam_role.nodes.arn
}

output "node_group_arn" {
  description = "EKS node group ARN"
  value       = aws_eks_node_group.main.arn
}

output "node_group_status" {
  description = "EKS node group status"
  value       = aws_eks_node_group.main.status
}

output "oidc_issuer_url" {
  description = "The URL on the EKS cluster OIDC Issuer"
  value       = aws_eks_cluster.main.identity[0].oidc[0].issuer
}

output "kubeconfig_command" {
  description = "Command to get kubeconfig for EKS (requires bastion or VPN access)"
  value       = "aws eks update-kubeconfig --region ${data.aws_caller_identity.current.account_id} --name ${var.cluster_name}"
}

output "kms_key_arn" {
  description = "ARN of the KMS key used for EKS cluster encryption"
  value       = aws_kms_key.cluster.arn
}
</file>

<file path="modules/azure/aks/main.tf">
locals {
  node_size_map = {
    small = {
      vm_size            = "Standard_D2s_v3"  # 2 vCPU, 8 GB
      node_count         = 2
      min_count          = 1
      max_count          = 5
      os_disk_size_gb    = 50
    }
    medium = {
      vm_size            = "Standard_D4s_v3"  # 4 vCPU, 16 GB
      node_count         = 3
      min_count          = 2
      max_count          = 10
      os_disk_size_gb    = 100
    }
    large = {
      vm_size            = "Standard_D8s_v3"  # 8 vCPU, 32 GB
      node_count         = 5
      min_count          = 3
      max_count          = 20
      os_disk_size_gb    = 100
    }
  }

  node_config = local.node_size_map[var.node_size_config]
}

# Create Key Vault for encryption if not provided
resource "azurerm_key_vault" "aks" {
  count                       = var.key_vault_key_id == null ? 1 : 0
  name                        = "${substr(replace(var.cluster_name, "-", ""), 0, 20)}kv"
  location                    = var.location
  resource_group_name         = var.resource_group_name
  enabled_for_disk_encryption = true
  tenant_id                   = data.azurerm_client_config.current.tenant_id
  soft_delete_retention_days  = 7
  purge_protection_enabled    = true
  sku_name                   = "standard"

  access_policy {
    tenant_id = data.azurerm_client_config.current.tenant_id
    object_id = data.azurerm_client_config.current.object_id

    key_permissions = [
      "Create",
      "Get",
      "List",
      "Delete",
      "Purge",
      "Recover",
      "Update",
      "GetRotationPolicy",
      "SetRotationPolicy"
    ]
  }

  tags = var.tags
}

resource "azurerm_key_vault_key" "aks" {
  count        = var.key_vault_key_id == null ? 1 : 0
  name         = "${var.cluster_name}-key"
  key_vault_id = azurerm_key_vault.aks[0].id
  key_type     = "RSA"
  key_size     = 2048

  key_opts = [
    "decrypt",
    "encrypt",
    "sign",
    "unwrapKey",
    "verify",
    "wrapKey",
  ]

  rotation_policy {
    automatic {
      time_before_expiry = "P30D"
    }
    expire_after         = "P90D"
    notify_before_expiry = "P29D"
  }

  tags = var.tags
}

# Create disk encryption set
resource "azurerm_disk_encryption_set" "aks" {
  name                = "${var.cluster_name}-des"
  resource_group_name = var.resource_group_name
  location            = var.location
  key_vault_key_id    = var.key_vault_key_id != null ? var.key_vault_key_id : azurerm_key_vault_key.aks[0].id

  identity {
    type = "SystemAssigned"
  }

  tags = var.tags
}

# Grant disk encryption set access to key vault
resource "azurerm_key_vault_access_policy" "disk_encryption_set" {
  key_vault_id = var.key_vault_key_id != null ? split("/keys/", var.key_vault_key_id)[0] : azurerm_key_vault.aks[0].id

  tenant_id = azurerm_disk_encryption_set.aks.identity[0].tenant_id
  object_id = azurerm_disk_encryption_set.aks.identity[0].principal_id

  key_permissions = [
    "Get",
    "UnwrapKey",
    "WrapKey"
  ]
}

data "azurerm_client_config" "current" {}

# User assigned identity for AKS
resource "azurerm_user_assigned_identity" "aks" {
  name                = "${var.cluster_name}-identity"
  location            = var.location
  resource_group_name = var.resource_group_name
  tags                = var.tags
}

# AKS Cluster - Private with encryption
resource "azurerm_kubernetes_cluster" "main" {
  name                = var.cluster_name
  location            = var.location
  resource_group_name = var.resource_group_name
  dns_prefix          = var.cluster_name
  kubernetes_version  = var.kubernetes_version

  # Private cluster configuration
  private_cluster_enabled             = true
  private_dns_zone_id                 = var.private_dns_zone_id
  private_cluster_public_fqdn_enabled = false

  # Enable Azure Policy
  azure_policy_enabled = var.enable_azure_policy

  # Enable disk encryption
  disk_encryption_set_id = azurerm_disk_encryption_set.aks.id

  # Network configuration
  network_profile {
    network_plugin     = var.network_plugin
    network_policy     = var.network_policy
    dns_service_ip     = var.dns_service_ip
    service_cidr       = var.service_cidr
    load_balancer_sku  = "standard"
    outbound_type      = "userDefinedRouting"  # For private clusters
  }

  default_node_pool {
    name                         = "system"
    node_count                   = local.node_config.node_count
    vm_size                      = local.node_config.vm_size
    os_disk_size_gb             = local.node_config.os_disk_size_gb
    vnet_subnet_id              = var.subnet_id
    type                        = "VirtualMachineScaleSets"
    enable_auto_scaling         = true
    min_count                   = local.node_config.min_count
    max_count                   = local.node_config.max_count
    enable_host_encryption      = var.enable_host_encryption
    only_critical_addons_enabled = true
    
    upgrade_settings {
      max_surge = "10%"
    }

    node_labels = {
      "nodepool-type"    = "system"
      "environment"      = var.environment
      "nodepoolos"       = "linux"
      "app"              = "system-apps"
    }

    tags = var.tags
  }

  identity {
    type         = "UserAssigned"
    identity_ids = [azurerm_user_assigned_identity.aks.id]
  }

  # Enable Microsoft Defender
  microsoft_defender {
    log_analytics_workspace_id = var.enable_microsoft_defender ? var.log_analytics_workspace_id : null
  }

  # Enable OIDC and workload identity
  oidc_issuer_enabled       = var.enable_workload_identity
  workload_identity_enabled = var.enable_workload_identity

  # Key Management Service for etcd encryption
  key_management_service {
    key_vault_key_id         = var.key_vault_key_id != null ? var.key_vault_key_id : azurerm_key_vault_key.aks[0].id
    key_vault_network_access = "Private"
  }

  # Auto-scaler profile
  auto_scaler_profile {
    balance_similar_node_groups      = true
    expander                          = "random"
    max_graceful_termination_sec     = 600
    max_node_provisioning_time       = "15m"
    max_unready_nodes                = 3
    max_unready_percentage           = 45
    new_pod_scale_up_delay           = "0s"
    scale_down_delay_after_add       = "10m"
    scale_down_delay_after_delete    = "10s"
    scale_down_delay_after_failure   = "3m"
    scan_interval                    = "10s"
    scale_down_unneeded              = "10m"
    scale_down_unready               = "20m"
    scale_down_utilization_threshold = "0.5"
  }

  # Maintenance window
  maintenance_window {
    allowed {
      day   = "Saturday"
      hours = [1, 4]
    }
    allowed {
      day   = "Sunday"  
      hours = [1, 4]
    }
  }

  tags = var.tags
}

# Additional node pool for workloads
resource "azurerm_kubernetes_cluster_node_pool" "workload" {
  name                  = "workload"
  kubernetes_cluster_id = azurerm_kubernetes_cluster.main.id
  vm_size              = local.node_config.vm_size
  node_count           = local.node_config.node_count
  vnet_subnet_id       = var.subnet_id
  
  enable_auto_scaling    = true
  min_count             = local.node_config.min_count
  max_count             = local.node_config.max_count
  enable_host_encryption = var.enable_host_encryption
  os_disk_size_gb       = local.node_config.os_disk_size_gb

  node_labels = {
    "nodepool-type" = "workload"
    "environment"   = var.environment
  }

  node_taints = var.workload_node_taints

  upgrade_settings {
    max_surge = "10%"
  }

  tags = var.tags
}

# Grant AKS identity access to ACR if provided
resource "azurerm_role_assignment" "aks_acr" {
  count                = var.container_registry_id != null ? 1 : 0
  principal_id         = azurerm_user_assigned_identity.aks.principal_id
  role_definition_name = "AcrPull"
  scope                = var.container_registry_id
}

# Network contributor role for AKS identity on subnet
resource "azurerm_role_assignment" "aks_network" {
  principal_id         = azurerm_user_assigned_identity.aks.principal_id
  role_definition_name = "Network Contributor"
  scope                = var.subnet_id
}
</file>

<file path="modules/azure/aks/outputs.tf">
output "cluster_id" {
  description = "The AKS cluster ID"
  value       = azurerm_kubernetes_cluster.main.id
}

output "cluster_name" {
  description = "The AKS cluster name"
  value       = azurerm_kubernetes_cluster.main.name
}

output "cluster_fqdn" {
  description = "The AKS cluster FQDN"
  value       = azurerm_kubernetes_cluster.main.fqdn
}

output "cluster_endpoint" {
  description = "The AKS cluster endpoint"
  value       = azurerm_kubernetes_cluster.main.kube_config[0].host
  sensitive   = true
}

output "kube_config" {
  description = "Kubernetes config for the cluster"
  value       = azurerm_kubernetes_cluster.main.kube_config_raw
  sensitive   = true
}

output "oidc_issuer_url" {
  description = "The OIDC issuer URL for workload identity"
  value       = azurerm_kubernetes_cluster.main.oidc_issuer_url
}

output "identity_principal_id" {
  description = "The principal ID of the managed identity"
  value       = azurerm_user_assigned_identity.aks.principal_id
}

output "identity_client_id" {
  description = "The client ID of the managed identity"
  value       = azurerm_user_assigned_identity.aks.client_id
}

output "disk_encryption_set_id" {
  description = "The disk encryption set ID"
  value       = azurerm_disk_encryption_set.aks.id
}

output "key_vault_key_id" {
  description = "The Key Vault key ID used for encryption"
  value       = var.key_vault_key_id != null ? var.key_vault_key_id : azurerm_key_vault_key.aks[0].id
}
</file>

<file path="modules/azure/aks/variables.tf">
variable "cluster_name" {
  description = "Name of the AKS cluster"
  type        = string
}

variable "resource_group_name" {
  description = "Name of the resource group"
  type        = string
}

variable "location" {
  description = "Azure region"
  type        = string
}

variable "kubernetes_version" {
  description = "Kubernetes version"
  type        = string
  default     = "1.28"
}

variable "node_size_config" {
  description = "Node size configuration (small, medium, large)"
  type        = string
  default     = "small"
  validation {
    condition     = contains(["small", "medium", "large"], var.node_size_config)
    error_message = "Node size config must be one of: small, medium, large."
  }
}

variable "subnet_id" {
  description = "Subnet ID for AKS nodes"
  type        = string
}

variable "private_dns_zone_id" {
  description = "Private DNS Zone ID for private cluster"
  type        = string
  default     = "System"
}

variable "network_plugin" {
  description = "Network plugin for AKS (azure or kubenet)"
  type        = string
  default     = "azure"
}

variable "network_policy" {
  description = "Network policy for AKS"
  type        = string
  default     = "azure"
}

variable "dns_service_ip" {
  description = "DNS service IP address"
  type        = string
  default     = "10.0.0.10"
}

variable "service_cidr" {
  description = "Service CIDR for Kubernetes services"
  type        = string
  default     = "10.0.0.0/16"
}

variable "key_vault_key_id" {
  description = "ID of existing Key Vault key for encryption. If not provided, a new key will be created."
  type        = string
  default     = null
}

variable "enable_host_encryption" {
  description = "Enable host encryption for nodes"
  type        = bool
  default     = true
}

variable "enable_azure_policy" {
  description = "Enable Azure Policy for AKS"
  type        = bool
  default     = true
}

variable "enable_microsoft_defender" {
  description = "Enable Microsoft Defender for Cloud"
  type        = bool
  default     = true
}

variable "log_analytics_workspace_id" {
  description = "Log Analytics workspace ID for monitoring"
  type        = string
  default     = null
}

variable "enable_workload_identity" {
  description = "Enable workload identity"
  type        = bool
  default     = true
}

variable "container_registry_id" {
  description = "Azure Container Registry ID to grant pull access"
  type        = string
  default     = null
}

variable "workload_node_taints" {
  description = "Taints for workload node pool"
  type = list(object({
    key    = string
    value  = string
    effect = string
  }))
  default = []
}

variable "environment" {
  description = "Environment name"
  type        = string
}

variable "tags" {
  description = "Tags to apply to resources"
  type        = map(string)
  default     = {}
}
</file>

<file path="modules/azure/vpc/main.tf">
# Resource group
resource "azurerm_resource_group" "main" {
  name     = var.resource_group_name
  location = var.location
  tags     = var.tags
}

# Virtual Network
resource "azurerm_virtual_network" "main" {
  name                = "${var.name_prefix}-vnet"
  address_space       = [var.vnet_cidr]
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name
  tags                = var.tags
}

# Private subnet for AKS
resource "azurerm_subnet" "aks" {
  name                 = "${var.name_prefix}-aks-subnet"
  resource_group_name  = azurerm_resource_group.main.name
  virtual_network_name = azurerm_virtual_network.main.name
  address_prefixes     = [var.aks_subnet_cidr]
  
  # Disable private endpoint network policies for AKS
  private_endpoint_network_policies = "Disabled"
  
  # Service endpoints for secure access to Azure services
  service_endpoints = [
    "Microsoft.Storage",
    "Microsoft.KeyVault",
    "Microsoft.ContainerRegistry",
    "Microsoft.Sql"
  ]
}

# Private subnet for other workloads
resource "azurerm_subnet" "private" {
  name                 = "${var.name_prefix}-private-subnet"
  resource_group_name  = azurerm_resource_group.main.name
  virtual_network_name = azurerm_virtual_network.main.name
  address_prefixes     = [var.private_subnet_cidr]
  
  service_endpoints = [
    "Microsoft.Storage",
    "Microsoft.KeyVault",
    "Microsoft.ContainerRegistry"
  ]
}

# Bastion subnet (if enabled)
resource "azurerm_subnet" "bastion" {
  count                = var.enable_bastion ? 1 : 0
  name                 = "AzureBastionSubnet"  # Must be named exactly this
  resource_group_name  = azurerm_resource_group.main.name
  virtual_network_name = azurerm_virtual_network.main.name
  address_prefixes     = [var.bastion_subnet_cidr]
}

# Public IP for NAT Gateway
resource "azurerm_public_ip" "nat" {
  count               = var.enable_nat_gateway ? 1 : 0
  name                = "${var.name_prefix}-nat-pip"
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name
  allocation_method   = "Static"
  sku                 = "Standard"
  zones               = ["1"]
  tags                = var.tags
}

# NAT Gateway for outbound connectivity
resource "azurerm_nat_gateway" "main" {
  count               = var.enable_nat_gateway ? 1 : 0
  name                = "${var.name_prefix}-nat"
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name
  sku_name            = "Standard"
  idle_timeout_in_minutes = 10
  zones               = ["1"]
  tags                = var.tags
}

# Associate NAT Gateway with Public IP
resource "azurerm_nat_gateway_public_ip_association" "main" {
  count                = var.enable_nat_gateway ? 1 : 0
  nat_gateway_id       = azurerm_nat_gateway.main[0].id
  public_ip_address_id = azurerm_public_ip.nat[0].id
}

# Associate NAT Gateway with AKS subnet
resource "azurerm_subnet_nat_gateway_association" "aks" {
  count          = var.enable_nat_gateway ? 1 : 0
  subnet_id      = azurerm_subnet.aks.id
  nat_gateway_id = azurerm_nat_gateway.main[0].id
}

# Associate NAT Gateway with private subnet
resource "azurerm_subnet_nat_gateway_association" "private" {
  count          = var.enable_nat_gateway ? 1 : 0
  subnet_id      = azurerm_subnet.private.id
  nat_gateway_id = azurerm_nat_gateway.main[0].id
}

# Network Security Group for AKS subnet
resource "azurerm_network_security_group" "aks" {
  name                = "${var.name_prefix}-aks-nsg"
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name
  
  # Allow internal communication within VNet
  security_rule {
    name                       = "AllowVNetInBound"
    priority                   = 100
    direction                  = "Inbound"
    access                     = "Allow"
    protocol                   = "*"
    source_port_range          = "*"
    destination_port_range     = "*"
    source_address_prefix      = "VirtualNetwork"
    destination_address_prefix = "VirtualNetwork"
  }
  
  # Deny all other inbound traffic
  security_rule {
    name                       = "DenyAllInBound"
    priority                   = 4096
    direction                  = "Inbound"
    access                     = "Deny"
    protocol                   = "*"
    source_port_range          = "*"
    destination_port_range     = "*"
    source_address_prefix      = "*"
    destination_address_prefix = "*"
  }
  
  tags = var.tags
}

# Associate NSG with AKS subnet
resource "azurerm_subnet_network_security_group_association" "aks" {
  subnet_id                 = azurerm_subnet.aks.id
  network_security_group_id = azurerm_network_security_group.aks.id
}

# Private DNS Zone for AKS
resource "azurerm_private_dns_zone" "aks" {
  count               = var.create_private_dns_zone ? 1 : 0
  name                = "privatelink.${var.location}.azmk8s.io"
  resource_group_name = azurerm_resource_group.main.name
  tags                = var.tags
}

# Link Private DNS Zone to VNet
resource "azurerm_private_dns_zone_virtual_network_link" "aks" {
  count                 = var.create_private_dns_zone ? 1 : 0
  name                  = "${var.name_prefix}-aks-dns-link"
  resource_group_name   = azurerm_resource_group.main.name
  private_dns_zone_name = azurerm_private_dns_zone.aks[0].name
  virtual_network_id    = azurerm_virtual_network.main.id
  registration_enabled  = false
  tags                  = var.tags
}

# Azure Bastion (if enabled)
resource "azurerm_public_ip" "bastion" {
  count               = var.enable_bastion ? 1 : 0
  name                = "${var.name_prefix}-bastion-pip"
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name
  allocation_method   = "Static"
  sku                 = "Standard"
  tags                = var.tags
}

resource "azurerm_bastion_host" "main" {
  count               = var.enable_bastion ? 1 : 0
  name                = "${var.name_prefix}-bastion"
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name
  sku                 = "Basic"
  
  ip_configuration {
    name                 = "configuration"
    subnet_id            = azurerm_subnet.bastion[0].id
    public_ip_address_id = azurerm_public_ip.bastion[0].id
  }
  
  tags = var.tags
}

# Log Analytics Workspace for monitoring
resource "azurerm_log_analytics_workspace" "main" {
  count               = var.enable_log_analytics ? 1 : 0
  name                = "${var.name_prefix}-logs"
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name
  sku                 = "PerGB2018"
  retention_in_days   = var.log_retention_days
  tags                = var.tags
}
</file>

<file path="modules/azure/vpc/outputs.tf">
output "resource_group_name" {
  description = "Resource group name"
  value       = azurerm_resource_group.main.name
}

output "resource_group_id" {
  description = "Resource group ID"
  value       = azurerm_resource_group.main.id
}

output "vnet_id" {
  description = "Virtual network ID"
  value       = azurerm_virtual_network.main.id
}

output "vnet_name" {
  description = "Virtual network name"
  value       = azurerm_virtual_network.main.name
}

output "aks_subnet_id" {
  description = "AKS subnet ID"
  value       = azurerm_subnet.aks.id
}

output "private_subnet_id" {
  description = "Private subnet ID"
  value       = azurerm_subnet.private.id
}

output "private_dns_zone_id" {
  description = "Private DNS zone ID for AKS"
  value       = var.create_private_dns_zone ? azurerm_private_dns_zone.aks[0].id : null
}

output "log_analytics_workspace_id" {
  description = "Log Analytics workspace ID"
  value       = var.enable_log_analytics ? azurerm_log_analytics_workspace.main[0].id : null
}

output "nat_gateway_id" {
  description = "NAT Gateway ID"
  value       = var.enable_nat_gateway ? azurerm_nat_gateway.main[0].id : null
}

output "bastion_host_id" {
  description = "Bastion host ID"
  value       = var.enable_bastion ? azurerm_bastion_host.main[0].id : null
}
</file>

<file path="modules/azure/vpc/variables.tf">
variable "resource_group_name" {
  description = "Name of the resource group"
  type        = string
}

variable "location" {
  description = "Azure region"
  type        = string
}

variable "name_prefix" {
  description = "Prefix for resource names"
  type        = string
}

variable "vnet_cidr" {
  description = "CIDR block for the virtual network"
  type        = string
  default     = "10.0.0.0/16"
}

variable "aks_subnet_cidr" {
  description = "CIDR block for AKS subnet"
  type        = string
  default     = "10.0.1.0/24"
}

variable "private_subnet_cidr" {
  description = "CIDR block for private subnet"
  type        = string
  default     = "10.0.2.0/24"
}

variable "bastion_subnet_cidr" {
  description = "CIDR block for bastion subnet"
  type        = string
  default     = "10.0.3.0/24"
}

variable "enable_nat_gateway" {
  description = "Enable NAT Gateway for outbound internet access"
  type        = bool
  default     = true
}

variable "enable_bastion" {
  description = "Enable Azure Bastion for secure remote access"
  type        = bool
  default     = false
}

variable "create_private_dns_zone" {
  description = "Create private DNS zone for AKS"
  type        = bool
  default     = true
}

variable "enable_log_analytics" {
  description = "Enable Log Analytics workspace"
  type        = bool
  default     = true
}

variable "log_retention_days" {
  description = "Log retention in days"
  type        = number
  default     = 30
}

variable "tags" {
  description = "Tags to apply to resources"
  type        = map(string)
  default     = {}
}
</file>

<file path="modules/gcp/gke/outputs.tf">
output "cluster_id" {
  description = "GKE cluster ID"
  value       = module.gke.cluster_id
}

output "cluster_name" {
  description = "GKE cluster name"
  value       = module.gke.name
}

output "cluster_endpoint" {
  description = "Endpoint for GKE control plane"
  value       = module.gke.endpoint
  sensitive   = true
}

output "cluster_ca_certificate" {
  description = "Base64 encoded public certificate for GKE cluster"
  value       = module.gke.ca_certificate
  sensitive   = true
}

output "cluster_master_version" {
  description = "Current master version of the GKE cluster"
  value       = module.gke.master_version
}

output "cluster_region" {
  description = "GKE cluster region"
  value       = module.gke.region
}

output "cluster_zones" {
  description = "List of zones in which the cluster resides"
  value       = module.gke.zones
}

output "service_account" {
  description = "The service account used by the node pool"
  value       = module.gke.service_account
}

output "network_name" {
  description = "The name of the VPC network"
  value       = var.network_name
}

output "subnet_name" {
  description = "The name of the subnet"
  value       = var.subnetwork_name
}

output "master_authorized_networks_config" {
  description = "Master authorized networks configuration"
  value       = module.gke.master_authorized_networks_config
}

output "node_pools_names" {
  description = "List of node pools names"
  value       = module.gke.node_pools_names
}

output "node_pools_versions" {
  description = "Node pools versions"
  value       = module.gke.node_pools_versions
}

output "cluster_identity_namespace" {
  description = "Workload Identity namespace"
  value       = var.enable_workload_identity ? "${var.project_id}.svc.id.goog" : null
}

output "get_credentials_command" {
  description = "gcloud command to get cluster credentials"
  value       = "gcloud container clusters get-credentials ${module.gke.name} --region ${module.gke.region} --project ${var.project_id} --internal-ip"
}

output "kubectl_config_command" {
  description = "Command to configure kubectl"
  value       = "kubectl config use-context gke_${var.project_id}_${module.gke.region}_${module.gke.name}"
}
</file>

<file path="modules/gcp/provider.tf">
terraform {
  required_version = ">= 1.0"

  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 5.0"
    }
    google-beta = {
      source  = "hashicorp/google-beta"
      version = "~> 5.0"
    }
  }
}
</file>

<file path="README.md">
# Multi-Cloud Kubernetes Terraform Template

This repository provides a secure, production-ready Terraform template for deploying Kubernetes clusters on Azure AKS and AWS EKS with consistent configuration and security best practices.

## Features

- **Multi-cloud support**: Deploy to Azure AKS or AWS EKS
- **Secure by default**: RBAC, network policies, encryption, audit logging
- **Size-based configuration**: Small (dev), Medium (staging), Large (production)
- **Environment-specific configurations**: Pre-built tfvars for different environments
- **Modular design**: Separate modules for Azure and AWS

## Quick Start

1. **Copy example configuration**:
   ```bash
   cp terraform.tfvars.example terraform.tfvars
   ```

2. **Edit terraform.tfvars** for your environment:
   ```hcl
   cloud_provider = "azure"  # or "aws"
   cluster_name = "my-cluster"
   environment = "dev"
   node_size_config = "small"  # small, medium, or large
   ```

3. **Deploy**:
   ```bash
   terraform init
   terraform plan
   terraform apply
   ```

4. **Configure kubectl**:
   ```bash
   # For Azure
   az aks get-credentials --resource-group <rg-name> --name <cluster-name>
   
   # For AWS
   aws eks update-kubeconfig --region <region> --name <cluster-name>
   ```

## Size Configurations

| Size | Use Case | Node Count | Instance Type (Azure/AWS) |
|------|----------|------------|---------------------------|
| Small | Dev/Test | 2 (1-5) | Standard_DS2_v2 / t3.medium |
| Medium | Staging | 3 (2-10) | Standard_DS3_v2 / t3.large |
| Large | Production | 5 (3-20) | Standard_DS4_v2 / t3.xlarge |

## Environment Examples

Use pre-configured environment files:

```bash
# Development on Azure
terraform apply -var-file="environments/dev-azure.tfvars"

# Performance testing on AWS
terraform apply -var-file="environments/perf-aws.tfvars"

# Production on Azure
terraform apply -var-file="environments/prod-azure.tfvars"
```

## Security Features

- **RBAC**: Role-based access control enabled
- **Network Security**: Network policies and security groups
- **Encryption**: Secrets encrypted at rest (KMS/Key Vault)
- **Audit Logging**: Comprehensive cluster audit logs
- **Private Networking**: Private subnets with NAT gateways
- **Auto-scaling**: Cluster and pod auto-scaling enabled

## Directory Structure

```
├── main.tf                    # Main Terraform configuration
├── variables.tf               # Variable definitions
├── outputs.tf                 # Output definitions
├── terraform.tfvars.example   # Example configuration
├── size-mapping.md           # Instance size reference
├── environments/             # Pre-configured environments
│   ├── dev-azure.tfvars
│   ├── dev-aws.tfvars
│   ├── perf-azure.tfvars
│   ├── perf-aws.tfvars
│   ├── prod-azure.tfvars
│   └── prod-aws.tfvars
└── modules/
    ├── azure/               # Azure AKS module
    └── aws/                 # AWS EKS module
```

## Prerequisites

- Terraform >= 1.0
- Azure CLI (for Azure deployments)
- AWS CLI (for AWS deployments)
- kubectl

## Authentication

### Azure
```bash
az login
az account set --subscription <subscription-id>
```

### AWS
```bash
aws configure
# or use AWS_PROFILE environment variable
```

## Customization

All configurations can be customized through variables. See `variables.tf` for all available options.

## Cleanup

```bash
terraform destroy
```

**Warning**: This will delete all resources including the cluster and all workloads.
</file>

<file path="size-mapping.md">
# Instance Size Mapping Configuration

This document describes the instance size mappings used across Azure AKS and AWS EKS deployments.

## Size Configurations

### Small (Development/Testing)
**Use case**: Development environments, testing, small workloads

| Cloud | Instance Type | Node Count | Min/Max | Disk Size | Cost Level |
|-------|---------------|------------|---------|-----------|------------|
| Azure | Standard_DS2_v2 | 2 | 1-5 | 128GB | $ |
| AWS | t3.medium | 2 | 1-5 | 20GB | $ |

**Specifications**:
- 2 vCPUs, 7GB RAM (Azure) / 2 vCPUs, 4GB RAM (AWS)
- Max pods per node: 30 (Azure) / Default (AWS)
- Auto-scaling enabled

### Medium (Staging/Pre-production)
**Use case**: Staging environments, medium workloads, performance testing

| Cloud | Instance Type | Node Count | Min/Max | Disk Size | Cost Level |
|-------|---------------|------------|---------|-----------|------------|
| Azure | Standard_DS3_v2 | 3 | 2-10 | 128GB | $$ |
| AWS | t3.large | 3 | 2-10 | 30GB | $$ |

**Specifications**:
- 4 vCPUs, 14GB RAM (Azure) / 2 vCPUs, 8GB RAM (AWS)
- Max pods per node: 50 (Azure) / Default (AWS)
- Auto-scaling enabled

### Large (Production)
**Use case**: Production environments, high-performance workloads

| Cloud | Instance Type | Node Count | Min/Max | Disk Size | Cost Level |
|-------|---------------|------------|---------|-----------|------------|
| Azure | Standard_DS4_v2 | 5 | 3-20 | 128GB | $$$ |
| AWS | t3.xlarge | 5 | 3-20 | 50GB | $$$ |

**Specifications**:
- 8 vCPUs, 28GB RAM (Azure) / 4 vCPUs, 16GB RAM (AWS)
- Max pods per node: 110 (Azure) / Default (AWS)
- Auto-scaling enabled

## Security Features

All configurations include:
- RBAC enabled
- Network policies (Azure CNI/AWS VPC CNI)
- Pod security policies
- Audit logging
- Encryption at rest
- Private endpoints (configurable)
- Monitoring and log analytics

## Usage

Set the `node_size_config` variable in your terraform.tfvars file:

```hcl
node_size_config = "small"    # or "medium" or "large"
```
</file>

<file path="terraform.tfvars.example">
# Example terraform.tfvars - copy and customize for your environment

# Choose cloud provider: "azure" or "aws"
cloud_provider = "azure"

# Cluster configuration
cluster_name = "my-k8s-cluster"
environment  = "dev"

# Node size configuration: "small", "medium", or "large"
node_size_config = "small"

# Azure-specific variables (only needed when cloud_provider = "azure")
resource_group_name = "my-k8s-rg"
location           = "East US"

# AWS-specific variables (only needed when cloud_provider = "aws")
aws_region = "us-east-1"

# Common tags
tags = {
  Environment = "dev"
  Project     = "kubernetes-cluster"
  Team        = "devops"
  Owner       = "your-team@company.com"
}
</file>

<file path="examples/ecs-ec2.tfvars">
# EC2 ECS Configuration Example
cloud_provider   = "aws"
aws_region       = "us-east-1"
cluster_name     = "my-ec2-cluster"
environment      = "dev"
node_size_config = "small"

# Enable ECS with EC2
enable_ecs          = true
ecs_launch_type     = "EC2"
ecs_container_image = "nginx:latest"
ecs_instance_type   = "t3.medium"
ecs_key_name        = "my-ec2-keypair" # Optional: Specify your EC2 key pair name for SSH access

tags = {
  Environment = "development"
  Project     = "multicloud-tf"
  LaunchType  = "ec2"
}
</file>

<file path="examples/ecs-fargate.tfvars">
# Fargate ECS Configuration Example
cloud_provider   = "aws"
aws_region       = "us-east-1"
cluster_name     = "my-fargate-cluster"
environment      = "dev"
node_size_config = "small"

# Enable ECS with Fargate
enable_ecs          = true
ecs_launch_type     = "FARGATE"
ecs_container_image = "nginx:latest"

tags = {
  Environment = "development"
  Project     = "multicloud-tf"
  LaunchType  = "fargate"
}
</file>

<file path="modules/aws/eks/variables.tf">
variable "cluster_name" {
  description = "Name of the EKS cluster"
  type        = string
}

variable "vpc_id" {
  description = "VPC ID where EKS cluster will be created"
  type        = string
}

variable "vpc_cidr_block" {
  description = "CIDR block of the VPC"
  type        = string
}

variable "private_subnet_ids" {
  description = "List of private subnet IDs for EKS cluster"
  type        = list(string)
}

variable "node_size_config" {
  description = "Node size configuration (small, medium, large)"
  type        = string
  default     = "small"
  validation {
    condition     = contains(["small", "medium", "large"], var.node_size_config)
    error_message = "Node size config must be one of: small, medium, large."
  }
}

variable "kubernetes_version" {
  description = "Kubernetes version for EKS cluster"
  type        = string
  default     = "1.28"
}

variable "ami_type" {
  description = "AMI type for EKS worker nodes"
  type        = string
  default     = "AL2_x86_64"
}

variable "capacity_type" {
  description = "Capacity type for EKS worker nodes (ON_DEMAND or SPOT)"
  type        = string
  default     = "ON_DEMAND"
  validation {
    condition     = contains(["ON_DEMAND", "SPOT"], var.capacity_type)
    error_message = "Capacity type must be either ON_DEMAND or SPOT."
  }
}

variable "enabled_cluster_log_types" {
  description = "List of cluster log types to enable"
  type        = list(string)
  default = [
    "api",
    "audit",
    "authenticator",
    "controllerManager",
    "scheduler"
  ]
}

variable "log_retention_in_days" {
  description = "CloudWatch log retention in days"
  type        = number
  default     = 7
}

variable "bastion_security_group_id" {
  description = "Security group ID of bastion host (optional)"
  type        = string
  default     = null
}

variable "node_ssh_key_name" {
  description = "EC2 Key Pair name for SSH access to worker nodes"
  type        = string
  default     = null
}

variable "addon_versions" {
  description = "Versions for EKS add-ons"
  type = object({
    vpc_cni    = optional(string, null)
    coredns    = optional(string, null)
    kube_proxy = optional(string, null)
    ebs_csi    = optional(string, null)
  })
  default = {}
}

variable "kms_key_arn" {
  description = "ARN of existing KMS key for EKS cluster encryption. If not provided, a new key will be created."
  type        = string
  default     = null
}

variable "tags" {
  description = "Tags to apply to resources"
  type        = map(string)
  default     = {}
}
</file>

<file path="modules/aws/vpc/outputs.tf">
output "vpc_id" {
  description = "ID of the VPC"
  value       = var.create_vpc ? aws_vpc.main[0].id : var.vpc_id
}

output "vpc_cidr_block" {
  description = "CIDR block of the VPC"
  value       = var.create_vpc ? aws_vpc.main[0].cidr_block : (var.vpc_id != "" ? data.aws_vpc.existing[0].cidr_block : "")
}

output "private_subnet_ids" {
  description = "IDs of the private subnets"
  value       = var.create_vpc ? aws_subnet.private[*].id : var.private_subnet_ids
}

output "public_subnet_ids" {
  description = "IDs of the public subnets"
  value       = var.create_vpc ? aws_subnet.public[*].id : var.public_subnet_ids
}

output "private_route_table_ids" {
  description = "IDs of the private route tables"
  value       = var.create_vpc ? aws_route_table.private[*].id : []
}

output "public_route_table_id" {
  description = "ID of the public route table"
  value       = var.create_vpc && var.enable_public_subnets ? aws_route_table.public[0].id : null
}

output "internet_gateway_id" {
  description = "ID of the internet gateway"
  value       = var.create_vpc && var.enable_public_subnets ? aws_internet_gateway.main[0].id : null
}

output "nat_gateway_ids" {
  description = "IDs of the NAT gateways"
  value       = var.create_vpc ? aws_nat_gateway.main[*].id : []
}

output "vpc_endpoints_security_group_id" {
  description = "Security group ID for VPC endpoints"
  value       = var.create_vpc && var.enable_vpc_endpoints ? aws_security_group.vpc_endpoints[0].id : null
}

output "availability_zones" {
  description = "List of availability zones used"
  value       = var.create_vpc ? slice(data.aws_availability_zones.available.names, 0, var.availability_zones_count) : []
}

output "created_vpc" {
  description = "Whether a new VPC was created"
  value       = var.create_vpc
}
</file>

<file path="modules/aws/vpc/variables.tf">
variable "name_prefix" {
  description = "Prefix for resource names"
  type        = string
}

variable "create_vpc" {
  description = "Whether to create a new VPC or use existing subnets"
  type        = bool
  default     = true
}

variable "private_subnet_ids" {
  description = "List of existing private subnet IDs (required when create_vpc = false)"
  type        = list(string)
  default     = []
}

variable "public_subnet_ids" {
  description = "List of existing public subnet IDs (required when create_vpc = false)"
  type        = list(string)
  default     = []
}

variable "vpc_id" {
  description = "Existing VPC ID (required when create_vpc = false)"
  type        = string
  default     = ""
}

variable "vpc_cidr" {
  description = "CIDR block for VPC"
  type        = string
  default     = "10.0.0.0/16"
}

variable "availability_zones_count" {
  description = "Number of availability zones to use (only applies when create_vpc = true)"
  type        = number
  default     = 3
  validation {
    condition     = var.availability_zones_count >= 2 && var.availability_zones_count <= 6
    error_message = "Availability zones count must be between 2 and 6."
  }
}

variable "enable_private_subnets" {
  description = "Whether to create private subnets (only applies when create_vpc = true)"
  type        = bool
  default     = true
}

variable "enable_public_subnets" {
  description = "Whether to create public subnets (only applies when create_vpc = true)"
  type        = bool
  default     = true
}

variable "enable_nat_gateway" {
  description = "Whether to create NAT gateways for private subnets (only applies when create_vpc = true)"
  type        = bool
  default     = true
}

variable "enable_vpc_endpoints" {
  description = "Whether to create VPC endpoints for AWS services (only applies when create_vpc = true)"
  type        = bool
  default     = true
}

variable "tags" {
  description = "Tags to apply to resources"
  type        = map(string)
  default     = {}
}
</file>

<file path="modules/gcp/gke/main.tf">
# Create KMS keyring and key if not provided
resource "google_kms_key_ring" "gke" {
  count    = var.database_encryption_key_name == null ? 1 : 0
  name     = "${var.cluster_name}-keyring"
  location = var.region
  project  = var.project_id
}

resource "google_kms_crypto_key" "gke" {
  count           = var.database_encryption_key_name == null ? 1 : 0
  name            = "${var.cluster_name}-key"
  key_ring        = google_kms_key_ring.gke[0].id
  rotation_period = "7776000s" # 90 days
  purpose         = "ENCRYPT_DECRYPT"

  lifecycle {
    prevent_destroy = false
  }
}

# Grant GKE service account access to the KMS key
data "google_project" "project" {
  project_id = var.project_id
}

resource "google_kms_crypto_key_iam_member" "gke_sa" {
  count         = var.database_encryption_key_name == null ? 1 : 0
  crypto_key_id = google_kms_crypto_key.gke[0].id
  role          = "roles/cloudkms.cryptoKeyEncrypterDecrypter"
  member        = "serviceAccount:service-${data.google_project.project.number}@container-engine-robot.iam.gserviceaccount.com"
}

locals {
  # Determine which KMS key to use
  kms_key_name = var.database_encryption_key_name != null ? var.database_encryption_key_name : (var.database_encryption_key_name == null ? google_kms_crypto_key.gke[0].id : null)
  
  # Map node configurations to match AWS c5.2xlarge (8 vCPU, 16 GB RAM)
  node_size_map = {
    small = {
      machine_type = "n2-standard-2"  # 2 vCPU, 8 GB
      min_count    = 1
      max_count    = 5
      initial_node_count = 2
      disk_size_gb = 50
      disk_type    = "pd-standard"
    }
    medium = {
      machine_type = "n2-standard-4"  # 4 vCPU, 16 GB
      min_count    = 2
      max_count    = 10
      initial_node_count = 3
      disk_size_gb = 100
      disk_type    = "pd-standard"
    }
    large = {
      machine_type = "n2-standard-8"  # 8 vCPU, 32 GB - matches AWS c5.2xlarge
      min_count    = 3
      max_count    = 20
      initial_node_count = 5
      disk_size_gb = 100
      disk_type    = "pd-ssd"
    }
  }

  node_config = local.node_size_map[var.node_size_config]
}

# Use the terraform-google-modules/kubernetes-engine module for GKE
module "gke" {
  source  = "terraform-google-modules/kubernetes-engine/google//modules/private-cluster"
  version = "~> 29.0"

  project_id = var.project_id
  name       = var.cluster_name
  region     = var.region

  # Network configuration - assumes existing VPC
  network           = var.network_name
  subnetwork        = var.subnetwork_name
  ip_range_pods     = var.pods_range_name
  ip_range_services = var.services_range_name

  # Private cluster configuration
  enable_private_endpoint = true
  enable_private_nodes    = true
  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  # Master authorized networks - for admin access
  master_authorized_networks = var.master_authorized_networks

  # Kubernetes version and release channel
  kubernetes_version = var.kubernetes_version
  release_channel    = var.release_channel

  # Security features
  enable_shielded_nodes       = var.enable_shielded_nodes
  enable_binary_authorization = var.enable_binary_authorization
  workload_identity_enabled   = var.enable_workload_identity
  enable_network_policy       = var.enable_network_policy

  # RBAC configuration - Google Groups for admins
  authenticator_security_group = var.masters_group_email != "" ? var.masters_group_email : null
  rbac_group_domain            = var.rbac_group_domain

  # Monitoring and logging
  logging_enabled_components    = var.logging_enabled_components
  monitoring_enabled_components = var.monitoring_enabled_components

  # Cluster addons
  horizontal_pod_autoscaling = var.enable_horizontal_pod_autoscaling
  vertical_pod_autoscaling   = var.enable_vertical_pod_autoscaling
  filestore_csi_driver       = var.enable_filestore_csi_driver
  gcs_fuse_csi_driver       = var.enable_gcs_fuse_csi_driver

  # Disable public endpoint completely for full privacy
  deploy_using_private_endpoint = true
  
  # Enable Dataplane V2 for better network performance
  datapath_provider = "ADVANCED_DATAPATH"

  # Node pool configuration
  node_pools = [
    {
      name               = "${var.cluster_name}-node-pool"
      machine_type       = local.node_config.machine_type
      min_count          = local.node_config.min_count
      max_count          = local.node_config.max_count
      initial_node_count = local.node_config.initial_node_count
      disk_size_gb       = local.node_config.disk_size_gb
      disk_type          = local.node_config.disk_type
      auto_repair        = true
      auto_upgrade       = true
      
      # Use spot instances for cost optimization (similar to AWS spot)
      spot               = var.node_size_config != "large"
      
      # Enable workload identity for the node pool
      workload_metadata_config = var.enable_workload_identity ? "GKE_METADATA" : "UNSPECIFIED"
      
      # Preemptible instances for non-production
      preemptible        = false
      
      # Node taints and labels
      node_metadata      = "GKE_METADATA_SERVER"
      
      # Security settings
      enable_secure_boot          = var.enable_shielded_nodes
      enable_integrity_monitoring = var.enable_shielded_nodes
      
      # Service account - will be created by the module
      service_account = "default"
    }
  ]

  # Node pool OAuth scopes
  node_pools_oauth_scopes = {
    all = [
      "https://www.googleapis.com/auth/cloud-platform",
    ]
  }

  # Node pool labels
  node_pools_labels = {
    all = merge(var.labels, {
      cluster_name = var.cluster_name
      node_pool    = "default"
    })
  }

  # Node pool tags
  node_pools_tags = {
    all = [
      var.cluster_name,
      "gke-node",
      "private"
    ]
  }

  # Cluster resource labels
  cluster_resource_labels = merge(var.labels, {
    cluster_name = var.cluster_name
    environment  = "private"
  })

  # Configure cluster autoscaling
  cluster_autoscaling = {
    enabled             = true
    autoscaling_profile = "OPTIMIZE_UTILIZATION"
    min_cpu_cores       = local.node_config.min_count * 8  # Based on n2-standard-8
    max_cpu_cores       = local.node_config.max_count * 8
    min_memory_gb       = local.node_config.min_count * 32
    max_memory_gb       = local.node_config.max_count * 32
    gpu_resources       = []
  }

  # Maintenance window
  maintenance_start_time = "05:00"
  maintenance_end_time   = "07:00"
  maintenance_recurrence = "FREQ=WEEKLY;BYDAY=SA,SU"

  # Remove default node pool
  remove_default_node_pool = true
  
  # Database encryption with Cloud KMS
  database_encryption = [
    {
      state    = "ENCRYPTED"
      key_name = local.kms_key_name
    }
  ]
}

# Create IAM binding for masters group if provided
resource "google_project_iam_member" "cluster_admin" {
  count   = var.masters_group_email != "" ? 1 : 0
  project = var.project_id
  role    = "roles/container.clusterAdmin"
  member  = "group:${var.masters_group_email}"
}

# Create firewall rules for private GKE access if needed
resource "google_compute_firewall" "gke_master_to_nodes" {
  name    = "${var.cluster_name}-master-to-nodes"
  network = var.network_name
  project = var.project_id

  allow {
    protocol = "tcp"
    ports    = ["443", "10250"]
  }

  source_ranges = [var.master_ipv4_cidr_block]
  target_tags   = ["gke-${var.cluster_name}"]
}
</file>

<file path="modules/gcp/gke/variables.tf">
variable "cluster_name" {
  description = "Name of the GKE cluster"
  type        = string
}

variable "project_id" {
  description = "GCP Project ID"
  type        = string
}

variable "region" {
  description = "GCP region"
  type        = string
}

variable "network_name" {
  description = "Name of the existing VPC network"
  type        = string
}

variable "subnetwork_name" {
  description = "Name of the existing subnet for GKE cluster"
  type        = string
}

variable "pods_range_name" {
  description = "Name of the secondary IP range for pods"
  type        = string
  default     = "gke-pods"
}

variable "services_range_name" {
  description = "Name of the secondary IP range for services"
  type        = string
  default     = "gke-services"
}

variable "node_size_config" {
  description = "Node size configuration (small, medium, large)"
  type        = string
  default     = "large"
  validation {
    condition     = contains(["small", "medium", "large"], var.node_size_config)
    error_message = "Node size config must be one of: small, medium, large."
  }
}

variable "kubernetes_version" {
  description = "Kubernetes version for GKE cluster"
  type        = string
  default     = "1.27"
}

variable "release_channel" {
  description = "GKE release channel"
  type        = string
  default     = "REGULAR"
}

variable "master_authorized_networks" {
  description = "List of networks allowed to access the Kubernetes master"
  type = list(object({
    cidr_block   = string
    display_name = string
  }))
  default = []
}

variable "master_ipv4_cidr_block" {
  description = "The IP range in CIDR notation for the master"
  type        = string
  default     = "172.16.0.0/28"
}

variable "rbac_group_domain" {
  description = "Google Groups domain for RBAC"
  type        = string
  default     = ""
}

variable "masters_group_email" {
  description = "Email of the Google Group for cluster masters/admins"
  type        = string
  default     = ""
}

variable "enable_binary_authorization" {
  description = "Enable Binary Authorization"
  type        = bool
  default     = false
}

variable "enable_shielded_nodes" {
  description = "Enable Shielded GKE nodes"
  type        = bool
  default     = true
}

variable "enable_workload_identity" {
  description = "Enable Workload Identity"
  type        = bool
  default     = true
}

variable "enable_network_policy" {
  description = "Enable network policy"
  type        = bool
  default     = true
}

variable "enable_filestore_csi_driver" {
  description = "Enable Filestore CSI driver"
  type        = bool
  default     = true
}

variable "enable_gcs_fuse_csi_driver" {
  description = "Enable GCS FUSE CSI driver"
  type        = bool
  default     = false
}

variable "logging_enabled_components" {
  description = "List of GKE components to log"
  type        = list(string)
  default     = ["SYSTEM_COMPONENTS", "WORKLOADS"]
}

variable "monitoring_enabled_components" {
  description = "List of GKE components to monitor"
  type        = list(string)
  default     = ["SYSTEM_COMPONENTS"]
}

variable "enable_vertical_pod_autoscaling" {
  description = "Enable vertical pod autoscaling"
  type        = bool
  default     = true
}

variable "enable_horizontal_pod_autoscaling" {
  description = "Enable horizontal pod autoscaling"
  type        = bool
  default     = true
}

variable "tags" {
  description = "A map of tags to apply to all resources"
  type        = map(string)
  default     = {}
}

variable "database_encryption_key_name" {
  description = "The Cloud KMS key name to use for cluster database encryption. Format: projects/PROJECT_ID/locations/LOCATION/keyRings/RING_NAME/cryptoKeys/KEY_NAME. If not provided, a new key will be created."
  type        = string
  default     = null
}

variable "labels" {
  description = "A map of labels to apply to all resources"
  type        = map(string)
  default     = {}
}
</file>

<file path="environments/dev-aws.tfvars">
# Development environment - AWS EKS
cloud_provider   = "aws"
cluster_name     = "dev-eks-cluster"
environment      = "dev"
node_size_config = "small"
aws_region       = "us-east-1"

tags = {
  Environment = "dev"
  Project     = "kubernetes-platform"
  Team        = "platform-engineering"
  CostCenter  = "engineering"
}
</file>

<file path="environments/perf-aws.tfvars">
# Performance testing environment - AWS EKS
cloud_provider   = "aws"
cluster_name     = "perf-eks-cluster"
environment      = "perf"
node_size_config = "large"
aws_region       = "us-west-2"

tags = {
  Environment = "performance"
  Project     = "kubernetes-platform"
  Team        = "qa-engineering"
  CostCenter  = "engineering"
}
</file>

<file path="environments/prod-aws.tfvars">
# Production environment - AWS EKS
cloud_provider   = "aws"
cluster_name     = "prod-eks-cluster"
environment      = "prod"
node_size_config = "large"
aws_region       = "us-west-2"

tags = {
  Environment = "production"
  Project     = "kubernetes-platform"
  Team        = "platform-engineering"
  CostCenter  = "production"
  Backup      = "required"
}
</file>

<file path="examples/aws-ecs-multi-service-setup.md">
# Multi-Service ECS Setup Example

This document explains how to deploy multiple ECS services with different configurations using this Terraform project.

## Current Architecture Limitation

The current Terraform modules support **one ECS service per deployment**. To deploy multiple services, you need to create separate Terraform configurations or workspaces.

## Recommended Approach

### Option 1: Separate Terraform Workspaces

Deploy each service in its own workspace:

```bash
# Deploy main service with secrets sidecar
terraform workspace new main-service
terraform apply -var-file="examples/aws-ecs-dual-ecr-services.tfvars"

# Deploy lightweight service without secrets sidecar  
terraform workspace new lightweight-service
terraform apply -var-file="examples/aws-ecs-lightweight-service.tfvars"
```

### Option 2: Separate Directories

Create separate directories for each service:

```
infrastructure/
├── main-service/
│   ├── main.tf -> symlink to ../../main.tf
│   ├── variables.tf -> symlink to ../../variables.tf
│   └── terraform.tfvars (main service config)
└── lightweight-service/
    ├── main.tf -> symlink to ../../main.tf
    ├── variables.tf -> symlink to ../../variables.tf
    └── terraform.tfvars (lightweight service config)
```

## Service Configuration Examples

### Main Service (with secrets sidecar)
- **Resources**: 4 vCPU, 4GB RAM
- **Port**: 50001
- **Features**: Secrets sidecar, certificates, WAF protection
- **Use case**: Main application requiring secure access to secrets and certificates

See: `aws-ecs-dual-ecr-services.tfvars`

### Lightweight Service (no secrets sidecar)
- **Resources**: 256 CPU units (~200m vCPU), 200MB RAM  
- **Port**: 8080
- **Features**: Minimal setup, no secrets sidecar, cost-optimized
- **Use case**: Simple API or microservice without secret requirements

See: `aws-ecs-lightweight-service.tfvars`

## Shared Infrastructure

When deploying multiple services, consider sharing:

1. **VPC**: Use `create_vpc = false` and `existing_vpc_id` for additional services
2. **Bastion Host**: Deploy once and reference from multiple services
3. **Load Balancer**: Current architecture creates one ALB per service

## Example Deployment Sequence

```bash
# 1. Deploy main service (creates VPC, bastion, etc.)
terraform workspace new main-service
terraform apply -var-file="examples/aws-ecs-dual-ecr-services.tfvars"

# 2. Get VPC ID from main service
MAIN_VPC_ID=$(terraform output vpc_id)

# 3. Deploy lightweight service using existing VPC
terraform workspace new lightweight-service
terraform apply -var-file="examples/aws-ecs-lightweight-service.tfvars" \
  -var="create_vpc=false" \
  -var="existing_vpc_id=${MAIN_VPC_ID}" \
  -var="enable_bastion=false"  # Reuse bastion from main service
```

## Service Communication

If services need to communicate:

1. **Internal ALB**: Set `ecs_internal_alb = true` for internal services
2. **Security Groups**: Services in the same VPC can communicate via security group rules
3. **Service Discovery**: Consider AWS Cloud Map for service discovery

## Secrets Management

- **With Sidecar**: Set `ecs_enable_secrets_sidecar = true`
  - Secrets accessed via HTTP endpoint (`http://localhost:8080`)
  - Automatic certificate management
  - Secure secret rotation support

- **Without Sidecar**: Set `ecs_enable_secrets_sidecar = false`
  - Direct environment variables only
  - No automatic secret rotation
  - Reduced resource usage and complexity
</file>

<file path="examples/create-keypair.sh">
#!/bin/bash

# Create Secure EC2 Key Pair for ECS Deployment
# This script generates a highly secure RSA key pair for EC2 instances

set -euo pipefail  # Exit on any error, undefined variable, or pipe failure

# Configuration
KEYPAIR_NAME="secure-app-keypair"
AWS_REGION="us-west-2"
KEY_SIZE=4096  # 4096-bit RSA key for maximum security
PRIVATE_KEY_FILE="${KEYPAIR_NAME}.pem"
PUBLIC_KEY_FILE="${KEYPAIR_NAME}.pub"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Helper functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Function to check if required tools are installed
check_dependencies() {
    local missing_tools=()
    
    if ! command -v openssl &> /dev/null; then
        missing_tools+=("openssl")
    fi
    
    if ! command -v aws &> /dev/null; then
        missing_tools+=("aws-cli")
    fi
    
    if ! command -v ssh-keygen &> /dev/null; then
        missing_tools+=("ssh-keygen")
    fi
    
    if [[ ${#missing_tools[@]} -gt 0 ]]; then
        log_error "Missing required tools: ${missing_tools[*]}"
        log_info "Please install the missing tools and run this script again"
        exit 1
    fi
    
    log_success "All required tools are available"
}

# Function to check AWS CLI configuration
check_aws_cli() {
    if ! aws sts get-caller-identity &> /dev/null; then
        log_error "AWS CLI is not configured or credentials are invalid"
        log_info "Configure with: aws configure"
        exit 1
    fi
    
    log_success "AWS CLI is configured and working"
}

# Function to generate secure private key
generate_private_key() {
    log_info "Generating $KEY_SIZE-bit RSA private key..."
    
    # Generate private key with strong encryption
    openssl genpkey \
        -algorithm RSA \
        -pkcs8 \
        -aes256 \
        -out "$PRIVATE_KEY_FILE" \
        -pkeyopt rsa_keygen_bits:$KEY_SIZE
    
    # Set secure permissions (read-only for owner)
    chmod 400 "$PRIVATE_KEY_FILE"
    
    log_success "Private key generated: $PRIVATE_KEY_FILE"
}

# Function to extract public key
generate_public_key() {
    log_info "Extracting public key from private key..."
    
    # Extract public key in OpenSSH format
    ssh-keygen -y -f "$PRIVATE_KEY_FILE" > "$PUBLIC_KEY_FILE"
    
    # Set secure permissions
    chmod 444 "$PUBLIC_KEY_FILE"
    
    log_success "Public key generated: $PUBLIC_KEY_FILE"
}

# Function to import key pair to AWS
import_to_aws() {
    log_info "Importing key pair to AWS EC2..."
    
    # Check if key pair already exists
    if aws ec2 describe-key-pairs --key-names "$KEYPAIR_NAME" --region "$AWS_REGION" &> /dev/null; then
        log_warning "Key pair '$KEYPAIR_NAME' already exists in AWS"
        echo -n "Do you want to delete and recreate it? (y/N): "
        read -r response
        
        if [[ "$response" =~ ^[Yy]$ ]]; then
            log_info "Deleting existing key pair..."
            aws ec2 delete-key-pair --key-name "$KEYPAIR_NAME" --region "$AWS_REGION"
            log_success "Existing key pair deleted"
        else
            log_info "Skipping AWS import. Using existing key pair."
            return 0
        fi
    fi
    
    # Import the public key to AWS
    aws ec2 import-key-pair \
        --key-name "$KEYPAIR_NAME" \
        --public-key-material "fileb://$PUBLIC_KEY_FILE" \
        --region "$AWS_REGION" > /dev/null
    
    log_success "Key pair imported to AWS: $KEYPAIR_NAME"
}

# Function to display key information
display_key_info() {
    echo
    log_info "=== Key Pair Information ==="
    log_info "Key pair name: $KEYPAIR_NAME"
    log_info "Key size: $KEY_SIZE bits"
    log_info "Private key file: $PRIVATE_KEY_FILE"
    log_info "Public key file: $PUBLIC_KEY_FILE"
    log_info "AWS Region: $AWS_REGION"
    echo
    
    # Display public key fingerprint
    local fingerprint
    fingerprint=$(ssh-keygen -lf "$PUBLIC_KEY_FILE" | awk '{print $2}')
    log_info "Public key fingerprint: $fingerprint"
    
    # Display file sizes
    local private_size public_size
    private_size=$(stat -f%z "$PRIVATE_KEY_FILE" 2>/dev/null || stat -c%s "$PRIVATE_KEY_FILE" 2>/dev/null || echo "unknown")
    public_size=$(stat -f%z "$PUBLIC_KEY_FILE" 2>/dev/null || stat -c%s "$PUBLIC_KEY_FILE" 2>/dev/null || echo "unknown")
    log_info "Private key size: $private_size bytes"
    log_info "Public key size: $public_size bytes"
}

# Function to create backup
create_backup() {
    local backup_dir="keypair-backup-$(date +%Y%m%d-%H%M%S)"
    
    echo -n "Do you want to create a backup of the key pair? (Y/n): "
    read -r response
    
    if [[ "$response" =~ ^[Nn]$ ]]; then
        return 0
    fi
    
    mkdir -p "$backup_dir"
    cp "$PRIVATE_KEY_FILE" "$backup_dir/"
    cp "$PUBLIC_KEY_FILE" "$backup_dir/"
    
    log_success "Backup created in: $backup_dir"
    log_warning "Store this backup in a secure location!"
}

# Function to validate key pair
validate_key_pair() {
    log_info "Validating key pair..."
    
    # Check if private key is valid
    if ! openssl pkey -in "$PRIVATE_KEY_FILE" -check -noout &> /dev/null; then
        log_error "Private key validation failed"
        return 1
    fi
    
    # Check if public key is valid
    if ! ssh-keygen -lf "$PUBLIC_KEY_FILE" &> /dev/null; then
        log_error "Public key validation failed"
        return 1
    fi
    
    # Verify that public key matches private key
    local private_pub_key public_key
    private_pub_key=$(ssh-keygen -y -f "$PRIVATE_KEY_FILE")
    public_key=$(cat "$PUBLIC_KEY_FILE")
    
    if [[ "$private_pub_key" != "$public_key" ]]; then
        log_error "Public and private keys do not match"
        return 1
    fi
    
    log_success "Key pair validation passed"
}

# Main script
main() {
    echo "============================================"
    echo "Secure EC2 Key Pair Generation Script"
    echo "============================================"
    echo
    log_info "This script will generate a $KEY_SIZE-bit RSA key pair for EC2 instances"
    log_info "Key pair name: $KEYPAIR_NAME"
    log_info "AWS Region: $AWS_REGION"
    echo
    
    # Check dependencies and AWS configuration
    check_dependencies
    check_aws_cli
    
    # Check if files already exist
    if [[ -f "$PRIVATE_KEY_FILE" ]] || [[ -f "$PUBLIC_KEY_FILE" ]]; then
        log_warning "Key files already exist in current directory"
        echo -n "Do you want to overwrite them? (y/N): "
        read -r response
        
        if [[ ! "$response" =~ ^[Yy]$ ]]; then
            log_info "Operation cancelled by user"
            exit 0
        fi
        
        rm -f "$PRIVATE_KEY_FILE" "$PUBLIC_KEY_FILE"
    fi
    
    # Confirm before proceeding
    echo -n "Do you want to continue with key generation? (Y/n): "
    read -r response
    if [[ "$response" =~ ^[Nn]$ ]]; then
        log_info "Operation cancelled by user"
        exit 0
    fi
    
    echo
    log_info "=== Generating Key Pair ==="
    
    # Generate private key (will prompt for passphrase)
    generate_private_key
    
    # Generate public key
    generate_public_key
    
    # Validate key pair
    validate_key_pair
    
    # Import to AWS
    import_to_aws
    
    # Display information
    display_key_info
    
    # Create backup
    create_backup
    
    echo
    log_success "Key pair generation completed successfully!"
    echo
    log_info "=== Security Recommendations ==="
    log_warning "• Store the private key ($PRIVATE_KEY_FILE) in a secure location"
    log_warning "• Never share the private key or commit it to version control"
    log_warning "• The private key is encrypted with AES-256 - remember your passphrase!"
    log_warning "• Consider using SSH agent for key management"
    log_warning "• Regularly rotate your key pairs for better security"
    echo
    log_info "=== Next Steps ==="
    log_info "1. Update your terraform.tfvars with: bastion_key_name = \"$KEYPAIR_NAME\""
    log_info "2. Run ./generate-certs.sh to generate TLS certificates"
    log_info "3. Run ./upload-secrets.sh to upload application secrets"
    log_info "4. Deploy your infrastructure with terraform apply"
    echo
    log_info "=== SSH Connection Example ==="
    log_info "ssh -i $PRIVATE_KEY_FILE ec2-user@<instance-ip>"
    echo
}

# Handle script interruption
trap 'log_error "Script interrupted by user"; exit 1' INT TERM

# Run main function
main "$@"
</file>

<file path="examples/ECS-HTTP-SECRETS-README.md">
# ECS HTTP Server with Secrets Sidecar Deployment Guide

This example demonstrates how to deploy an ECS cluster with two services:
1. **HTTP Server Service** - A secure HTTPS server with self-signed certificates
2. **Secrets Sidecar Service** - Fetches secrets from AWS Secrets Manager and provides them to the HTTP server

## Architecture Overview

```
Internet → ALB (HTTPS) → ECS Service
                           ├── HTTP Server Container (nginx:443)
                           └── Secrets Sidecar Container (localhost:8080)
                               └── AWS Secrets Manager
```

## Prerequisites

1. **AWS CLI** configured with appropriate permissions
2. **Terraform** v1.0+ installed
3. **OpenSSL** for certificate generation
4. **AWS Key Pair** for bastion host access

### Required AWS Permissions

Your AWS credentials need the following permissions:
- EC2 (VPC, Subnets, Security Groups, ALB)
- ECS (Clusters, Services, Task Definitions)
- IAM (Roles, Policies)
- Secrets Manager (Create/Read secrets)
- Certificate Manager (Import certificates)
- CloudWatch (Log Groups)

## Quick Start

### 1. Clone and Navigate

```bash
cd /path/to/multicloud-tf
```

### 2. Create AWS Key Pair

```bash
# Create a new key pair (replace 'my-key-pair' with your preferred name)
aws ec2 create-key-pair --key-name my-key-pair --query 'KeyMaterial' --output text > my-key-pair.pem
chmod 400 my-key-pair.pem
```

### 3. Generate Self-Signed Certificates

```bash
./scripts/generate-certificates.sh
```

### 4. Setup AWS Secrets Manager

```bash
./scripts/setup-secrets.sh
```

### 5. Deploy the Infrastructure

```bash
# Initialize Terraform
terraform init

# Plan the deployment
terraform plan -var-file="examples/ecs-http-secrets-example.tfvars"

# Deploy
terraform apply -var-file="examples/ecs-http-secrets-example.tfvars"
```

## Configuration Details

### Service Architecture

#### HTTP Server Container
- **Image**: nginx:latest (replace with your application)
- **Port**: 443 (HTTPS)
- **Resources**: 0.5 vCPU, 1GB RAM
- **Certificates**: Mounted from EFS or S3
- **Environment Variables**:
  - `SECRETS_SIDECAR_URL=http://localhost:8080`
  - `CERT_PATH=/etc/ssl/certs/server.crt`
  - `KEY_PATH=/etc/ssl/private/server.key`

#### Secrets Sidecar Container
- **Image**: amazon/aws-cli:latest (replace with your sidecar)
- **Port**: 8080 (HTTP, localhost only)
- **Purpose**: Fetch secrets from AWS Secrets Manager
- **API Endpoints**:
  - `GET /secrets/{secret-name}` - Retrieve specific secret
  - `GET /health` - Health check

### Secrets Management

The following secrets are created in AWS Secrets Manager under the `http-server/` prefix:

| Secret Name | Purpose | Example Value |
|-------------|---------|---------------|
| `database_password` | Database authentication | `my-secure-db-password` |
| `api_key` | External API access | `sk-1234567890abcdefghijklmnop` |
| `jwt_signing_key` | JWT token signing | `super-secret-jwt-key-for-token-signing` |
| `redis_password` | Redis authentication | `redis-secure-password-123` |
| `encryption_key` | Data encryption | `32-char-aes256-encryption-key!!` |
| `oauth_client_secret` | OAuth integration | `oauth-app-client-secret-value` |
| `smtp_password` | Email service | `email-service-password-secure` |
| `third_party_api_key` | 3rd party service | `3rd-party-service-api-key-123` |

### Security Features

- **VPC Isolation**: Services run in private subnets
- **Security Groups**: Restrictive ingress/egress rules
- **SSL/TLS**: End-to-end encryption with self-signed certificates
- **WAF**: Web Application Firewall with rate limiting
- **Secrets Management**: Secure secret storage and retrieval
- **Bastion Host**: Secure access for troubleshooting

## Usage Examples

### Accessing Your Service

After deployment, get the ALB DNS name:

```bash
# Get ALB DNS name
terraform output alb_dns_name

# Test the service
curl -k https://<alb-dns-name>/health
```

### Fetching Secrets (from within containers)

The HTTP server can fetch secrets from the sidecar:

```bash
# Get database password
curl http://localhost:8080/secrets/database_password

# Get API key
curl http://localhost:8080/secrets/api_key
```

### SSH Access via Bastion

```bash
# Get bastion IP
terraform output bastion_public_ip

# SSH to bastion
ssh -i my-key-pair.pem ec2-user@<bastion-ip>

# From bastion, you can access private resources
```

## Customization

### Using Your Own Images

1. **Build your HTTP server image**:
```bash
# Example Dockerfile for HTTP server
FROM nginx:alpine
COPY nginx.conf /etc/nginx/nginx.conf
COPY ssl/ /etc/ssl/
EXPOSE 443
CMD ["nginx", "-g", "daemon off;"]
```

2. **Build your secrets sidecar image**:
```bash
# Example Dockerfile for secrets sidecar
FROM python:3.9-alpine
RUN pip install boto3 flask
COPY sidecar.py /app/
EXPOSE 8080
CMD ["python", "/app/sidecar.py"]
```

3. **Push to ECR**:
```bash
# Create ECR repositories
aws ecr create-repository --repository-name http-server
aws ecr create-repository --repository-name secrets-sidecar

# Push images (follow ECR push commands)
```

4. **Update tfvars**:
```hcl
ecs_container_image = "123456789012.dkr.ecr.us-west-2.amazonaws.com/http-server:latest"
ecs_secrets_sidecar_image = "123456789012.dkr.ecr.us-west-2.amazonaws.com/secrets-sidecar:latest"
```

### Environment-Specific Configuration

Create separate tfvars files for different environments:

```bash
# Development
terraform apply -var-file="examples/ecs-http-secrets-example.tfvars" -var="environment=dev"

# Staging
terraform apply -var-file="examples/ecs-http-secrets-example.tfvars" -var="environment=staging"

# Production
terraform apply -var-file="examples/ecs-http-secrets-example.tfvars" -var="environment=prod" -var="ecs_desired_count=5"
```

## Monitoring and Troubleshooting

### CloudWatch Logs

```bash
# View ECS service logs
aws logs describe-log-groups --log-group-name-prefix="/ecs/http-secrets-cluster"

# Tail logs
aws logs tail "/ecs/http-secrets-cluster" --follow
```

### ECS Service Status

```bash
# Check service status
aws ecs describe-services --cluster http-secrets-cluster --services http-secrets-cluster-with-alb

# Check task health
aws ecs describe-tasks --cluster http-secrets-cluster --tasks <task-arn>
```

### Common Issues

1. **Service not starting**: Check CloudWatch logs for container errors
2. **ALB health checks failing**: Verify health check path and port
3. **Secrets not accessible**: Check IAM roles and Secrets Manager permissions
4. **SSL certificate issues**: Verify certificate format and file paths

## Cleanup

```bash
# Destroy the infrastructure
terraform destroy -var-file="examples/ecs-http-secrets-example.tfvars"

# Clean up secrets (optional)
./scripts/cleanup-secrets.sh

# Remove certificates
rm -rf certs/
```

## Security Best Practices

1. **Restrict ALB access**: Update `ecs_allowed_cidr_blocks` to specific IPs
2. **Use ACM certificates**: Replace self-signed certificates with ACM for production
3. **Rotate secrets**: Implement secret rotation policies
4. **Monitor access**: Enable CloudTrail and VPC Flow Logs
5. **Update images**: Regularly update container images
6. **Network segmentation**: Use separate subnets for different tiers

## Support

For issues or questions:
1. Check the troubleshooting section above
2. Review CloudWatch logs
3. Verify AWS permissions
4. Submit an issue to the project repository

## Related Examples

- `aws-ecs-full-stack.tfvars` - Full stack ECS deployment
- `aws-ecs-production-ready.tfvars` - Production-ready configuration
- `aws-ecs-dual-service-concept.tfvars` - Multi-service deployment concept
</file>

<file path="examples/generate-certs.sh">
#!/bin/bash

# Generate SSL Certificates and Upload to AWS Secrets Manager
# This script creates SSL certificates for inter-container TLS communication
# and optionally for load balancer usage if ACM certificate is not provided

set -euo pipefail  # Exit on any error, undefined variable, or pipe failure

# Configuration
SECRETS_PREFIX="secure-app/prod"
AWS_REGION="us-west-2"
CERT_VALIDITY_DAYS=365
KEY_SIZE=4096

# Certificate file names
PRIVATE_KEY_FILE="ssl_certificate.key"
CERTIFICATE_FILE="ssl_certificate.pem"
CSR_FILE="ssl_certificate.csr"
CONFIG_FILE="ssl_certificate.conf"

# Default certificate settings
DEFAULT_COUNTRY="US"
DEFAULT_STATE="California"
DEFAULT_CITY="San Francisco"
DEFAULT_ORGANIZATION="Secure App Corp"
DEFAULT_OU="IT Department"
DEFAULT_COMMON_NAME="secure-app.local"
DEFAULT_EMAIL="admin@secure-app.com"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Helper functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Function to check if required tools are installed
check_dependencies() {
    local missing_tools=()
    
    if ! command -v openssl &> /dev/null; then
        missing_tools+=("openssl")
    fi
    
    if ! command -v aws &> /dev/null; then
        missing_tools+=("aws-cli")
    fi
    
    if [[ ${#missing_tools[@]} -gt 0 ]]; then
        log_error "Missing required tools: ${missing_tools[*]}"
        log_info "Please install the missing tools and run this script again"
        exit 1
    fi
    
    log_success "All required tools are available"
}

# Function to check AWS CLI configuration
check_aws_cli() {
    if ! aws sts get-caller-identity &> /dev/null; then
        log_error "AWS CLI is not configured or credentials are invalid"
        log_info "Configure with: aws configure"
        exit 1
    fi
    
    log_success "AWS CLI is configured and working"
}

# Function to collect certificate information
collect_cert_info() {
    echo
    log_info "=== SSL Certificate Configuration ==="
    echo "Please provide certificate information (press Enter for defaults):"
    echo
    
    echo -n "Country (2-letter code) [$DEFAULT_COUNTRY]: "
    read -r COUNTRY
    COUNTRY=${COUNTRY:-$DEFAULT_COUNTRY}
    
    echo -n "State/Province [$DEFAULT_STATE]: "
    read -r STATE
    STATE=${STATE:-$DEFAULT_STATE}
    
    echo -n "City [$DEFAULT_CITY]: "
    read -r CITY
    CITY=${CITY:-$DEFAULT_CITY}
    
    echo -n "Organization [$DEFAULT_ORGANIZATION]: "
    read -r ORGANIZATION
    ORGANIZATION=${ORGANIZATION:-$DEFAULT_ORGANIZATION}
    
    echo -n "Organizational Unit [$DEFAULT_OU]: "
    read -r OU
    OU=${OU:-$DEFAULT_OU}
    
    echo -n "Common Name (domain) [$DEFAULT_COMMON_NAME]: "
    read -r COMMON_NAME
    COMMON_NAME=${COMMON_NAME:-$DEFAULT_COMMON_NAME}
    
    echo -n "Email Address [$DEFAULT_EMAIL]: "
    read -r EMAIL
    EMAIL=${EMAIL:-$DEFAULT_EMAIL}
    
    echo
    log_info "Certificate will be created with:"
    log_info "  Country: $COUNTRY"
    log_info "  State: $STATE"
    log_info "  City: $CITY"
    log_info "  Organization: $ORGANIZATION"
    log_info "  Organizational Unit: $OU"
    log_info "  Common Name: $COMMON_NAME"
    log_info "  Email: $EMAIL"
    log_info "  Validity: $CERT_VALIDITY_DAYS days"
    log_info "  Key Size: $KEY_SIZE bits"
}

# Function to create OpenSSL configuration file
create_ssl_config() {
    log_info "Creating OpenSSL configuration file..."
    
    cat > "$CONFIG_FILE" << EOF
[req]
default_bits = $KEY_SIZE
prompt = no
distinguished_name = req_distinguished_name
req_extensions = v3_req
x509_extensions = v3_ca

[req_distinguished_name]
C = $COUNTRY
ST = $STATE
L = $CITY
O = $ORGANIZATION
OU = $OU
CN = $COMMON_NAME
emailAddress = $EMAIL

[v3_req]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
extendedKeyUsage = serverAuth, clientAuth

[v3_ca]
subjectKeyIdentifier = hash
authorityKeyIdentifier = keyid:always,issuer
basicConstraints = critical,CA:true
keyUsage = critical, digitalSignature, cRLSign, keyCertSign
subjectAltName = @alt_names
extendedKeyUsage = serverAuth, clientAuth

[alt_names]
DNS.1 = $COMMON_NAME
DNS.2 = localhost
DNS.3 = *.local
DNS.4 = secure-app
DNS.5 = *.secure-app.local
IP.1 = 127.0.0.1
IP.2 = 10.0.0.0/8
IP.3 = 172.16.0.0/12
IP.4 = 192.168.0.0/16
EOF

    log_success "OpenSSL configuration created: $CONFIG_FILE"
}

# Function to generate private key
generate_private_key() {
    log_info "Generating $KEY_SIZE-bit RSA private key..."
    
    openssl genrsa -out "$PRIVATE_KEY_FILE" $KEY_SIZE
    chmod 600 "$PRIVATE_KEY_FILE"
    
    log_success "Private key generated: $PRIVATE_KEY_FILE"
}

# Function to generate certificate signing request
generate_csr() {
    log_info "Generating Certificate Signing Request..."
    
    openssl req -new \
        -key "$PRIVATE_KEY_FILE" \
        -out "$CSR_FILE" \
        -config "$CONFIG_FILE"
    
    log_success "CSR generated: $CSR_FILE"
}

# Function to generate self-signed certificate
generate_certificate() {
    log_info "Generating self-signed certificate..."
    
    openssl req -x509 \
        -new \
        -key "$PRIVATE_KEY_FILE" \
        -out "$CERTIFICATE_FILE" \
        -days $CERT_VALIDITY_DAYS \
        -config "$CONFIG_FILE" \
        -extensions v3_ca
    
    chmod 644 "$CERTIFICATE_FILE"
    
    log_success "Self-signed certificate generated: $CERTIFICATE_FILE"
}

# Function to validate certificate
validate_certificate() {
    log_info "Validating generated certificate..."
    
    # Check certificate validity
    if ! openssl x509 -in "$CERTIFICATE_FILE" -text -noout > /dev/null 2>&1; then
        log_error "Certificate validation failed"
        return 1
    fi
    
    # Check if private key matches certificate
    local cert_modulus key_modulus
    cert_modulus=$(openssl x509 -noout -modulus -in "$CERTIFICATE_FILE" | openssl md5)
    key_modulus=$(openssl rsa -noout -modulus -in "$PRIVATE_KEY_FILE" | openssl md5)
    
    if [[ "$cert_modulus" != "$key_modulus" ]]; then
        log_error "Private key does not match certificate"
        return 1
    fi
    
    log_success "Certificate validation passed"
}

# Function to display certificate information
display_cert_info() {
    echo
    log_info "=== Certificate Information ==="
    
    # Display certificate details
    openssl x509 -in "$CERTIFICATE_FILE" -text -noout | grep -A 1 "Subject:"
    openssl x509 -in "$CERTIFICATE_FILE" -text -noout | grep -A 1 "Validity"
    openssl x509 -in "$CERTIFICATE_FILE" -text -noout | grep -A 10 "Subject Alternative Name"
    
    # Display certificate fingerprints
    echo
    log_info "Certificate Fingerprints:"
    local sha256_fp sha1_fp
    sha256_fp=$(openssl x509 -noout -fingerprint -sha256 -in "$CERTIFICATE_FILE" | cut -d'=' -f2)
    sha1_fp=$(openssl x509 -noout -fingerprint -sha1 -in "$CERTIFICATE_FILE" | cut -d'=' -f2)
    log_info "  SHA256: $sha256_fp"
    log_info "  SHA1:   $sha1_fp"
}

# Function to upload certificates to AWS Secrets Manager
upload_to_secrets_manager() {
    log_info "Uploading certificates to AWS Secrets Manager..."
    
    # Read certificate and private key content
    local cert_content key_content
    cert_content=$(cat "$CERTIFICATE_FILE")
    key_content=$(cat "$PRIVATE_KEY_FILE")
    
    # Upload certificate
    local cert_secret_name="$SECRETS_PREFIX/ssl_certificate"
    if aws secretsmanager describe-secret --secret-id "$cert_secret_name" --region "$AWS_REGION" &> /dev/null; then
        log_warning "Certificate secret already exists. Updating..."
        aws secretsmanager update-secret \
            --secret-id "$cert_secret_name" \
            --secret-string "$cert_content" \
            --region "$AWS_REGION" > /dev/null
    else
        aws secretsmanager create-secret \
            --name "$cert_secret_name" \
            --description "SSL certificate for secure-app (generated by generate-certs.sh)" \
            --secret-string "$cert_content" \
            --region "$AWS_REGION" > /dev/null
    fi
    log_success "Certificate uploaded to: $cert_secret_name"
    
    # Upload private key
    local key_secret_name="$SECRETS_PREFIX/ssl_private_key"
    if aws secretsmanager describe-secret --secret-id "$key_secret_name" --region "$AWS_REGION" &> /dev/null; then
        log_warning "Private key secret already exists. Updating..."
        aws secretsmanager update-secret \
            --secret-id "$key_secret_name" \
            --secret-string "$key_content" \
            --region "$AWS_REGION" > /dev/null
    else
        aws secretsmanager create-secret \
            --name "$key_secret_name" \
            --description "SSL private key for secure-app (generated by generate-certs.sh)" \
            --secret-string "$key_content" \
            --region "$AWS_REGION" > /dev/null
    fi
    log_success "Private key uploaded to: $key_secret_name"
}

# Function to create certificate bundle
create_certificate_bundle() {
    local bundle_file="ssl_certificate_bundle.pem"
    
    log_info "Creating certificate bundle..."
    
    # Combine certificate and private key into a single file
    cat "$CERTIFICATE_FILE" "$PRIVATE_KEY_FILE" > "$bundle_file"
    chmod 600 "$bundle_file"
    
    log_success "Certificate bundle created: $bundle_file"
    
    # Upload bundle to secrets manager
    local bundle_secret_name="$SECRETS_PREFIX/ssl_certificate_bundle"
    local bundle_content
    bundle_content=$(cat "$bundle_file")
    
    if aws secretsmanager describe-secret --secret-id "$bundle_secret_name" --region "$AWS_REGION" &> /dev/null; then
        aws secretsmanager update-secret \
            --secret-id "$bundle_secret_name" \
            --secret-string "$bundle_content" \
            --region "$AWS_REGION" > /dev/null
    else
        aws secretsmanager create-secret \
            --name "$bundle_secret_name" \
            --description "SSL certificate bundle for secure-app (generated by generate-certs.sh)" \
            --secret-string "$bundle_content" \
            --region "$AWS_REGION" > /dev/null
    fi
    
    log_success "Certificate bundle uploaded to: $bundle_secret_name"
}

# Function to cleanup temporary files
cleanup_files() {
    echo -n "Do you want to keep the local certificate files? (Y/n): "
    read -r response
    
    if [[ "$response" =~ ^[Nn]$ ]]; then
        rm -f "$PRIVATE_KEY_FILE" "$CERTIFICATE_FILE" "$CSR_FILE" "$CONFIG_FILE" "ssl_certificate_bundle.pem"
        log_success "Local certificate files cleaned up"
    else
        log_info "Local certificate files preserved"
        log_warning "Remember to store these files securely and never commit them to version control"
    fi
}

# Main script
main() {
    echo "============================================"
    echo "SSL Certificate Generation Script"
    echo "============================================"
    echo
    log_info "This script will generate SSL certificates for inter-container TLS communication"
    log_info "Certificates will be uploaded to AWS Secrets Manager under: $SECRETS_PREFIX/"
    log_info "AWS Region: $AWS_REGION"
    echo
    
    # Check dependencies and AWS configuration
    check_dependencies
    check_aws_cli
    
    # Check if certificate files already exist
    if [[ -f "$CERTIFICATE_FILE" ]] || [[ -f "$PRIVATE_KEY_FILE" ]]; then
        log_warning "Certificate files already exist in current directory"
        echo -n "Do you want to overwrite them? (y/N): "
        read -r response
        
        if [[ ! "$response" =~ ^[Yy]$ ]]; then
            log_info "Operation cancelled by user"
            exit 0
        fi
        
        rm -f "$PRIVATE_KEY_FILE" "$CERTIFICATE_FILE" "$CSR_FILE" "$CONFIG_FILE"
    fi
    
    # Collect certificate information
    collect_cert_info
    
    # Confirm before proceeding
    echo -n "Do you want to continue with certificate generation? (Y/n): "
    read -r response
    if [[ "$response" =~ ^[Nn]$ ]]; then
        log_info "Operation cancelled by user"
        exit 0
    fi
    
    echo
    log_info "=== Generating SSL Certificate ==="
    
    # Generate certificate components
    create_ssl_config
    generate_private_key
    generate_csr
    generate_certificate
    
    # Validate certificate
    validate_certificate
    
    # Display certificate information
    display_cert_info
    
    # Upload to AWS Secrets Manager
    upload_to_secrets_manager
    
    # Create certificate bundle
    create_certificate_bundle
    
    # Success message
    echo
    log_success "SSL certificate generation and upload completed successfully!"
    echo
    log_info "=== Certificates Created in AWS Secrets Manager ==="
    log_info "• $SECRETS_PREFIX/ssl_certificate"
    log_info "• $SECRETS_PREFIX/ssl_private_key"
    log_info "• $SECRETS_PREFIX/ssl_certificate_bundle"
    echo
    log_info "=== Certificate Usage ==="
    log_info "These certificates can be used for:"
    log_info "• Inter-container TLS communication"
    log_info "• Load balancer SSL termination (if ACM certificate not available)"
    log_info "• Service-to-service authentication"
    echo
    log_warning "=== Security Notes ==="
    log_warning "• These are self-signed certificates - not suitable for public internet"
    log_warning "• For production, consider using ACM certificates or CA-signed certificates"
    log_warning "• Certificate expires in $CERT_VALIDITY_DAYS days"
    log_warning "• Set up certificate rotation before expiry"
    echo
    log_info "=== Next Steps ==="
    log_info "1. Update terraform.tfvars to reference these certificate secrets"
    log_info "2. Run terraform apply to deploy your infrastructure"
    log_info "3. Your ECS services will automatically have access to these certificates"
    echo
    
    # Cleanup
    cleanup_files
}

# Handle script interruption
trap 'log_error "Script interrupted by user"; exit 1' INT TERM

# Run main function
main "$@"
</file>

<file path="examples/SECURE-DEPLOYMENT-README.md">
# Secure ECS Deployment Guide

This guide walks you through deploying a secure ECS cluster with ALB, secrets management, and TLS configuration using the provided scripts and configuration files.

## Architecture Overview

```
Internet --> ALB (Port 443) --> ECS Service (Port 50051)
                                 |
                                 +-- Main Application Container (Port 50051)
                                 +-- Secrets Broker Sidecar (Port 8080)
```

### Key Components

- **Main Service**: Runs on port 50051, accessible via ALB on port 443
- **Secrets Broker**: Sidecar container on port 8080 for secure secret access
- **TLS Communication**: Inter-container TLS using certificates from AWS Secrets Manager
- **Existing VPC**: Deploys into your existing VPC infrastructure

## Prerequisites

- AWS CLI installed and configured with appropriate permissions
- Terraform installed (version 0.14+ recommended)
- OpenSSL installed for certificate generation
- Existing VPC with public and private subnets
- Domain name and ACM certificate (optional, for production)

## Required AWS Permissions

Your AWS credentials need the following permissions:
- EC2: Full access for key pairs and VPC resources
- ECS: Full access for cluster and service management
- Secrets Manager: Create, read, update secrets
- Application Load Balancer: Create and manage ALBs
- IAM: Create roles and policies for ECS tasks
- CloudWatch: Create log groups and streams

## Quick Start

### Step 1: Prepare Scripts

All scripts are provided in the `examples/` directory:

```bash
cd examples/
ls -la *.sh
# Should show: create-keypair.sh, generate-certs.sh, upload-secrets.sh
```

### Step 2: Generate EC2 Key Pair

Create a secure 4096-bit RSA key pair for EC2 access:

```bash
./create-keypair.sh
```

This script will:
- Generate a 4096-bit RSA private key (encrypted with AES-256)
- Extract the corresponding public key
- Import the key pair to AWS EC2
- Create a backup copy

**Important**: You'll be prompted for a passphrase to encrypt the private key. Remember this passphrase!

### Step 3: Generate TLS Certificates

Create SSL certificates for inter-container communication:

```bash
./generate-certs.sh
```

This script will:
- Prompt for certificate details (country, organization, etc.)
- Generate a private key and self-signed certificate
- Create certificate bundles
- Upload all certificates to AWS Secrets Manager

### Step 4: Upload Application Secrets

Upload your product and platform credentials:

```bash
./upload-secrets.sh
```

The script will prompt for:
- Product Client ID
- Product Secret Key
- Platform Client ID  
- Platform Secret Key

Optionally, it can generate additional secrets:
- Database URL (placeholder)
- Redis Password
- JWT Signing Key
- Encryption Key

### Step 5: Configure Terraform Variables

Copy and customize the example configuration:

```bash
cp aws-ecs-secure-deployment.tfvars my-deployment.tfvars
```

Edit `my-deployment.tfvars` and update the following required values:

```hcl
# Your AWS region
aws_region = "us-west-2"

# Your existing VPC ID
existing_vpc_id = "vpc-0123456789abcdef0"

# Your existing subnet IDs
existing_private_subnet_ids = [
  "subnet-0123456789abcdef0",  # Private subnet 1
  "subnet-0fedcba9876543210"   # Private subnet 2
]

existing_public_subnet_ids = [
  "subnet-0abcdef0123456789",  # Public subnet 1
  "subnet-09876543210fedcba"   # Public subnet 2
]

# Your container image
ecs_container_image = "your-account.dkr.ecr.us-west-2.amazonaws.com/your-app:latest"

# ACM certificate ARN (if you have one)
ecs_acm_certificate_arn = "arn:aws:acm:us-west-2:123456789012:certificate/your-cert-id"
```

### Step 6: Deploy Infrastructure

Initialize and apply Terraform:

```bash
terraform init
terraform plan -var-file="my-deployment.tfvars"
terraform apply -var-file="my-deployment.tfvars"
```

## Configuration Details

### Secrets Management

All secrets are stored in AWS Secrets Manager under the prefix `secure-app/prod/`:

| Secret Path | Description |
|-------------|-------------|
| `/secure-app/prod/product_client_id` | Product service client ID |
| `/secure-app/prod/product_secret_key` | Product service secret key |
| `/secure-app/prod/platform_client_id` | Platform service client ID |
| `/secure-app/prod/platform_secret_key` | Platform service secret key |
| `/secure-app/prod/ssl_certificate` | TLS certificate for inter-container communication |
| `/secure-app/prod/ssl_private_key` | TLS private key |
| `/secure-app/prod/ssl_certificate_bundle` | Combined certificate and key |

### Accessing Secrets in Your Application

The secrets broker sidecar runs on port 8080 and provides these endpoints:

```bash
# Get individual secret
curl http://localhost:8080/secrets/product_client_id

# Get all secrets as JSON
curl http://localhost:8080/secrets

# Health check
curl http://localhost:8080/health

# Get SSL certificate
curl http://localhost:8080/certificate
```

### Example Application Code

**Node.js:**
```javascript
const fetch = require('node-fetch');

async function getSecret(secretName) {
  const response = await fetch(`http://localhost:8080/secrets/${secretName}`);
  return await response.text();
}

// Usage
const productClientId = await getSecret('product_client_id');
```

**Python:**
```python
import requests

def get_secret(secret_name):
    response = requests.get(f'http://localhost:8080/secrets/{secret_name}')
    return response.text

# Usage
product_client_id = get_secret('product_client_id')
```

**Go:**
```go
import (
    "fmt"
    "io/ioutil"
    "net/http"
)

func getSecret(secretName string) (string, error) {
    resp, err := http.Get(fmt.Sprintf("http://localhost:8080/secrets/%s", secretName))
    if err != nil {
        return "", err
    }
    defer resp.Body.Close()
    
    body, err := ioutil.ReadAll(resp.Body)
    return string(body), err
}
```

## Security Best Practices

### 1. Key Management
- Store private keys securely and never commit to version control
- Use strong passphrases for encrypted private keys
- Rotate keys regularly (recommend every 90 days)
- Use AWS Systems Manager Session Manager instead of SSH when possible

### 2. Secrets Management
- Use least-privilege IAM policies for secret access
- Enable AWS CloudTrail for secrets access logging
- Implement secret rotation for long-lived credentials
- Never log secret values in application logs

### 3. Network Security
- Use private subnets for ECS tasks
- Configure security groups with minimal required access
- Enable VPC Flow Logs for network monitoring
- Consider using AWS PrivateLink for service-to-service communication

### 4. Certificate Management
- Use ACM certificates for public-facing services
- Implement certificate rotation before expiry
- Monitor certificate expiration dates
- Use strong cipher suites and TLS 1.2+

## Troubleshooting

### Common Issues

**1. Key pair generation fails:**
```bash
# Check OpenSSL installation
openssl version

# Verify AWS CLI configuration
aws sts get-caller-identity
```

**2. Certificate upload fails:**
```bash
# Check AWS permissions
aws secretsmanager list-secrets --region us-west-2

# Verify certificate format
openssl x509 -in ssl_certificate.pem -text -noout
```

**3. ECS tasks fail to start:**
```bash
# Check ECS service events
aws ecs describe-services --cluster secure-app --services secure-app-service

# Check CloudWatch logs
aws logs describe-log-groups --log-group-name-prefix "/ecs/secure-app"
```

**4. Secrets broker not accessible:**
- Verify both containers are in the same task definition
- Check security group rules allow port 8080
- Ensure secrets sidecar is enabled in tfvars

### Debugging Steps

1. **Verify AWS Resources:**
   ```bash
   # Check if secrets exist
   aws secretsmanager list-secrets --region us-west-2 | grep secure-app
   
   # Check ECS cluster
   aws ecs describe-clusters --clusters secure-app
   
   # Check ALB
   aws elbv2 describe-load-balancers --names secure-app-alb
   ```

2. **Check ECS Task Logs:**
   ```bash
   # Get running tasks
   aws ecs list-tasks --cluster secure-app --service-name secure-app-service
   
   # Describe specific task
   aws ecs describe-tasks --cluster secure-app --tasks <task-arn>
   ```

3. **Validate Networking:**
   ```bash
   # Check VPC and subnets
   aws ec2 describe-vpcs --vpc-ids vpc-0123456789abcdef0
   aws ec2 describe-subnets --subnet-ids subnet-0123456789abcdef0
   
   # Check security groups
   aws ec2 describe-security-groups --group-names secure-app-ecs-sg
   ```

## Cleanup

To destroy all resources:

```bash
terraform destroy -var-file="my-deployment.tfvars"
```

**Note**: This will not delete secrets from AWS Secrets Manager. Delete them manually if needed:

```bash
aws secretsmanager delete-secret --secret-id secure-app/prod/product_client_id --force-delete-without-recovery
# Repeat for all secrets
```

## Support

For issues with this deployment:

1. Check the Terraform documentation
2. Review AWS ECS and Secrets Manager documentation
3. Verify AWS permissions and quotas
4. Check CloudWatch logs for detailed error messages

## Security Considerations

- All secrets are encrypted at rest in AWS Secrets Manager
- Inter-container communication uses TLS encryption
- ECS tasks run with minimal IAM permissions
- Network access is restricted to required ports only
- All components follow AWS security best practices

This deployment provides a secure, production-ready foundation for containerized applications with comprehensive secrets management and TLS communication.
</file>

<file path="examples/upload-secrets.sh">
#!/bin/bash

# Upload Product and Platform Credentials to AWS Secrets Manager
# This script securely uploads the required credentials for the ECS deployment

set -euo pipefail  # Exit on any error, undefined variable, or pipe failure

# Configuration
SECRETS_PREFIX="secure-app/prod"
AWS_REGION="us-west-2"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Helper functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Function to check if AWS CLI is installed and configured
check_aws_cli() {
    if ! command -v aws &> /dev/null; then
        log_error "AWS CLI is not installed. Please install it first."
        log_info "Install with: curl 'https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip' -o 'awscliv2.zip' && unzip awscliv2.zip && sudo ./aws/install"
        exit 1
    fi

    if ! aws sts get-caller-identity &> /dev/null; then
        log_error "AWS CLI is not configured or credentials are invalid."
        log_info "Configure with: aws configure"
        exit 1
    fi

    log_success "AWS CLI is configured and working"
}

# Function to prompt for secret input
prompt_for_secret() {
    local secret_name="$1"
    local prompt_text="$2"
    local secret_value=""
    
    echo
    log_info "$prompt_text"
    echo -n "Enter value: "
    read -s secret_value
    echo
    
    if [[ -z "$secret_value" ]]; then
        log_error "Secret value cannot be empty"
        return 1
    fi
    
    echo "$secret_value"
}

# Function to create or update a secret in AWS Secrets Manager
create_or_update_secret() {
    local secret_name="$1"
    local secret_value="$2"
    
    log_info "Processing secret: $secret_name"
    
    # Check if secret already exists
    if aws secretsmanager describe-secret --secret-id "$secret_name" --region "$AWS_REGION" &> /dev/null; then
        log_warning "Secret '$secret_name' already exists. Updating..."
        
        # Ask for confirmation before updating
        echo -n "Do you want to update this secret? (y/N): "
        read -r response
        if [[ ! "$response" =~ ^[Yy]$ ]]; then
            log_info "Skipping update for '$secret_name'"
            return 0
        fi
        
        # Update existing secret
        aws secretsmanager update-secret \
            --secret-id "$secret_name" \
            --secret-string "$secret_value" \
            --region "$AWS_REGION" > /dev/null
        
        log_success "Updated secret: $secret_name"
    else
        # Create new secret
        aws secretsmanager create-secret \
            --name "$secret_name" \
            --description "Managed by upload-secrets.sh script" \
            --secret-string "$secret_value" \
            --region "$AWS_REGION" > /dev/null
        
        log_success "Created secret: $secret_name"
    fi
}

# Function to generate a secure random string
generate_secure_string() {
    local length="${1:-32}"
    openssl rand -base64 "$length" | tr -d "=+/" | cut -c1-"$length"
}

# Main script
main() {
    echo "============================================"
    echo "AWS Secrets Manager Upload Script"
    echo "============================================"
    echo
    log_info "This script will upload product and platform credentials to AWS Secrets Manager"
    log_info "Secrets will be stored under prefix: $SECRETS_PREFIX"
    log_info "AWS Region: $AWS_REGION"
    echo
    
    # Check AWS CLI
    check_aws_cli
    
    # Confirm before proceeding
    echo -n "Do you want to continue? (y/N): "
    read -r response
    if [[ ! "$response" =~ ^[Yy]$ ]]; then
        log_info "Operation cancelled by user"
        exit 0
    fi
    
    echo
    log_info "Please provide the following credentials:"
    echo
    
    # Collect Product Credentials
    log_info "=== Product Credentials ==="
    PRODUCT_CLIENT_ID=$(prompt_for_secret "product_client_id" "Product Client ID")
    PRODUCT_SECRET_KEY=$(prompt_for_secret "product_secret_key" "Product Secret Key")
    
    echo
    log_info "=== Platform Credentials ==="
    PLATFORM_CLIENT_ID=$(prompt_for_secret "platform_client_id" "Platform Client ID") 
    PLATFORM_SECRET_KEY=$(prompt_for_secret "platform_secret_key" "Platform Secret Key")
    
    # Ask if user wants to generate additional secrets
    echo
    log_info "=== Additional Application Secrets ==="
    echo -n "Do you want to generate additional application secrets? (database_url, redis_password, jwt_signing_key, encryption_key) (y/N): "
    read -r generate_additional
    
    # Upload secrets to AWS Secrets Manager
    echo
    log_info "=== Uploading secrets to AWS Secrets Manager ==="
    
    create_or_update_secret "$SECRETS_PREFIX/product_client_id" "$PRODUCT_CLIENT_ID"
    create_or_update_secret "$SECRETS_PREFIX/product_secret_key" "$PRODUCT_SECRET_KEY"
    create_or_update_secret "$SECRETS_PREFIX/platform_client_id" "$PLATFORM_CLIENT_ID"
    create_or_update_secret "$SECRETS_PREFIX/platform_secret_key" "$PLATFORM_SECRET_KEY"
    
    # Generate and upload additional secrets if requested
    if [[ "$generate_additional" =~ ^[Yy]$ ]]; then
        log_info "Generating additional application secrets..."
        
        # Generate database URL (placeholder - user should update)
        DB_URL="postgresql://app_user:$(generate_secure_string 16)@localhost:5432/secure_app_db"
        create_or_update_secret "$SECRETS_PREFIX/database_url" "$DB_URL"
        
        # Generate Redis password
        REDIS_PASS=$(generate_secure_string 24)
        create_or_update_secret "$SECRETS_PREFIX/redis_password" "$REDIS_PASS"
        
        # Generate JWT signing key
        JWT_KEY=$(generate_secure_string 64)
        create_or_update_secret "$SECRETS_PREFIX/jwt_signing_key" "$JWT_KEY"
        
        # Generate encryption key
        ENCRYPT_KEY=$(generate_secure_string 32)
        create_or_update_secret "$SECRETS_PREFIX/encryption_key" "$ENCRYPT_KEY"
        
        log_warning "Note: The generated database_url is a placeholder. Please update it with your actual database connection string."
    fi
    
    echo
    log_success "All secrets have been uploaded successfully!"
    echo
    log_info "=== Next Steps ==="
    log_info "1. Run ./create-keypair.sh to generate EC2 keypair"
    log_info "2. Run ./generate-certs.sh to generate and upload TLS certificates"
    log_info "3. Update terraform.tfvars with your VPC and subnet IDs"
    log_info "4. Run terraform apply to deploy your infrastructure"
    echo
    log_info "=== Secrets Created ==="
    log_info "• $SECRETS_PREFIX/product_client_id"
    log_info "• $SECRETS_PREFIX/product_secret_key"
    log_info "• $SECRETS_PREFIX/platform_client_id"
    log_info "• $SECRETS_PREFIX/platform_secret_key"
    
    if [[ "$generate_additional" =~ ^[Yy]$ ]]; then
        log_info "• $SECRETS_PREFIX/database_url"
        log_info "• $SECRETS_PREFIX/redis_password"
        log_info "• $SECRETS_PREFIX/jwt_signing_key"
        log_info "• $SECRETS_PREFIX/encryption_key"
    fi
    
    echo
    log_warning "SECURITY REMINDER: These credentials are now stored in AWS Secrets Manager."
    log_warning "Make sure to configure proper IAM permissions for your ECS tasks to access these secrets."
    echo
}

# Handle script interruption
trap 'log_error "Script interrupted by user"; exit 1' INT TERM

# Run main function
main "$@"
</file>

<file path="modules/azure/outputs.tf">
output "cluster_id" {
  description = "The AKS cluster ID"
  value       = module.aks.cluster_id
}

output "cluster_name" {
  description = "The AKS cluster name"
  value       = module.aks.cluster_name
}

output "cluster_fqdn" {
  description = "The AKS cluster FQDN"
  value       = module.aks.cluster_fqdn
}

output "cluster_endpoint" {
  description = "The AKS cluster endpoint"
  value       = module.aks.cluster_endpoint
  sensitive   = true
}

output "kube_config" {
  description = "Kubernetes config for the cluster"
  value       = module.aks.kube_config
  sensitive   = true
}

output "resource_group_name" {
  description = "Resource group name"
  value       = module.vpc.resource_group_name
}

output "vnet_id" {
  description = "Virtual network ID"
  value       = module.vpc.vnet_id
}

output "aks_subnet_id" {
  description = "AKS subnet ID"
  value       = module.vpc.aks_subnet_id
}

output "identity_principal_id" {
  description = "The principal ID of the managed identity"
  value       = module.aks.identity_principal_id
}

output "identity_client_id" {
  description = "The client ID of the managed identity"
  value       = module.aks.identity_client_id
}

output "disk_encryption_set_id" {
  description = "The disk encryption set ID"
  value       = module.aks.disk_encryption_set_id
}

output "key_vault_key_id" {
  description = "The Key Vault key ID used for encryption"
  value       = module.aks.key_vault_key_id
}
</file>

<file path="modules/azure/variables.tf">
variable "cluster_name" {
  description = "Name of the AKS cluster"
  type        = string
}

variable "resource_group_name" {
  description = "Name of the resource group"
  type        = string
}

variable "location" {
  description = "Azure region"
  type        = string
}

variable "environment" {
  description = "Environment name"
  type        = string
}

variable "kubernetes_version" {
  description = "Kubernetes version"
  type        = string
  default     = "1.28"
}

variable "node_size_config" {
  description = "Node size configuration (small, medium, large)"
  type        = string
  default     = "small"
}

# Network configuration
variable "vnet_cidr" {
  description = "CIDR block for the virtual network"
  type        = string
  default     = "10.0.0.0/16"
}

variable "aks_subnet_cidr" {
  description = "CIDR block for AKS subnet"
  type        = string
  default     = "10.0.1.0/24"
}

variable "private_subnet_cidr" {
  description = "CIDR block for private subnet"
  type        = string
  default     = "10.0.2.0/24"
}

variable "bastion_subnet_cidr" {
  description = "CIDR block for bastion subnet"
  type        = string
  default     = "10.0.3.0/24"
}

variable "dns_service_ip" {
  description = "DNS service IP address"
  type        = string
  default     = "10.0.0.10"
}

variable "service_cidr" {
  description = "Service CIDR for Kubernetes services"
  type        = string
  default     = "10.0.0.0/16"
}

variable "network_plugin" {
  description = "Network plugin for AKS"
  type        = string
  default     = "azure"
}

variable "network_policy" {
  description = "Network policy for AKS"
  type        = string
  default     = "azure"
}

# Security and encryption
variable "key_vault_key_id" {
  description = "ID of existing Key Vault key for encryption"
  type        = string
  default     = null
}

variable "enable_host_encryption" {
  description = "Enable host encryption for nodes"
  type        = bool
  default     = true
}

variable "enable_azure_policy" {
  description = "Enable Azure Policy for AKS"
  type        = bool
  default     = true
}

variable "enable_microsoft_defender" {
  description = "Enable Microsoft Defender for Cloud"
  type        = bool
  default     = true
}

variable "enable_workload_identity" {
  description = "Enable workload identity"
  type        = bool
  default     = true
}

# Infrastructure features
variable "enable_nat_gateway" {
  description = "Enable NAT Gateway for outbound internet access"
  type        = bool
  default     = true
}

variable "enable_bastion" {
  description = "Enable Azure Bastion for secure remote access"
  type        = bool
  default     = false
}

variable "create_private_dns_zone" {
  description = "Create private DNS zone for AKS"
  type        = bool
  default     = true
}

variable "enable_log_analytics" {
  description = "Enable Log Analytics workspace"
  type        = bool
  default     = true
}

variable "log_retention_days" {
  description = "Log retention in days"
  type        = number
  default     = 30
}

# Optional resources
variable "container_registry_id" {
  description = "Azure Container Registry ID to grant pull access"
  type        = string
  default     = null
}

variable "workload_node_taints" {
  description = "Taints for workload node pool"
  type = list(object({
    key    = string
    value  = string
    effect = string
  }))
  default = []
}

variable "tags" {
  description = "Tags to apply to resources"
  type        = map(string)
  default     = {}
}
</file>

<file path="scripts/generate-certificates.sh">
#!/bin/bash

# generate-certificates.sh
# Script to generate self-signed SSL certificates for ECS HTTP server
# This script creates certificates that will be mounted to the container

set -euo pipefail

# Configuration
DOMAIN_NAME="http-server.example.local"
CERT_DIR="./certs"
KEY_SIZE=2048
DAYS=365

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check if OpenSSL is installed
check_openssl() {
    if ! command -v openssl &> /dev/null; then
        log_error "OpenSSL is not installed. Please install it first."
        log_info "On macOS: brew install openssl"
        log_info "On Ubuntu/Debian: sudo apt-get install openssl"
        log_info "On RHEL/CentOS: sudo yum install openssl"
        exit 1
    fi
    
    log_success "OpenSSL found: $(openssl version)"
}

# Create certificate directory
create_cert_directory() {
    if [ -d "$CERT_DIR" ]; then
        log_warning "Certificate directory already exists: $CERT_DIR"
        read -p "Do you want to overwrite existing certificates? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            log_info "Exiting without generating certificates."
            exit 0
        fi
        rm -rf "$CERT_DIR"
    fi
    
    mkdir -p "$CERT_DIR"
    log_success "Created certificate directory: $CERT_DIR"
}

# Generate private key
generate_private_key() {
    log_info "Generating private key..."
    
    openssl genrsa -out "$CERT_DIR/server.key" $KEY_SIZE
    chmod 600 "$CERT_DIR/server.key"
    
    log_success "Private key generated: $CERT_DIR/server.key"
}

# Generate certificate signing request
generate_csr() {
    log_info "Generating certificate signing request..."
    
    # Create a config file for the CSR to include Subject Alternative Names
    cat > "$CERT_DIR/csr.conf" <<EOF
[req]
default_bits = $KEY_SIZE
prompt = no
distinguished_name = dn
req_extensions = v3_req

[dn]
C = US
ST = California
L = San Francisco
O = Development
OU = Engineering
CN = $DOMAIN_NAME

[v3_req]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = $DOMAIN_NAME
DNS.2 = localhost
DNS.3 = *.example.local
IP.1 = 127.0.0.1
IP.2 = 10.0.0.0
EOF

    openssl req -new -key "$CERT_DIR/server.key" -out "$CERT_DIR/server.csr" -config "$CERT_DIR/csr.conf"
    
    log_success "Certificate signing request generated: $CERT_DIR/server.csr"
}

# Generate self-signed certificate
generate_certificate() {
    log_info "Generating self-signed certificate..."
    
    # Create certificate extensions file
    cat > "$CERT_DIR/cert.conf" <<EOF
[req]
default_bits = $KEY_SIZE
prompt = no
distinguished_name = dn
req_extensions = v3_req

[dn]
C = US
ST = California
L = San Francisco
O = Development
OU = Engineering
CN = $DOMAIN_NAME

[v3_req]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
extendedKeyUsage = serverAuth

[alt_names]
DNS.1 = $DOMAIN_NAME
DNS.2 = localhost
DNS.3 = *.example.local
IP.1 = 127.0.0.1
IP.2 = 10.0.0.0
EOF

    openssl x509 -req -in "$CERT_DIR/server.csr" \
        -signkey "$CERT_DIR/server.key" \
        -out "$CERT_DIR/server.crt" \
        -days $DAYS \
        -extensions v3_req \
        -extfile "$CERT_DIR/cert.conf"
    
    log_success "Self-signed certificate generated: $CERT_DIR/server.crt"
}

# Create certificate bundle
create_certificate_bundle() {
    log_info "Creating certificate bundle..."
    
    # Create a bundle with cert and key for easy deployment
    cat "$CERT_DIR/server.crt" "$CERT_DIR/server.key" > "$CERT_DIR/server-bundle.pem"
    
    # Create PEM files for different use cases
    cp "$CERT_DIR/server.crt" "$CERT_DIR/server.pem"
    
    log_success "Certificate bundle created: $CERT_DIR/server-bundle.pem"
}

# Generate CA certificate (for client trust)
generate_ca_certificate() {
    log_info "Generating CA certificate for client trust..."
    
    # Generate CA private key
    openssl genrsa -out "$CERT_DIR/ca.key" $KEY_SIZE
    chmod 600 "$CERT_DIR/ca.key"
    
    # Generate CA certificate
    openssl req -x509 -new -nodes -key "$CERT_DIR/ca.key" -sha256 -days $DAYS \
        -out "$CERT_DIR/ca.crt" -subj "/C=US/ST=CA/L=SF/O=Dev/OU=Engineering/CN=DevCA"
    
    log_success "CA certificate generated: $CERT_DIR/ca.crt"
    log_info "Install ca.crt in your browser/system to trust the certificates"
}

# Set appropriate permissions
set_permissions() {
    log_info "Setting appropriate file permissions..."
    
    # Private keys should be readable only by owner
    chmod 600 "$CERT_DIR"/*.key 2>/dev/null || true
    
    # Certificates can be readable by group
    chmod 644 "$CERT_DIR"/*.crt "$CERT_DIR"/*.pem 2>/dev/null || true
    
    log_success "File permissions set correctly"
}

# Verify certificates
verify_certificates() {
    log_info "Verifying generated certificates..."
    
    # Verify certificate
    if openssl x509 -in "$CERT_DIR/server.crt" -text -noout > /dev/null 2>&1; then
        log_success "Certificate is valid"
        
        # Show certificate details
        echo -e "\n${BLUE}Certificate Details:${NC}"
        openssl x509 -in "$CERT_DIR/server.crt" -text -noout | grep -A1 "Subject:"
        openssl x509 -in "$CERT_DIR/server.crt" -text -noout | grep -A5 "Subject Alternative Name:"
        openssl x509 -in "$CERT_DIR/server.crt" -text -noout | grep -A2 "Validity"
    else
        log_error "Generated certificate is invalid"
        exit 1
    fi
    
    # Verify private key
    if openssl rsa -in "$CERT_DIR/server.key" -check -noout > /dev/null 2>&1; then
        log_success "Private key is valid"
    else
        log_error "Generated private key is invalid"
        exit 1
    fi
    
    # Verify key-certificate pair
    cert_modulus=$(openssl x509 -noout -modulus -in "$CERT_DIR/server.crt" | openssl md5)
    key_modulus=$(openssl rsa -noout -modulus -in "$CERT_DIR/server.key" | openssl md5)
    
    if [ "$cert_modulus" = "$key_modulus" ]; then
        log_success "Certificate and private key match"
    else
        log_error "Certificate and private key do not match"
        exit 1
    fi
}

# Create nginx configuration template
create_nginx_config() {
    log_info "Creating nginx configuration template..."
    
    cat > "$CERT_DIR/nginx.conf" <<EOF
events {
    worker_connections 1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;
    
    # Logging
    log_format main '\$remote_addr - \$remote_user [\$time_local] "\$request" '
                    '\$status \$body_bytes_sent "\$http_referer" '
                    '"\$http_user_agent" "\$http_x_forwarded_for"';
    
    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;
    
    # Basic settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    
    # Security headers
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
    
    # Upstream for secrets sidecar
    upstream secrets_sidecar {
        server localhost:8080;
    }
    
    # HTTPS server
    server {
        listen 443 ssl http2;
        server_name $DOMAIN_NAME localhost;
        
        # SSL configuration
        ssl_certificate /etc/ssl/certs/server.crt;
        ssl_certificate_key /etc/ssl/private/server.key;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;
        ssl_prefer_server_ciphers off;
        ssl_session_cache shared:SSL:10m;
        ssl_session_timeout 10m;
        
        # Health check endpoint
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
        
        # Proxy to secrets sidecar (internal only)
        location /internal/secrets/ {
            internal;
            proxy_pass http://secrets_sidecar/;
            proxy_set_header Host \$host;
            proxy_set_header X-Real-IP \$remote_addr;
        }
        
        # Main application
        location / {
            root /usr/share/nginx/html;
            index index.html;
            
            # Example: Add secrets to headers (for demo purposes)
            # In production, secrets should be used by the application, not exposed in headers
            set \$db_password "";
            set \$api_key "";
            
            # Uncomment these lines to fetch secrets from sidecar
            # access_by_lua_block {
            #     local http = require "resty.http"
            #     local httpc = http.new()
            #     local res, err = httpc:request_uri("http://localhost:8080/secrets/database_password")
            #     if res and res.body then
            #         ngx.var.db_password = res.body
            #     end
            # }
        }
        
        # Error pages
        error_page 500 502 503 504 /50x.html;
        location = /50x.html {
            root /usr/share/nginx/html;
        }
    }
    
    # Redirect HTTP to HTTPS
    server {
        listen 80;
        server_name $DOMAIN_NAME localhost;
        return 301 https://\$server_name\$request_uri;
    }
}
EOF
    
    log_success "Nginx configuration template created: $CERT_DIR/nginx.conf"
}

# Create Dockerfile template
create_dockerfile() {
    log_info "Creating Dockerfile template..."
    
    cat > "$CERT_DIR/Dockerfile.http-server" <<EOF
# Multi-stage build for HTTP server with self-signed certificates
FROM nginx:alpine

# Install required packages
RUN apk add --no-cache curl openssl

# Create SSL directories
RUN mkdir -p /etc/ssl/certs /etc/ssl/private

# Copy SSL certificates
COPY certs/server.crt /etc/ssl/certs/
COPY certs/server.key /etc/ssl/private/
COPY certs/nginx.conf /etc/nginx/nginx.conf

# Set appropriate permissions
RUN chmod 644 /etc/ssl/certs/server.crt && \
    chmod 600 /etc/ssl/private/server.key && \
    chown root:root /etc/ssl/certs/server.crt /etc/ssl/private/server.key

# Create a simple index page
RUN echo '<h1>HTTP Server with Secrets Sidecar</h1><p>Server is running with HTTPS and secrets integration.</p>' > /usr/share/nginx/html/index.html

# Expose HTTPS port
EXPOSE 443

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -k -f https://localhost/health || exit 1

# Start nginx
CMD ["nginx", "-g", "daemon off;"]
EOF
    
    log_success "Dockerfile template created: $CERT_DIR/Dockerfile.http-server"
}

# Create AWS import script
create_aws_import_script() {
    log_info "Creating AWS certificate import script..."
    
    cat > "$CERT_DIR/import-to-acm.sh" <<EOF
#!/bin/bash

# import-to-acm.sh
# Script to import self-signed certificate to AWS Certificate Manager

set -euo pipefail

CERT_FILE="./server.crt"
KEY_FILE="./server.key"
REGION="us-west-2"

# Check if AWS CLI is installed
if ! command -v aws &> /dev/null; then
    echo "AWS CLI is not installed. Please install it first."
    exit 1
fi

# Check if certificate files exist
if [[ ! -f "\$CERT_FILE" ]] || [[ ! -f "\$KEY_FILE" ]]; then
    echo "Certificate files not found. Please run generate-certificates.sh first."
    exit 1
fi

# Import certificate to ACM
echo "Importing certificate to AWS Certificate Manager..."

CERT_ARN=\$(aws acm import-certificate \
    --certificate fileb://\$CERT_FILE \
    --private-key fileb://\$KEY_FILE \
    --region \$REGION \
    --query 'CertificateArn' \
    --output text)

echo "Certificate imported successfully!"
echo "Certificate ARN: \$CERT_ARN"
echo ""
echo "You can now use this certificate ARN in your Terraform configuration:"
echo "ecs_acm_certificate_arn = \"\$CERT_ARN\""
EOF
    
    chmod +x "$CERT_DIR/import-to-acm.sh"
    log_success "AWS import script created: $CERT_DIR/import-to-acm.sh"
}

# Print summary
print_summary() {
    echo -e "\n${GREEN}=================== CERTIFICATE GENERATION COMPLETE ===================${NC}"
    echo -e "\n${BLUE}Generated Files:${NC}"
    echo "📁 Certificate directory: $CERT_DIR"
    echo "🔐 Private key: $CERT_DIR/server.key"
    echo "📜 Certificate: $CERT_DIR/server.crt"
    echo "📦 Bundle: $CERT_DIR/server-bundle.pem"
    echo "🏛️  CA certificate: $CERT_DIR/ca.crt"
    echo "⚙️  Nginx config: $CERT_DIR/nginx.conf"
    echo "🐳 Dockerfile: $CERT_DIR/Dockerfile.http-server"
    echo "☁️  AWS import script: $CERT_DIR/import-to-acm.sh"
    
    echo -e "\n${BLUE}Next Steps:${NC}"
    echo "1. 📋 Review certificate details above"
    echo "2. 🚀 Use certificates in your ECS deployment"
    echo "3. 🔧 Customize nginx.conf for your application"
    echo "4. 🐳 Build Docker image using the Dockerfile template"
    echo "5. ☁️  Import to ACM: cd $CERT_DIR && ./import-to-acm.sh"
    
    echo -e "\n${YELLOW}Security Notes:${NC}"
    echo "⚠️  Self-signed certificates are for development/testing only"
    echo "🔒 Private key permissions are set to 600 (owner read only)"
    echo "🏢 For production, use certificates from a trusted CA"
    echo "🔄 Consider setting up certificate rotation"
    
    echo -e "\n${BLUE}Certificate Details:${NC}"
    echo "🌐 Domain: $DOMAIN_NAME"
    echo "📅 Valid for: $DAYS days"
    echo "🔑 Key size: $KEY_SIZE bits"
    echo "✅ Includes Subject Alternative Names for flexibility"
    
    echo -e "\n${GREEN}Certificate generation completed successfully!${NC}"
}

# Main execution
main() {
    echo -e "${GREEN}=================== SSL CERTIFICATE GENERATOR ===================${NC}"
    echo -e "${BLUE}Generating self-signed SSL certificates for ECS HTTP server${NC}"
    echo -e "${BLUE}Domain: $DOMAIN_NAME${NC}"
    echo -e "${BLUE}Certificate directory: $CERT_DIR${NC}\n"
    
    check_openssl
    create_cert_directory
    generate_private_key
    generate_csr
    generate_certificate
    create_certificate_bundle
    generate_ca_certificate
    set_permissions
    verify_certificates
    create_nginx_config
    create_dockerfile
    create_aws_import_script
    print_summary
}

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -d|--domain)
            DOMAIN_NAME="$2"
            shift 2
            ;;
        -o|--output)
            CERT_DIR="$2"
            shift 2
            ;;
        -k|--key-size)
            KEY_SIZE="$2"
            shift 2
            ;;
        --days)
            DAYS="$2"
            shift 2
            ;;
        -h|--help)
            echo "Usage: $0 [OPTIONS]"
            echo ""
            echo "Generate self-signed SSL certificates for ECS HTTP server"
            echo ""
            echo "Options:"
            echo "  -d, --domain DOMAIN     Domain name for the certificate (default: http-server.example.local)"
            echo "  -o, --output DIR        Output directory for certificates (default: ./certs)"
            echo "  -k, --key-size SIZE     RSA key size in bits (default: 2048)"
            echo "      --days DAYS         Certificate validity in days (default: 365)"
            echo "  -h, --help              Show this help message"
            echo ""
            echo "Examples:"
            echo "  $0                                    # Use defaults"
            echo "  $0 -d myapp.local -o ./ssl           # Custom domain and output directory"
            echo "  $0 -k 4096 --days 730                # 4096-bit key, valid for 2 years"
            exit 0
            ;;
        *)
            log_error "Unknown option: $1"
            echo "Use -h or --help for usage information"
            exit 1
            ;;
    esac
done

# Run main function
main
</file>

<file path="scripts/setup-secrets.sh">
#!/bin/bash

# setup-secrets.sh
# Script to create dummy secrets in AWS Secrets Manager for ECS HTTP server
# This script creates secrets that the sidecar container will fetch

set -euo pipefail

# Configuration
REGION="us-west-2"
SECRET_PREFIX="http-server/"
ENVIRONMENT="development"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check if AWS CLI is installed
check_aws_cli() {
    if ! command -v aws &> /dev/null; then
        log_error "AWS CLI is not installed. Please install it first."
        log_info "Installation instructions:"
        log_info "  macOS: brew install awscli"
        log_info "  Linux: pip install awscli"
        log_info "  Windows: Download from https://aws.amazon.com/cli/"
        exit 1
    fi
    
    log_success "AWS CLI found: $(aws --version)"
}

# Check AWS credentials
check_aws_credentials() {
    if ! aws sts get-caller-identity &> /dev/null; then
        log_error "AWS credentials not configured or invalid."
        log_info "Configure credentials using one of:"
        log_info "  aws configure"
        log_info "  export AWS_ACCESS_KEY_ID=your_access_key"
        log_info "  export AWS_SECRET_ACCESS_KEY=your_secret_key"
        log_info "  Use IAM roles or AWS SSO"
        exit 1
    fi
    
    local caller_identity=$(aws sts get-caller-identity --query 'Account' --output text)
    log_success "AWS credentials configured for account: $caller_identity"
}

# Generate secure random password
generate_password() {
    local length=${1:-32}
    # Generate a secure random password with mixed case, numbers, and safe symbols
    openssl rand -base64 $((length * 3 / 4)) | tr -d "=+/" | cut -c1-$length
}

# Generate API key
generate_api_key() {
    local prefix=${1:-"sk"}
    echo "${prefix}-$(openssl rand -hex 16)"
}

# Create secret in AWS Secrets Manager
create_secret() {
    local secret_name="$1"
    local secret_value="$2"
    local description="$3"
    
    local full_secret_name="${SECRET_PREFIX}${secret_name}"
    
    log_info "Creating secret: $full_secret_name"
    
    # Check if secret already exists
    if aws secretsmanager describe-secret --secret-id "$full_secret_name" --region "$REGION" &> /dev/null; then
        log_warning "Secret $full_secret_name already exists"
        
        # Ask user if they want to update it
        read -p "Do you want to update the existing secret? (y/N): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            aws secretsmanager update-secret \
                --secret-id "$full_secret_name" \
                --secret-string "$secret_value" \
                --region "$REGION" \
                --description "$description" \
                > /dev/null
            log_success "Updated secret: $full_secret_name"
        else
            log_info "Skipped updating secret: $full_secret_name"
        fi
    else
        # Create new secret
        aws secretsmanager create-secret \
            --name "$full_secret_name" \
            --secret-string "$secret_value" \
            --region "$REGION" \
            --description "$description" \
            --tags "[{\"Key\":\"Environment\",\"Value\":\"$ENVIRONMENT\"},{\"Key\":\"ManagedBy\",\"Value\":\"terraform\"},{\"Key\":\"Purpose\",\"Value\":\"ecs-http-server\"}]" \
            > /dev/null
        log_success "Created secret: $full_secret_name"
    fi
}

# Create JSON secret in AWS Secrets Manager
create_json_secret() {
    local secret_name="$1"
    local secret_json="$2"
    local description="$3"
    
    local full_secret_name="${SECRET_PREFIX}${secret_name}"
    
    log_info "Creating JSON secret: $full_secret_name"
    
    # Check if secret already exists
    if aws secretsmanager describe-secret --secret-id "$full_secret_name" --region "$REGION" &> /dev/null; then
        log_warning "Secret $full_secret_name already exists"
        
        # Ask user if they want to update it
        read -p "Do you want to update the existing secret? (y/N): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            aws secretsmanager update-secret \
                --secret-id "$full_secret_name" \
                --secret-string "$secret_json" \
                --region "$REGION" \
                --description "$description" \
                > /dev/null
            log_success "Updated JSON secret: $full_secret_name"
        else
            log_info "Skipped updating secret: $full_secret_name"
        fi
    else
        # Create new secret
        aws secretsmanager create-secret \
            --name "$full_secret_name" \
            --secret-string "$secret_json" \
            --region "$REGION" \
            --description "$description" \
            --tags "[{\"Key\":\"Environment\",\"Value\":\"$ENVIRONMENT\"},{\"Key\":\"ManagedBy\",\"Value\":\"terraform\"},{\"Key\":\"Purpose\",\"Value\":\"ecs-http-server\"}]" \
            > /dev/null
        log_success "Created JSON secret: $full_secret_name"
    fi
}

# Create individual secrets
create_individual_secrets() {
    log_info "Creating individual secrets..."
    
    # Database credentials
    local db_password=$(generate_password 24)
    create_secret "database_password" "$db_password" "Database password for HTTP server"
    
    # API keys
    local api_key=$(generate_api_key "sk")
    create_secret "api_key" "$api_key" "External API key for third-party services"
    
    local third_party_key=$(generate_api_key "tpk")
    create_secret "third_party_api_key" "$third_party_key" "Third-party service API key"
    
    # JWT signing key (base64 encoded for security)
    local jwt_key=$(openssl rand -base64 64 | tr -d '\n')
    create_secret "jwt_signing_key" "$jwt_key" "JWT token signing key (base64 encoded)"
    
    # Redis password
    local redis_password=$(generate_password 20)
    create_secret "redis_password" "$redis_password" "Redis instance password"
    
    # Encryption key (32 bytes for AES-256, base64 encoded)
    local encryption_key=$(openssl rand -base64 32 | tr -d '\n')
    create_secret "encryption_key" "$encryption_key" "AES-256 encryption key (base64 encoded)"
    
    # OAuth credentials
    local oauth_secret=$(generate_password 40)
    create_secret "oauth_client_secret" "$oauth_secret" "OAuth client secret for authentication"
    
    # SMTP password
    local smtp_password=$(generate_password 16)
    create_secret "smtp_password" "$smtp_password" "SMTP server password for email delivery"
    
    log_success "Individual secrets created successfully"
}

# Create database configuration as JSON secret
create_database_config() {
    log_info "Creating database configuration secret..."
    
    local db_host="db.example.internal"
    local db_port="5432"
    local db_name="httpserver"
    local db_user="app_user"
    local db_password=$(generate_password 24)
    
    local db_config=$(cat <<EOF
{
    "host": "$db_host",
    "port": $db_port,
    "database": "$db_name",
    "username": "$db_user",
    "password": "$db_password",
    "ssl_mode": "require",
    "max_connections": 20,
    "connection_timeout": 30
}
EOF
)
    
    create_json_secret "database_config" "$db_config" "Database configuration (JSON) for HTTP server"
    
    log_success "Database configuration secret created"
}

# Create Redis configuration as JSON secret
create_redis_config() {
    log_info "Creating Redis configuration secret..."
    
    local redis_host="redis.example.internal"
    local redis_port="6379"
    local redis_password=$(generate_password 20)
    
    local redis_config=$(cat <<EOF
{
    "host": "$redis_host",
    "port": $redis_port,
    "password": "$redis_password",
    "db": 0,
    "ssl": true,
    "max_connections": 10,
    "timeout": 5
}
EOF
)
    
    create_json_secret "redis_config" "$redis_config" "Redis configuration (JSON) for HTTP server"
    
    log_success "Redis configuration secret created"
}

# Create OAuth configuration as JSON secret
create_oauth_config() {
    log_info "Creating OAuth configuration secret..."
    
    local client_id="http_server_$(openssl rand -hex 8)"
    local client_secret=$(generate_password 40)
    local auth_url="https://auth.example.com/oauth/authorize"
    local token_url="https://auth.example.com/oauth/token"
    
    local oauth_config=$(cat <<EOF
{
    "client_id": "$client_id",
    "client_secret": "$client_secret",
    "auth_url": "$auth_url",
    "token_url": "$token_url",
    "scope": "read write admin",
    "redirect_uri": "https://http-server.example.local/oauth/callback"
}
EOF
)
    
    create_json_secret "oauth_config" "$oauth_config" "OAuth configuration (JSON) for HTTP server"
    
    log_success "OAuth configuration secret created"
}

# Create external service credentials
create_external_service_secrets() {
    log_info "Creating external service secrets..."
    
    # AWS S3 credentials (for file storage)
    local s3_access_key="AKIA$(openssl rand -hex 8 | tr '[:lower:]' '[:upper:]')"
    local s3_secret_key=$(openssl rand -base64 30 | tr -d '\n')
    
    local s3_config=$(cat <<EOF
{
    "access_key_id": "$s3_access_key",
    "secret_access_key": "$s3_secret_key",
    "region": "$REGION",
    "bucket": "http-server-storage-bucket",
    "prefix": "uploads/"
}
EOF
)
    
    create_json_secret "s3_credentials" "$s3_config" "AWS S3 credentials for file storage"
    
    # Email service configuration
    local email_config=$(cat <<EOF
{
    "smtp_host": "smtp.example.com",
    "smtp_port": 587,
    "username": "http-server@example.com",
    "password": "$(generate_password 16)",
    "use_tls": true,
    "from_email": "noreply@example.com",
    "from_name": "HTTP Server Application"
}
EOF
)
    
    create_json_secret "email_config" "$email_config" "Email service configuration"
    
    # Monitoring/analytics service API key
    local monitoring_key=$(generate_api_key "mon")
    create_secret "monitoring_api_key" "$monitoring_key" "Monitoring service API key"
    
    log_success "External service secrets created"
}

# Create feature flags configuration
create_feature_flags() {
    log_info "Creating feature flags configuration..."
    
    local feature_flags=$(cat <<EOF
{
    "enable_new_ui": true,
    "enable_analytics": true,
    "enable_cache": true,
    "enable_debug_mode": false,
    "max_upload_size_mb": 10,
    "rate_limit_per_minute": 100,
    "maintenance_mode": false,
    "beta_features": {
        "advanced_search": true,
        "real_time_updates": false,
        "ai_suggestions": true
    }
}
EOF
)
    
    create_json_secret "feature_flags" "$feature_flags" "Feature flags configuration for HTTP server"
    
    log_success "Feature flags configuration created"
}

# List created secrets
list_created_secrets() {
    log_info "Listing created secrets..."
    
    echo -e "\n${BLUE}Created Secrets in AWS Secrets Manager:${NC}"
    
    # Get all secrets with the prefix
    local secrets=$(aws secretsmanager list-secrets \
        --region "$REGION" \
        --query "SecretList[?starts_with(Name, '${SECRET_PREFIX}')].{Name:Name,Description:Description}" \
        --output table)
    
    if [ -n "$secrets" ]; then
        echo "$secrets"
    else
        log_warning "No secrets found with prefix: $SECRET_PREFIX"
    fi
}

# Create IAM policy for ECS tasks to access secrets
create_iam_policy() {
    log_info "Creating IAM policy for secrets access..."
    
    local policy_name="ECS-HTTP-Server-Secrets-Policy"
    local policy_document=$(cat <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "secretsmanager:GetSecretValue",
                "secretsmanager:DescribeSecret"
            ],
            "Resource": [
                "arn:aws:secretsmanager:${REGION}:*:secret:${SECRET_PREFIX}*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "kms:Decrypt"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "kms:ViaService": "secretsmanager.${REGION}.amazonaws.com"
                }
            }
        }
    ]
}
EOF
)
    
    # Create policy file
    echo "$policy_document" > /tmp/secrets-policy.json
    
    # Check if policy already exists
    if aws iam get-policy --policy-arn "arn:aws:iam::$(aws sts get-caller-identity --query 'Account' --output text):policy/$policy_name" &> /dev/null; then
        log_warning "IAM policy $policy_name already exists"
        log_info "You may want to update it manually or delete and recreate"
    else
        # Create the policy
        local policy_arn=$(aws iam create-policy \
            --policy-name "$policy_name" \
            --policy-document file:///tmp/secrets-policy.json \
            --description "Policy for ECS HTTP server to access secrets" \
            --query 'Policy.Arn' \
            --output text)
        
        log_success "IAM policy created: $policy_arn"
        
        echo -e "\n${YELLOW}Important:${NC} Attach this policy to your ECS task role"
        echo "Policy ARN: $policy_arn"
    fi
    
    # Clean up temp file
    rm -f /tmp/secrets-policy.json
}

# Create sample secrets sidecar application
create_sidecar_example() {
    log_info "Creating sample secrets sidecar application..."
    
    mkdir -p ./sidecar
    
    # Create Python sidecar application
    cat > ./sidecar/app.py <<EOF
#!/usr/bin/env python3
"""
Secrets Sidecar Application
A simple HTTP server that fetches secrets from AWS Secrets Manager
and provides them to other containers in the same task.
"""

import json
import logging
import os
import time
from typing import Optional, Dict, Any

import boto3
from flask import Flask, jsonify, request
from botocore.exceptions import ClientError, NoCredentialsError

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
REGION = os.getenv('AWS_DEFAULT_REGION', '${REGION}')
SECRET_PREFIX = os.getenv('SECRET_PREFIX', '${SECRET_PREFIX}')
PORT = int(os.getenv('PORT', '8080'))
CACHE_TTL = int(os.getenv('CACHE_TTL', '300'))  # 5 minutes

# Initialize Flask app
app = Flask(__name__)

# Initialize AWS Secrets Manager client
try:
    secrets_client = boto3.client('secretsmanager', region_name=REGION)
    logger.info(f"Initialized Secrets Manager client for region: {REGION}")
except NoCredentialsError:
    logger.error("AWS credentials not found")
    secrets_client = None

# In-memory cache for secrets
secrets_cache: Dict[str, Dict[str, Any]] = {}


def get_secret_from_aws(secret_name: str) -> Optional[str]:
    """Fetch secret from AWS Secrets Manager"""
    if not secrets_client:
        logger.error("Secrets Manager client not initialized")
        return None
    
    try:
        full_secret_name = f"{SECRET_PREFIX}{secret_name}"
        logger.info(f"Fetching secret: {full_secret_name}")
        
        response = secrets_client.get_secret_value(SecretId=full_secret_name)
        return response['SecretString']
    
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == 'ResourceNotFoundException':
            logger.error(f"Secret not found: {secret_name}")
        elif error_code == 'AccessDenied':
            logger.error(f"Access denied for secret: {secret_name}")
        else:
            logger.error(f"Error fetching secret {secret_name}: {e}")
        return None
    
    except Exception as e:
        logger.error(f"Unexpected error fetching secret {secret_name}: {e}")
        return None


def get_cached_secret(secret_name: str) -> Optional[str]:
    """Get secret from cache or fetch from AWS if not cached or expired"""
    now = time.time()
    
    # Check if secret is in cache and not expired
    if secret_name in secrets_cache:
        cached_data = secrets_cache[secret_name]
        if now - cached_data['timestamp'] < CACHE_TTL:
            logger.debug(f"Returning cached secret: {secret_name}")
            return cached_data['value']
        else:
            logger.debug(f"Cache expired for secret: {secret_name}")
    
    # Fetch from AWS
    secret_value = get_secret_from_aws(secret_name)
    
    if secret_value is not None:
        # Cache the secret
        secrets_cache[secret_name] = {
            'value': secret_value,
            'timestamp': now
        }
        logger.info(f"Cached secret: {secret_name}")
    
    return secret_value


@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'timestamp': time.time(),
        'cache_size': len(secrets_cache)
    })


@app.route('/secrets/<secret_name>', methods=['GET'])
def get_secret(secret_name: str):
    """Get a specific secret"""
    if not secret_name:
        return jsonify({'error': 'Secret name is required'}), 400
    
    secret_value = get_cached_secret(secret_name)
    
    if secret_value is None:
        return jsonify({'error': f'Secret not found: {secret_name}'}), 404
    
    # Try to parse as JSON, return as string if not JSON
    try:
        parsed_value = json.loads(secret_value)
        return jsonify({
            'secret_name': secret_name,
            'value': parsed_value,
            'type': 'json'
        })
    except json.JSONDecodeError:
        return jsonify({
            'secret_name': secret_name,
            'value': secret_value,
            'type': 'string'
        })


@app.route('/secrets', methods=['GET'])
def list_secrets():
    """List all cached secrets (names only, not values)"""
    return jsonify({
        'cached_secrets': list(secrets_cache.keys()),
        'cache_ttl': CACHE_TTL,
        'region': REGION,
        'secret_prefix': SECRET_PREFIX
    })


@app.route('/cache/clear', methods=['POST'])
def clear_cache():
    """Clear the secrets cache"""
    secrets_cache.clear()
    logger.info("Secrets cache cleared")
    return jsonify({'message': 'Cache cleared successfully'})


@app.route('/cache/refresh/<secret_name>', methods=['POST'])
def refresh_secret(secret_name: str):
    """Force refresh a specific secret"""
    if secret_name in secrets_cache:
        del secrets_cache[secret_name]
    
    secret_value = get_cached_secret(secret_name)
    
    if secret_value is None:
        return jsonify({'error': f'Secret not found: {secret_name}'}), 404
    
    return jsonify({
        'message': f'Secret refreshed: {secret_name}',
        'timestamp': time.time()
    })


@app.errorhandler(Exception)
def handle_exception(e):
    """Global exception handler"""
    logger.error(f"Unhandled exception: {e}", exc_info=True)
    return jsonify({'error': 'Internal server error'}), 500


if __name__ == '__main__':
    logger.info(f"Starting secrets sidecar on port {PORT}")
    logger.info(f"Region: {REGION}")
    logger.info(f"Secret prefix: {SECRET_PREFIX}")
    logger.info(f"Cache TTL: {CACHE_TTL} seconds")
    
    app.run(host='0.0.0.0', port=PORT, debug=False)
EOF

    # Create requirements.txt
    cat > ./sidecar/requirements.txt <<EOF
Flask==2.3.3
boto3==1.34.0
botocore==1.34.0
Werkzeug==2.3.7
EOF

    # Create Dockerfile for sidecar
    cat > ./sidecar/Dockerfile <<EOF
FROM python:3.11-alpine

# Install required packages
RUN apk add --no-cache curl

# Set working directory
WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY app.py .

# Create non-root user
RUN adduser -D -s /bin/sh appuser
USER appuser

# Expose port
EXPOSE 8080

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=10s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# Start application
CMD ["python", "app.py"]
EOF

    # Create docker-compose for local testing
    cat > ./sidecar/docker-compose.yml <<EOF
version: '3.8'

services:
  secrets-sidecar:
    build: .
    ports:
      - "8080:8080"
    environment:
      - AWS_DEFAULT_REGION=${REGION}
      - SECRET_PREFIX=${SECRET_PREFIX}
      - PORT=8080
      - CACHE_TTL=300
    # Mount AWS credentials for local testing
    volumes:
      - ~/.aws:/home/appuser/.aws:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
EOF

    # Create README for sidecar
    cat > ./sidecar/README.md <<EOF
# Secrets Sidecar Application

A lightweight HTTP server that fetches secrets from AWS Secrets Manager and provides them to other containers.

## Features

- Fetches secrets from AWS Secrets Manager
- In-memory caching with configurable TTL
- JSON and string secret support
- Health check endpoint
- RESTful API for secret retrieval

## API Endpoints

- \`GET /health\` - Health check
- \`GET /secrets/<name>\` - Get specific secret
- \`GET /secrets\` - List cached secrets
- \`POST /cache/clear\` - Clear cache
- \`POST /cache/refresh/<name>\` - Refresh specific secret

## Environment Variables

- \`AWS_DEFAULT_REGION\` - AWS region (default: ${REGION})
- \`SECRET_PREFIX\` - Secrets prefix (default: ${SECRET_PREFIX})
- \`PORT\` - Server port (default: 8080)
- \`CACHE_TTL\` - Cache TTL in seconds (default: 300)

## Local Testing

\`\`\`bash
# Build and run
docker-compose up --build

# Test endpoints
curl http://localhost:8080/health
curl http://localhost:8080/secrets/database_password
\`\`\`

## Deployment

This sidecar is designed to run alongside your main application container in the same ECS task.
EOF

    chmod +x ./sidecar/app.py
    log_success "Sample secrets sidecar created in ./sidecar/"
}

# Print summary
print_summary() {
    echo -e "\n${GREEN}=================== SECRETS SETUP COMPLETE ===================${NC}"
    
    echo -e "\n${BLUE}Created Secrets:${NC}"
    echo "🔐 Individual secrets (8 total)"
    echo "📊 Database configuration (JSON)"
    echo "🗄️  Redis configuration (JSON)"
    echo "🔑 OAuth configuration (JSON)"
    echo "☁️  S3 credentials (JSON)"
    echo "📧 Email configuration (JSON)"
    echo "📊 Monitoring API key"
    echo "🎛️  Feature flags (JSON)"
    
    echo -e "\n${BLUE}Created IAM Resources:${NC}"
    echo "📜 IAM policy for ECS task secrets access"
    
    echo -e "\n${BLUE}Sample Applications:${NC}"
    echo "🐍 Python secrets sidecar (./sidecar/)"
    echo "🐳 Docker configuration"
    echo "📝 API documentation"
    
    echo -e "\n${BLUE}Next Steps:${NC}"
    echo "1. 📋 Review the created secrets in AWS Console"
    echo "2. 🔗 Attach the IAM policy to your ECS task role"
    echo "3. 🧪 Test the sidecar application locally"
    echo "4. 🐳 Build and push Docker images to ECR"
    echo "5. 🚀 Deploy using Terraform"
    
    echo -e "\n${BLUE}Secret Access:${NC}"
    echo "🌐 Region: $REGION"
    echo "📁 Prefix: $SECRET_PREFIX"
    echo "🏷️  Tags: Environment=$ENVIRONMENT, ManagedBy=terraform"
    
    echo -e "\n${YELLOW}Security Notes:${NC}"
    echo "⚠️  Generated passwords are cryptographically secure"
    echo "🔒 Secrets are encrypted at rest in AWS"
    echo "🎯 IAM policy follows least-privilege principle"
    echo "⏰ Sidecar implements caching to reduce API calls"
    echo "🔄 Consider implementing secret rotation"
    
    echo -e "\n${GREEN}Secrets setup completed successfully!${NC}"
}

# Main execution
main() {
    echo -e "${GREEN}=================== AWS SECRETS MANAGER SETUP ===================${NC}"
    echo -e "${BLUE}Setting up dummy secrets for ECS HTTP server${NC}"
    echo -e "${BLUE}Region: $REGION${NC}"
    echo -e "${BLUE}Secret prefix: $SECRET_PREFIX${NC}\n"
    
    check_aws_cli
    check_aws_credentials
    create_individual_secrets
    create_database_config
    create_redis_config
    create_oauth_config
    create_external_service_secrets
    create_feature_flags
    create_iam_policy
    create_sidecar_example
    list_created_secrets
    print_summary
}

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -r|--region)
            REGION="$2"
            shift 2
            ;;
        -p|--prefix)
            SECRET_PREFIX="$2"
            shift 2
            ;;
        -e|--environment)
            ENVIRONMENT="$2"
            shift 2
            ;;
        --dry-run)
            log_info "Dry run mode - no secrets will be created"
            DRY_RUN=true
            shift
            ;;
        -h|--help)
            echo "Usage: $0 [OPTIONS]"
            echo ""
            echo "Create dummy secrets in AWS Secrets Manager for ECS HTTP server"
            echo ""
            echo "Options:"
            echo "  -r, --region REGION      AWS region (default: us-west-2)"
            echo "  -p, --prefix PREFIX      Secret name prefix (default: http-server/)"
            echo "  -e, --environment ENV    Environment tag (default: development)"
            echo "      --dry-run           Show what would be created without creating"
            echo "  -h, --help              Show this help message"
            echo ""
            echo "Examples:"
            echo "  $0                                    # Use defaults"
            echo "  $0 -r us-east-1 -p myapp/            # Custom region and prefix"
            echo "  $0 -e production                     # Production environment"
            exit 0
            ;;
        *)
            log_error "Unknown option: $1"
            echo "Use -h or --help for usage information"
            exit 1
            ;;
    esac
done

# Check if dry run mode
if [ "${DRY_RUN:-false}" = "true" ]; then
    log_warning "Dry run mode - no actual secrets will be created"
    log_info "Would create secrets with prefix: $SECRET_PREFIX"
    log_info "Would use region: $REGION"
    log_info "Would tag with environment: $ENVIRONMENT"
    exit 0
fi

# Run main function
main
</file>

<file path="environments/dev-azure.tfvars">
# Development environment - Azure AKS
cloud_provider      = "azure"
cluster_name        = "dev-aks-cluster"
environment         = "dev"
node_size_config    = "small"
resource_group_name = "dev-k8s-rg"
location            = "East US"

tags = {
  Environment = "dev"
  Project     = "kubernetes-platform"
  Team        = "platform-engineering"
  CostCenter  = "engineering"
}
</file>

<file path="environments/perf-azure.tfvars">
# Performance testing environment - Azure AKS
cloud_provider      = "azure"
cluster_name        = "perf-aks-cluster"
environment         = "perf"
node_size_config    = "large"
resource_group_name = "perf-k8s-rg"
location            = "East US"

tags = {
  Environment = "performance"
  Project     = "kubernetes-platform"
  Team        = "qa-engineering"
  CostCenter  = "engineering"
}
</file>

<file path="environments/prod-azure.tfvars">
# Production environment - Azure AKS
cloud_provider      = "azure"
cluster_name        = "prod-aks-cluster"
environment         = "prod"
node_size_config    = "large"
resource_group_name = "prod-k8s-rg"
location            = "East US"

tags = {
  Environment = "production"
  Project     = "kubernetes-platform"
  Team        = "platform-engineering"
  CostCenter  = "production"
  Backup      = "required"
}
</file>

<file path="examples/aws-ecs-dual-ecr-services.tfvars">
# Example: Dual ECS Services from ECR with Secrets Sidecar
# This example deploys two services from ECR registry where images already exist:
# - Service 1: Main application with secrets and certificates via sidecar (4 vCPU, 4GB RAM)
# - Service 2: Lightweight service without secrets sidecar (200m vCPU, 200MB RAM)

# Basic Configuration
cloud_provider = "aws"
aws_region     = "us-west-2"
cluster_name   = "dual-ecr-services"
environment    = "production"

# Module Control - Enable ECS only
enable_eks           = false
enable_ecs           = true
enable_bastion       = true
enable_nat_gateway   = true
enable_vpc_endpoints = true

# VPC Configuration
create_vpc               = true
vpc_cidr                 = "10.0.0.0/16"
availability_zones_count = 3

# Main Service Configuration (Service 1 - with secrets sidecar)
# Uses 4 vCPU and 4GB RAM, gets secrets and certs via sidecar
ecs_container_image        = "123456789012.dkr.ecr.us-west-2.amazonaws.com/my-main-app:latest"
ecs_container_port         = 50001
ecs_task_cpu               = 4096 # 4 vCPU
ecs_task_memory            = 4096 # 4GB RAM
ecs_desired_count          = 2
ecs_enable_secrets_sidecar = true

# Secrets Configuration for Service 1
ecs_secrets_prefix = "main-app/"
ecs_secrets = {
  database_password   = "super-secret-db-password"
  api_key             = "your-external-api-key"
  encryption_key      = "your-32-char-encryption-key-here"
  jwt_secret          = "jwt-signing-secret-key"
  oauth_client_secret = "oauth-provider-client-secret"
}

# Environment Variables for Service 1
ecs_environment_variables = [
  {
    name  = "APP_ENV"
    value = "production"
  },
  {
    name  = "LOG_LEVEL"
    value = "info"
  },
  {
    name  = "PORT"
    value = "50001"
  },
  {
    name  = "SERVICE_NAME"
    value = "main-application"
  }
]

# ALB Configuration for Service 1
ecs_health_check_path   = "/health"
ecs_internal_alb        = false
ecs_allowed_cidr_blocks = ["0.0.0.0/0"]

# Certificate Configuration (use self-signed for this example)
ecs_create_self_signed_cert = true
ecs_domain_name             = "main-app.example.com"

# Security Configuration
ecs_enable_waf                 = true
ecs_rate_limit_per_5min        = 2000
ecs_enable_deletion_protection = false

# Bastion Configuration
bastion_key_name                = "my-key-pair"
bastion_instance_type           = "t3.micro"
bastion_allowed_ssh_cidr_blocks = ["203.0.113.0/24"]

# Common Configuration
log_retention_in_days = 30

tags = {
  Environment = "production"
  Project     = "dual-ecr-services"
  Service     = "main-app"
  ManagedBy   = "terraform"
  CostCenter  = "engineering"
}

# Note: For the second service (lightweight service), you would need to create
# a separate terraform configuration or modify the modules to support multiple services.
# This example shows the configuration for the main service with secrets sidecar.
# 
# To deploy the second service without secrets sidecar, you would use:
# ecs_enable_secrets_sidecar = false
# ecs_task_cpu = 256 (roughly equivalent to 200m vCPU)  
# ecs_task_memory = 200 (200MB RAM)
</file>

<file path="examples/aws-ecs-dual-service-concept.tfvars">
# Conceptual Example: Dual ECS Services Configuration
# 
# NOTE: This is a conceptual example showing how you WOULD configure
# two services if the modules supported multiple services in one deployment.
# Currently, this requires two separate Terraform deployments.
#
# Service 1: Main application with secrets sidecar (4 vCPU, 4GB RAM, port 50001)
# Service 2: Lightweight service without sidecar (256 CPU, 200MB RAM, port 8080)

# Basic Configuration
cloud_provider = "aws"
aws_region     = "us-west-2"
cluster_name   = "multi-service-cluster"
environment    = "production"

# Module Control
enable_eks           = false
enable_ecs           = true
enable_bastion       = true
enable_nat_gateway   = true
enable_vpc_endpoints = true

# VPC Configuration
create_vpc               = true
vpc_cidr                 = "10.0.0.0/16"
availability_zones_count = 3

# ============================================================================
# SERVICE 1 CONFIGURATION - Main Application with Secrets Sidecar
# ============================================================================

# Service 1: ECR Image and Resources
ecs_container_image        = "123456789012.dkr.ecr.us-west-2.amazonaws.com/main-app:latest"
ecs_container_port         = 50001
ecs_task_cpu               = 4096 # 4 vCPU
ecs_task_memory            = 4096 # 4GB RAM
ecs_desired_count          = 2
ecs_enable_secrets_sidecar = true # Enable secrets sidecar for main app

# Service 1: Secrets Configuration
ecs_secrets_prefix = "main-app/"
ecs_secrets = {
  database_url        = "postgresql://user:pass@db.internal:5432/mainapp"
  redis_url           = "redis://redis.internal:6379"
  api_key             = "sk-1234567890abcdef"
  jwt_secret          = "super-secret-jwt-signing-key"
  encryption_key      = "32-char-encryption-key-for-aes256"
  oauth_client_id     = "oauth_app_client_id"
  oauth_client_secret = "oauth_app_client_secret"
  smtp_password       = "email-service-password"
  s3_access_key       = "AKIA1234567890123456"
  s3_secret_key       = "abcdefghijklmnopqrstuvwxyz1234567890ABCD"
}

# Service 1: Environment Variables
ecs_environment_variables = [
  {
    name  = "APP_ENV"
    value = "production"
  },
  {
    name  = "LOG_LEVEL"
    value = "info"
  },
  {
    name  = "PORT"
    value = "50001"
  },
  {
    name  = "SERVICE_NAME"
    value = "main-application"
  },
  {
    name  = "ENABLE_METRICS"
    value = "true"
  },
  {
    name  = "CORS_ORIGIN"
    value = "https://app.example.com"
  }
]

# Service 1: ALB and Certificate Configuration
ecs_health_check_path   = "/api/health"
ecs_internal_alb        = false
ecs_allowed_cidr_blocks = ["0.0.0.0/0"]

# Service 1: Certificate (self-signed for demo)
ecs_create_self_signed_cert = true
ecs_domain_name             = "api.example.com"

# Service 1: Security Configuration
ecs_enable_waf                 = true
ecs_rate_limit_per_5min        = 2000
ecs_enable_deletion_protection = false

# ============================================================================  
# CONCEPTUAL SERVICE 2 CONFIGURATION - Lightweight Service
# ============================================================================
# NOTE: These variables don't exist in current modules but show the concept

# Hypothetical Service 2 variables (would need to be added to modules):
# service2_container_image         = "123456789012.dkr.ecr.us-west-2.amazonaws.com/lightweight-app:latest"
# service2_container_port          = 8080
# service2_task_cpu                = 256   # ~200m vCPU equivalent
# service2_task_memory             = 200   # 200MB RAM
# service2_desired_count           = 1
# service2_enable_secrets_sidecar  = false # No secrets sidecar

# service2_environment_variables = [
#   {
#     name  = "APP_ENV"
#     value = "production"
#   },
#   {
#     name  = "LOG_LEVEL"
#     value = "warn"
#   },
#   {
#     name  = "PORT"
#     value = "8080"
#   },
#   {
#     name  = "SERVICE_NAME"
#     value = "lightweight-service"
#   }
# ]

# service2_health_check_path = "/ping"
# service2_internal_alb      = true  # Internal service
# service2_create_alb        = false # Reuse main ALB or use service mesh

# ============================================================================
# SHARED CONFIGURATION
# ============================================================================

# Bastion Configuration
bastion_key_name                = "my-key-pair"
bastion_instance_type           = "t3.micro"
bastion_allowed_ssh_cidr_blocks = ["203.0.113.0/24"]

# Common Configuration
log_retention_in_days = 30

tags = {
  Environment = "production"
  Project     = "multi-service-deployment"
  ManagedBy   = "terraform"
  CostCenter  = "engineering"
}

# ============================================================================
# DEPLOYMENT INSTRUCTIONS
# ============================================================================
# 
# Since the current modules don't support multiple services, deploy as follows:
#
# 1. Main Service Deployment:
#    terraform workspace new main-service
#    terraform apply -var-file="examples/aws-ecs-dual-ecr-services.tfvars"
#
# 2. Lightweight Service Deployment:
#    terraform workspace new lightweight-service
#    terraform apply -var-file="examples/aws-ecs-lightweight-service.tfvars" \
#      -var="create_vpc=false" \
#      -var="existing_vpc_id=$(terraform output -raw vpc_id)" \
#      -var="enable_bastion=false"
#
# This approach:
# - Deploys main service with full features and secrets sidecar
# - Deploys lightweight service reusing the same VPC and bastion
# - Both services get their own ALB (could be optimized to share)
# - Services can communicate within the VPC via security group rules
</file>

<file path="examples/aws-ecs-full-stack.tfvars">
# Basic Configuration
region       = "us-west-2"
cluster_name = "my-ecs-app"
environment  = "production"

# Module Control - Enable ECS and disable EKS
enable_eks           = false
enable_ecs           = true
enable_bastion       = true
enable_nat_gateway   = true
enable_vpc_endpoints = true

# VPC Configuration
create_vpc               = true
vpc_cidr                 = "10.0.0.0/16"
availability_zones_count = 3

# ECS Container Configuration
ecs_container_image = "my-app:latest"
ecs_container_port  = 8000
ecs_task_cpu        = 1024
ecs_task_memory     = 2048
ecs_desired_count   = 3

# Secrets Configuration
ecs_secrets_prefix = "myapp/"
ecs_secrets = {
  database_url   = "postgresql://user:pass@db:5432/myapp"
  api_key        = "your-secret-api-key"
  encryption_key = "your-encryption-key"
}

# Environment Variables
ecs_environment_variables = [
  {
    name  = "APP_ENV"
    value = "production"
  },
  {
    name  = "LOG_LEVEL"
    value = "info"
  }
]

# ALB Configuration
ecs_health_check_path   = "/health"
ecs_internal_alb        = false
ecs_allowed_cidr_blocks = ["0.0.0.0/0"]

# Certificate Configuration (use existing certificate)
# ecs_acm_certificate_arn = "arn:aws:acm:us-west-2:123456789012:certificate/12345678-1234-1234-1234-123456789012"

# OR create self-signed certificate (default behavior)
ecs_create_self_signed_cert = true
ecs_domain_name             = "myapp.example.com"

# Security Configuration
ecs_enable_waf                 = true
ecs_rate_limit_per_5min        = 2000
ecs_enable_deletion_protection = false

# Bastion Configuration
bastion_key_name                = "my-key-pair"
bastion_instance_type           = "t3.micro"
bastion_allowed_ssh_cidr_blocks = ["203.0.113.0/24"]

# Common Configuration
log_retention_in_days = 30

tags = {
  Environment = "production"
  Project     = "my-ecs-app"
  ManagedBy   = "terraform"
}
</file>

<file path="examples/aws-ecs-lightweight-service.tfvars">
# Example: Lightweight ECS Service from ECR without Secrets Sidecar
# This example deploys a lightweight service using minimal resources
# - No secrets sidecar (secrets sidecar disabled)
# - 256 CPU units (~200m vCPU equivalent)
# - 200MB RAM

# Basic Configuration
cloud_provider = "aws"
aws_region     = "us-west-2"
cluster_name   = "lightweight-service"
environment    = "production"

# Module Control - Enable ECS only
enable_eks           = false
enable_ecs           = true
enable_bastion       = false # No bastion needed for lightweight setup
enable_nat_gateway   = true
enable_vpc_endpoints = false # Save costs for lightweight setup

# VPC Configuration
create_vpc               = true
vpc_cidr                 = "10.0.0.0/16"
availability_zones_count = 2 # Minimal AZ count to save costs

# Lightweight Service Configuration
# Uses minimal CPU/memory, no secrets sidecar
ecs_container_image        = "123456789012.dkr.ecr.us-west-2.amazonaws.com/my-lightweight-app:latest"
ecs_container_port         = 8080
ecs_task_cpu               = 256   # ~200m vCPU equivalent
ecs_task_memory            = 200   # 200MB RAM
ecs_desired_count          = 1     # Single instance for lightweight
ecs_enable_secrets_sidecar = false # No secrets sidecar

# No secrets needed for this service
ecs_secrets        = {}
ecs_secrets_prefix = ""

# Simple Environment Variables (no secrets endpoint)
ecs_environment_variables = [
  {
    name  = "APP_ENV"
    value = "production"
  },
  {
    name  = "LOG_LEVEL"
    value = "warn" # Less logging for lightweight
  },
  {
    name  = "PORT"
    value = "8080"
  },
  {
    name  = "SERVICE_NAME"
    value = "lightweight-service"
  },
  {
    name  = "ENABLE_METRICS"
    value = "false" # Disable metrics to save resources
  }
]

# ALB Configuration
ecs_health_check_path   = "/ping"
ecs_internal_alb        = false
ecs_allowed_cidr_blocks = ["0.0.0.0/0"]

# Skip certificate for lightweight setup (HTTP only)
ecs_create_self_signed_cert = false
ecs_domain_name             = null

# Minimal Security Configuration
ecs_enable_waf                 = false # No WAF to save costs
ecs_rate_limit_per_5min        = 1000  # Lower rate limit
ecs_enable_deletion_protection = false

# Common Configuration
log_retention_in_days = 7 # Shorter retention for lightweight

tags = {
  Environment = "production"
  Project     = "lightweight-services"
  Service     = "lightweight-app"
  ManagedBy   = "terraform"
  CostCenter  = "engineering"
  Tier        = "lightweight"
}
</file>

<file path="examples/aws-ecs-only.tfvars">
# Minimal ECS Configuration using existing VPC
region       = "us-west-2"
cluster_name = "simple-ecs"
environment  = "dev"

# Module Control - Only ECS, use existing VPC
enable_eks           = false
enable_ecs           = true
enable_bastion       = false
enable_nat_gateway   = false
enable_vpc_endpoints = false

# Use Existing VPC
create_vpc      = false
existing_vpc_id = "vpc-0123456789abcdef0"

# ECS Configuration
ecs_container_image = "nginx:latest"
ecs_container_port  = 80
ecs_task_cpu        = 256
ecs_task_memory     = 512
ecs_desired_count   = 1

# Simple secrets configuration
ecs_secrets_prefix = "simple-app/"
ecs_secrets = {
  app_secret = "my-secret-value"
}

# Use self-signed certificate
ecs_create_self_signed_cert = true

# Basic ALB settings
ecs_health_check_path = "/"
ecs_internal_alb      = false

# Security - disable WAF for simplicity
ecs_enable_waf = false

tags = {
  Environment = "dev"
  Project     = "simple-ecs"
}
</file>

<file path="examples/aws-ecs-secure-deployment.tfvars">
# Secure ECS Deployment with ALB, Secrets Manager, and TLS
# This example shows deployment of an ECS cluster in existing VPC with:
# - Main service on port 50051 (mapped from ALB 443)
# - Secrets broker sidecar on port 8080
# - TLS certificates for inter-container communication
# - Comprehensive secrets management via AWS Secrets Manager
# - Secure keypair and certificate generation scripts

# Basic Configuration
cloud_provider = "aws"
aws_region     = "us-west-2"
cluster_name   = "secure-app"
environment    = "production"

# Module Control - Enable ECS with existing VPC
enable_eks           = false
enable_ecs           = true
enable_bastion       = true
enable_nat_gateway   = true
enable_vpc_endpoints = true

# Existing VPC Configuration
# Set create_vpc = false to use existing VPC
create_vpc = false
# Specify your existing VPC ID
existing_vpc_id = "vpc-0123456789abcdef0"
# Specify existing subnet IDs (private subnets for ECS tasks)
existing_private_subnet_ids = [
  "subnet-0123456789abcdef0",
  "subnet-0fedcba9876543210"
]
# Specify existing public subnet IDs (for ALB)
existing_public_subnet_ids = [
  "subnet-0abcdef0123456789",
  "subnet-09876543210fedcba"
]

# ECS Service Configuration
ecs_container_image        = "your-account.dkr.ecr.us-west-2.amazonaws.com/your-app:latest"
ecs_container_port         = 50051 # Main service port
ecs_task_cpu               = 2048  # 2 vCPU
ecs_task_memory            = 4096  # 4GB RAM
ecs_desired_count          = 2     # Number of tasks
ecs_enable_secrets_sidecar = true  # Enable secrets broker on port 8080

# Secrets Configuration
# All secrets will be stored under this prefix in AWS Secrets Manager
ecs_secrets_prefix = "secure-app/prod/"

# Product and Platform Credentials
# These will be uploaded using the upload-secrets.sh script
ecs_secrets = {
  # Product Credentials
  product_client_id  = "/secure-app/prod/product_client_id"
  product_secret_key = "/secure-app/prod/product_secret_key"

  # Platform Credentials  
  platform_client_id  = "/secure-app/prod/platform_client_id"
  platform_secret_key = "/secure-app/prod/platform_secret_key"

  # TLS Certificates (uploaded via generate-certs.sh script)
  ssl_certificate = "/secure-app/prod/ssl_certificate"
  ssl_private_key = "/secure-app/prod/ssl_private_key"

  # Additional application secrets
  database_url    = "/secure-app/prod/database_url"
  redis_password  = "/secure-app/prod/redis_password"
  jwt_signing_key = "/secure-app/prod/jwt_signing_key"
  encryption_key  = "/secure-app/prod/encryption_key"
}

# Environment Variables
ecs_environment_variables = [
  {
    name  = "APP_ENV"
    value = "production"
  },
  {
    name  = "PORT"
    value = "50051"
  },
  {
    name  = "SECRETS_BROKER_PORT"
    value = "8080"
  },
  {
    name  = "SERVICE_NAME"
    value = "secure-app"
  },
  {
    name  = "LOG_LEVEL"
    value = "info"
  },
  {
    name  = "ENABLE_TLS"
    value = "true"
  }
]

# ALB Configuration - Maps 443 to service port 50051
ecs_health_check_path   = "/health"
ecs_internal_alb        = false # Public-facing ALB
ecs_allowed_cidr_blocks = ["0.0.0.0/0"]

# SSL Certificate Configuration
# Option 1: Use existing ACM certificate ARN (recommended for production)
ecs_acm_certificate_arn = "arn:aws:acm:us-west-2:123456789012:certificate/your-cert-id"

# Option 2: Use generated self-signed certificate (for development/testing)
# ecs_create_self_signed_cert = true
# ecs_domain_name            = "api.your-domain.com"

# Security Configuration
ecs_enable_waf                 = true
ecs_rate_limit_per_5min        = 3000
ecs_enable_deletion_protection = true

# Bastion Configuration (uses generated keypair)
bastion_key_name      = "secure-app-keypair"
bastion_instance_type = "t3.micro"
bastion_allowed_ssh_cidr_blocks = [
  "10.0.0.0/8",    # Internal networks
  "172.16.0.0/12", # Private networks
  "192.168.0.0/16" # Local networks
]

# Logging Configuration
log_retention_in_days = 30

# Resource Tags
tags = {
  Environment = "production"
  Project     = "secure-app"
  Service     = "main-api"
  ManagedBy   = "terraform"
  Owner       = "devops-team@company.com"
  CostCenter  = "engineering"
  Compliance  = "required"
  TLS         = "enabled"
}

# ============================================================================
# DEPLOYMENT INSTRUCTIONS
# ============================================================================
#
# Before running terraform apply, execute these scripts in order:
#
# 1. Generate secure keypair:
#    ./create-keypair.sh
#
# 2. Generate and upload TLS certificates:
#    ./generate-certs.sh
#
# 3. Upload product and platform credentials:
#    ./upload-secrets.sh
#
# 4. Deploy infrastructure:
#    terraform init
#    terraform plan -var-file="aws-ecs-secure-deployment.tfvars"
#    terraform apply -var-file="aws-ecs-secure-deployment.tfvars"
#
# ============================================================================
# SECRETS PATHS CONFIGURATION
# ============================================================================
#
# All secrets are organized under the prefix: secure-app/prod/
#
# Product Credentials:
# - /secure-app/prod/product_client_id
# - /secure-app/prod/product_secret_key
#
# Platform Credentials:
# - /secure-app/prod/platform_client_id  
# - /secure-app/prod/platform_secret_key
#
# TLS Certificates:
# - /secure-app/prod/ssl_certificate
# - /secure-app/prod/ssl_private_key
#
# Application Secrets:
# - /secure-app/prod/database_url
# - /secure-app/prod/redis_password
# - /secure-app/prod/jwt_signing_key
# - /secure-app/prod/encryption_key
#
# ============================================================================
# SERVICE ARCHITECTURE
# ============================================================================
#
# ALB (Port 443) -> ECS Service (Port 50051)
# |
# +-- Main Application Container (Port 50051)
# +-- Secrets Broker Sidecar (Port 8080)
#
# Inter-container communication uses TLS certificates stored in Secrets Manager
# Main service communicates with secrets broker via localhost:8080
#
# ============================================================================
</file>

<file path="examples/aws-vpc-only.tfvars">
# Example: VPC and networking only (no EKS)
cluster_name = "my-network-foundation"
region       = "us-east-1"
environment  = "staging"

# Create VPC only
create_vpc     = true
enable_eks     = false
enable_bastion = false

# VPC Configuration
vpc_cidr                 = "10.1.0.0/16"
availability_zones_count = 3
enable_nat_gateway       = true
enable_vpc_endpoints     = true

# Common Configuration
log_retention_in_days = 30

tags = {
  Environment = "staging"
  Project     = "network-foundation"
  Owner       = "infrastructure-team"
}
</file>

<file path="examples/ecs-http-secrets-example.tfvars">
# ECS HTTP Server with Secrets Sidecar Example Configuration
# 
# This example demonstrates deploying:
# 1. HTTP server service that communicates with a secrets sidecar
# 2. Secrets sidecar service that fetches secrets from AWS Secrets Manager
# 3. Self-signed certificates for HTTPS communication
# 4. Load balancer with SSL termination

# Basic Configuration
cloud_provider = "aws"
aws_region     = "us-west-2"
cluster_name   = "http-secrets-cluster"
environment    = "development"

# Module Control
enable_eks           = false
enable_ecs           = true
enable_bastion       = true
enable_nat_gateway   = true
enable_vpc_endpoints = true

# VPC Configuration
create_vpc               = true
vpc_cidr                 = "10.0.0.0/16"
availability_zones_count = 2

# ============================================================================
# HTTP SERVER SERVICE CONFIGURATION
# ============================================================================

# Container Configuration
ecs_container_image = "nginx:latest" # Replace with your HTTP server image
ecs_container_port  = 443            # HTTPS port for secure communication
ecs_task_cpu        = 512            # 0.5 vCPU
ecs_task_memory     = 1024           # 1GB RAM
ecs_desired_count   = 2

# Enable Secrets Sidecar
ecs_enable_secrets_sidecar = true
ecs_secrets_sidecar_image  = "amazon/aws-cli:latest" # Replace with secrets sidecar image

# Secrets Configuration - These will be created in AWS Secrets Manager
ecs_secrets_prefix = "http-server/"
ecs_secrets = {
  database_password   = "my-secure-db-password"
  api_key             = "sk-1234567890abcdefghijklmnop"
  jwt_signing_key     = "super-secret-jwt-key-for-token-signing"
  redis_password      = "redis-secure-password-123"
  encryption_key      = "32-char-aes256-encryption-key!!"
  oauth_client_secret = "oauth-app-client-secret-value"
  smtp_password       = "email-service-password-secure"
  third_party_api_key = "3rd-party-service-api-key-123"
}

# Environment Variables (non-sensitive configuration)
ecs_environment_variables = [
  {
    name  = "APP_ENV"
    value = "development"
  },
  {
    name  = "LOG_LEVEL"
    value = "info"
  },
  {
    name  = "PORT"
    value = "443"
  },
  {
    name  = "SERVICE_NAME"
    value = "http-server"
  },
  {
    name  = "ENABLE_HTTPS"
    value = "true"
  },
  {
    name  = "CERT_PATH"
    value = "/etc/ssl/certs/server.crt"
  },
  {
    name  = "KEY_PATH"
    value = "/etc/ssl/private/server.key"
  },
  {
    name  = "SECRETS_SIDECAR_URL"
    value = "http://localhost:8080"
  },
  {
    name  = "SECRETS_REFRESH_INTERVAL"
    value = "300"
  }
]

# ============================================================================
# LOAD BALANCER AND CERTIFICATE CONFIGURATION
# ============================================================================

# Health Check Configuration
ecs_health_check_path = "/health"

# Load Balancer Configuration
ecs_internal_alb        = false         # Public-facing ALB
ecs_allowed_cidr_blocks = ["0.0.0.0/0"] # Allow all traffic (adjust for production)

# SSL Certificate Configuration
ecs_create_self_signed_cert = true
ecs_domain_name             = "http-server.example.local"

# Security Configuration
ecs_enable_waf                 = true
ecs_rate_limit_per_5min        = 1000
ecs_enable_deletion_protection = false
ecs_ssl_policy                 = "ELBSecurityPolicy-TLS-1-2-2017-01"

# ============================================================================
# BASTION HOST CONFIGURATION
# ============================================================================

bastion_key_name                = "my-key-pair" # Create this key pair in AWS first
bastion_instance_type           = "t3.micro"
bastion_allowed_ssh_cidr_blocks = ["0.0.0.0/0"] # Restrict to your IP in production

# ============================================================================
# SHARED CONFIGURATION
# ============================================================================

# Logging
log_retention_in_days = 14

# Resource Tagging
tags = {
  Environment = "development"
  Project     = "http-secrets-demo"
  Owner       = "devops-team"
  ManagedBy   = "terraform"
  Purpose     = "ecs-http-server-with-secrets-sidecar"
}

# ============================================================================
# DEPLOYMENT NOTES
# ============================================================================
#
# Before deploying, ensure you have:
# 1. Created an AWS key pair named "my-key-pair" (or update the name above)
# 2. Run the setup scripts to create certificates and secrets:
#    - ./scripts/generate-certificates.sh
#    - ./scripts/setup-secrets.sh
# 3. Built and pushed your HTTP server and secrets sidecar images to ECR
#
# Deploy with:
# terraform apply -var-file="examples/ecs-http-secrets-example.tfvars"
#
# The deployment creates:
# - VPC with public/private subnets across 2 AZs
# - ECS cluster with Fargate tasks
# - HTTP server container with mounted SSL certificates
# - Secrets sidecar container for secure secrets retrieval
# - Application Load Balancer with SSL termination
# - Security groups allowing HTTPS traffic
# - CloudWatch log groups for monitoring
# - Bastion host for secure access
#
# After deployment:
# - Access your service via the ALB DNS name (HTTPS)
# - SSH to bastion host to troubleshoot if needed
# - Monitor logs in CloudWatch
</file>

<file path="examples/README.md">
# Terraform Configuration Examples

This directory contains example `terraform.tfvars` files demonstrating various deployment scenarios for the multicloud Terraform project.

## ECS Examples

### Basic ECS Deployments

- **`aws-ecs-full-stack.tfvars`** - Complete ECS deployment with all features
- **`aws-ecs-only.tfvars`** - ECS-only deployment without EKS
- **`aws-ecs-production-ready.tfvars`** - 🆕 Production-ready ECS with secrets sidecar, 4 vCPU/4GB RAM, port 50001

### ECR-Based Deployments

- **`aws-ecs-dual-ecr-services.tfvars`** - 🆕 Main application service from ECR with secrets sidecar
- **`aws-ecs-lightweight-service.tfvars`** - 🆕 Lightweight service (256 CPU/200MB RAM) without secrets sidecar
- **`aws-ecs-dual-service-concept.tfvars`** - 🆕 Conceptual example showing dual-service configuration

### Multi-Service Setup

- **`aws-ecs-multi-service-setup.md`** - 🆕 Guide for deploying multiple ECS services

## EKS Examples

- **`aws-eks-only.tfvars`** - EKS-only deployment without ECS

## Infrastructure Examples

- **`aws-full-stack.tfvars`** - Complete AWS deployment with both EKS and ECS
- **`aws-vpc-only.tfvars`** - VPC-only deployment for shared infrastructure

## New Features Highlighted

### Secrets Sidecar Control

All ECS examples now support the `ecs_enable_secrets_sidecar` variable:

```hcl
# Enable secrets sidecar (default)
ecs_enable_secrets_sidecar = true

# Disable secrets sidecar for lightweight services
ecs_enable_secrets_sidecar = false
```

### When to Use Secrets Sidecar

**Enable secrets sidecar when:**
- Application needs secure access to database credentials, API keys, certificates
- Secrets require rotation without container restart
- Application benefits from centralized secret management
- Security compliance requires encrypted secret access

**Disable secrets sidecar when:**
- Lightweight services with minimal resource requirements
- Applications don't use secrets (or use environment variables only)
- Cost optimization is prioritized over advanced secret management

## Resource Configurations

### Production Service (with secrets sidecar)
```hcl
ecs_task_cpu                = 4096  # 4 vCPU
ecs_task_memory             = 4096  # 4GB RAM
ecs_container_port          = 50001
ecs_enable_secrets_sidecar  = true
```

### Lightweight Service (without secrets sidecar)
```hcl
ecs_task_cpu                = 256   # ~200m vCPU equivalent
ecs_task_memory             = 200   # 200MB RAM
ecs_container_port          = 8080
ecs_enable_secrets_sidecar  = false
```

## Terraform Variable Files (tfvars) Reference

This section provides detailed information about all available `.tfvars` example files and their configurations.

### Core Infrastructure Examples

#### **`aws-full-stack.tfvars`**
Complete AWS deployment with both EKS and ECS components.

**Key Variables:**
```hcl
cluster_name  = "my-secure-eks"
region        = "us-west-2"
environment   = "dev"
create_vpc    = true
enable_eks    = true
enable_bastion = true

# VPC Configuration
vpc_cidr                 = "10.0.0.0/16"
availability_zones_count = 2
enable_nat_gateway       = true
enable_vpc_endpoints     = true

# EKS Configuration
node_size_config    = "medium"
kubernetes_version  = "1.28"
capacity_type      = "ON_DEMAND"
node_ssh_key_name  = "my-keypair"

# Bastion Configuration
bastion_key_name                = "my-keypair"
bastion_instance_type           = "t3.micro"
bastion_allowed_ssh_cidr_blocks = ["203.0.113.0/24"]
```

#### **`aws-vpc-only.tfvars`**
VPC-only deployment for shared infrastructure.

**Key Variables:**
```hcl
create_vpc    = true
enable_eks    = false
enable_ecs    = false
enable_bastion = false
vpc_cidr      = "10.0.0.0/16"
```

### ECS Deployment Examples

#### **`aws-ecs-production-ready.tfvars`**
Production-ready ECS with secrets sidecar, 4 vCPU/4GB RAM, port 50001.

**Key Variables:**
```hcl
ecs_container_image         = "123456789012.dkr.ecr.us-west-2.amazonaws.com/production-app:v1.2.3"
ecs_container_port          = 50001
ecs_task_cpu                = 4096  # 4 vCPU
ecs_task_memory             = 4096  # 4GB RAM
ecs_desired_count           = 3
ecs_enable_secrets_sidecar  = true

# Comprehensive secrets configuration
ecs_secrets = {
  database_url            = "postgresql://..."
  stripe_secret_key       = "sk_live_..."
  jwt_signing_key         = "HS256-..."
  # ... and many more production secrets
}

# SSL Certificate
ecs_acm_certificate_arn = "arn:aws:acm:us-west-2:123456789012:certificate/..."

# Production security
ecs_enable_waf              = true
ecs_rate_limit_per_5min     = 5000
ecs_enable_deletion_protection = true
```

#### **`aws-ecs-lightweight-service.tfvars`**
Lightweight service (256 CPU/200MB RAM) without secrets sidecar.

**Key Variables:**
```hcl
ecs_container_image         = "123456789012.dkr.ecr.us-west-2.amazonaws.com/my-lightweight-app:latest"
ecs_container_port          = 8080
ecs_task_cpu                = 256   # ~200m vCPU equivalent
ecs_task_memory             = 200   # 200MB RAM
ecs_desired_count           = 1
ecs_enable_secrets_sidecar  = false # No secrets sidecar

# Cost optimization
enable_bastion           = false
enable_vpc_endpoints     = false
ecs_enable_waf          = false
availability_zones_count = 2
```

#### **`aws-ecs-dual-ecr-services.tfvars`**
Main application service from ECR with secrets sidecar.

**Key Variables:**
```hcl
ecs_container_image         = "123456789012.dkr.ecr.us-west-2.amazonaws.com/my-main-app:latest"
ecs_container_port          = 50001
ecs_task_cpu                = 4096  # 4 vCPU
ecs_task_memory             = 4096  # 4GB RAM
ecs_enable_secrets_sidecar  = true

# Self-signed certificate for development
ecs_create_self_signed_cert = true
ecs_domain_name            = "main-app.example.com"
```

#### **`aws-ecs-full-stack.tfvars`**
Complete ECS deployment with all features enabled.

**Key Variables:**
```hcl
enable_ecs         = true
enable_bastion     = true
enable_nat_gateway = true
enable_vpc_endpoints = true
ecs_enable_secrets_sidecar = true
```

#### **`aws-ecs-only.tfvars`**
ECS-only deployment without EKS.

**Key Variables:**
```hcl
enable_eks = false
enable_ecs = true
```

### Specialized ECS Examples

#### **`ecs-http-secrets-example.tfvars`**
HTTP server with secrets sidecar demonstration.

**Key Variables:**
```hcl
ecs_container_image = "nginx:latest"
ecs_container_port  = 443  # HTTPS port
ecs_task_cpu        = 512
ecs_task_memory     = 1024
ecs_enable_secrets_sidecar = true

# Development-focused configuration
ecs_create_self_signed_cert = true
ecs_domain_name            = "http-server.example.local"

# Environment variables for HTTPS
ecs_environment_variables = [
  { name = "ENABLE_HTTPS", value = "true" },
  { name = "CERT_PATH", value = "/etc/ssl/certs/server.crt" },
  { name = "SECRETS_SIDECAR_URL", value = "http://localhost:8080" }
]
```

#### **`aws-ecs-secure-deployment.tfvars`**
Secure ECS deployment with existing VPC and comprehensive TLS configuration.

**Key Variables:**
```hcl
# Use existing VPC
create_vpc = false
existing_vpc_id = "vpc-0123456789abcdef0"
existing_private_subnet_ids = ["subnet-0123456789abcdef0", "subnet-0fedcba9876543210"]
existing_public_subnet_ids = ["subnet-0abcdef0123456789", "subnet-09876543210fedcba"]

# Service configuration
ecs_container_port = 50051
ecs_task_cpu       = 2048  # 2 vCPU
ecs_task_memory    = 4096  # 4GB RAM

# Comprehensive secrets with TLS certificates
ecs_secrets = {
  product_client_id    = "/secure-app/prod/product_client_id"
  platform_client_id   = "/secure-app/prod/platform_client_id"
  ssl_certificate      = "/secure-app/prod/ssl_certificate"
  ssl_private_key     = "/secure-app/prod/ssl_private_key"
}

# ACM certificate for production
ecs_acm_certificate_arn = "arn:aws:acm:us-west-2:123456789012:certificate/your-cert-id"
```

#### **`aws-ecs-dual-service-concept.tfvars`**
Conceptual example showing dual-service configuration approach.

### EKS Examples

#### **`aws-eks-only.tfvars`**
EKS-only deployment without ECS.

**Key Variables:**
```hcl
enable_eks = true
enable_ecs = false
node_size_config    = "medium"
kubernetes_version  = "1.28"
capacity_type      = "ON_DEMAND"
```

## Usage Examples

### Single Service Deployment
```bash
terraform apply -var-file="examples/aws-ecs-production-ready.tfvars"
```

### Multi-Service Deployment
```bash
# Deploy main service
terraform workspace new main-service
terraform apply -var-file="examples/aws-ecs-dual-ecr-services.tfvars"

# Deploy lightweight service (reusing VPC)
terraform workspace new lightweight-service
MAIN_VPC_ID=$(terraform workspace select main-service && terraform output -raw vpc_id)
terraform workspace select lightweight-service
terraform apply -var-file="examples/aws-ecs-lightweight-service.tfvars" \
  -var="create_vpc=false" \
  -var="existing_vpc_id=${MAIN_VPC_ID}" \
  -var="enable_bastion=false"
```

### Secure Deployment with Helper Scripts
```bash
# Setup secure deployment
./examples/create-keypair.sh
./examples/generate-certs.sh
./examples/upload-secrets.sh
terraform apply -var-file="examples/aws-ecs-secure-deployment.tfvars"
```

## Environment-Specific Configurations

The `environments/` directory contains environment-specific variable files:
- `dev-aws.tfvars`, `dev-azure.tfvars` - Development environments
- `perf-aws.tfvars`, `perf-azure.tfvars` - Performance testing environments  
- `prod-aws.tfvars`, `prod-azure.tfvars` - Production environments

## Secrets Sidecar Usage

When `ecs_enable_secrets_sidecar = true`, your application can access secrets via HTTP:

```bash
# Get individual secret
curl http://localhost:8080/secrets/database_url

# Get all secrets as JSON
curl http://localhost:8080/secrets

# Health check
curl http://localhost:8080/health
```

The sidecar provides:
- Secure AWS Secrets Manager integration
- Automatic secret rotation support
- In-memory caching with TTL
- Health monitoring and retries

## Next Steps

1. Copy an appropriate example file to `terraform.tfvars`
2. Modify the configuration for your specific requirements
3. Update container images to point to your ECR repositories
4. Configure secrets according to your application needs
5. Run `terraform plan` to review changes before applying
</file>

<file path="examples/sandbox-ecs-existing-vpc.tfvars">
# ECS Cluster in Existing VPC with ALB and Self-Signed Certs
# Environment: Sandbox, Region: us-west-2
# Uses existing VPC, private subnets, and ACM certificate for ALB
# Creates self-signed certificates for service containers

# Basic Configuration
cloud_provider = "aws"
aws_region     = "us-west-2"
cluster_name   = "sandbox-ecs-cluster"
environment    = "sandbox"

# Module Control - Enable ECS only
enable_eks           = false
enable_ecs           = true
enable_bastion       = false
enable_nat_gateway   = false
enable_vpc_endpoints = false

# Existing VPC Configuration
create_vpc = false
# Replace with your existing VPC ID
vpc_id = "vpc-0123456789abcdef0"

# Replace with your existing private subnet IDs
private_subnet_ids = [
  "subnet-0123456789abcdef0",
  "subnet-0123456789abcdef1",
  "subnet-0123456789abcdef2"
]

# Replace with your existing public subnet IDs (for ALB)
public_subnet_ids = [
  "subnet-abcdef0123456789a",
  "subnet-abcdef0123456789b",
  "subnet-abcdef0123456789c"
]

# ECS Service Configuration
ecs_container_image        = "123456789012.dkr.ecr.us-west-2.amazonaws.com/main-service:latest"
ecs_container_port         = 50051
ecs_task_cpu               = 1024 # 1 vCPU
ecs_task_memory            = 2048 # 2GB RAM
ecs_desired_count          = 2    # 2 instances
ecs_enable_secrets_sidecar = true # Enable secrets sidecar container

# Sidecar Configuration (ECR image)
ecs_secrets_sidecar_image = "123456789012.dkr.ecr.us-west-2.amazonaws.com/sidecar-service:latest"

# Environment Variables
ecs_environment_variables = [
  {
    name  = "APP_ENV"
    value = "sandbox"
  },
  {
    name  = "LOG_LEVEL"
    value = "debug"
  },
  {
    name  = "PORT"
    value = "50051"
  },
  {
    name  = "SERVICE_NAME"
    value = "main-service"
  },
  {
    name  = "ENABLE_METRICS"
    value = "true"
  }
]

# ALB Configuration - Front main service on 443:50051 with wildcard route
ecs_health_check_path   = "/health"
ecs_internal_alb        = false         # Public-facing ALB
ecs_allowed_cidr_blocks = ["0.0.0.0/0"] # Allow from anywhere

# SSL Certificate Configuration
# Use existing ACM certificate for ALB
ecs_acm_certificate_arn = "arn:aws:acm:us-west-2:123456789012:certificate/12345678-1234-1234-1234-123456789012"

# Create self-signed certificate for service containers
ecs_create_self_signed_cert = true
ecs_domain_name             = "sandbox-ecs-cluster.local"

# Security Configuration
ecs_enable_waf                 = false # Disabled for sandbox
ecs_rate_limit_per_5min        = 1000  # Lower rate limit for sandbox
ecs_enable_deletion_protection = false # Allow deletion in sandbox

# Logging Configuration
log_retention_in_days = 7 # Short retention for sandbox

# Tags
tags = {
  Environment = "sandbox"
  Project     = "ecs-deployment"
  Service     = "main-service"
  ManagedBy   = "terraform"
  Owner       = "devops-team"
  Purpose     = "testing"
}

# ============================================================================
# CONFIGURATION NOTES
# ============================================================================
#
# REQUIRED UPDATES BEFORE DEPLOYMENT:
#
# 1. VPC Configuration:
#    - Update vpc_id with your existing VPC ID
#    - Update private_subnet_ids with your existing private subnet IDs
#    - Update public_subnet_ids with your existing public subnet IDs
#
# 2. ECR Images:
#    - Update ecs_container_image with your main service ECR image URI
#    - Update ecs_secrets_sidecar_image with your sidecar ECR image URI
#
# 3. ACM Certificate:
#    - Update ecs_acm_certificate_arn with your existing ACM certificate ARN
#
# ALB Configuration:
# - ALB will listen on port 443 (HTTPS) using the ACM certificate
# - All requests will be forwarded to the main service on port 50051
# - Wildcard routing means all paths /* will go to the main service
#
# Certificate Setup:
# - ALB uses existing ACM certificate for HTTPS termination
# - ECS service containers get self-signed certificates for internal communication
#
# ============================================================================
</file>

<file path="modules/aws/bastion/main.tf">
data "aws_ami" "amazon_linux" {
  most_recent = true
  owners      = ["amazon"]

  filter {
    name   = "name"
    values = ["amzn2-ami-hvm-*-x86_64-gp2"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
}

data "aws_caller_identity" "current" {}

# Security group for bastion host
resource "aws_security_group" "bastion" {
  name        = "${var.name_prefix}-bastion-sg"
  description = "Security group for bastion host"
  vpc_id      = var.vpc_id

  # Allow SSH from specified CIDR blocks only
  dynamic "ingress" {
    for_each = var.allowed_ssh_cidr_blocks
    content {
      from_port   = 22
      to_port     = 22
      protocol    = "tcp"
      cidr_blocks = [ingress.value]
      description = "Allow SSH from ${ingress.value}"
    }
  }

  # Restrict egress to VPC CIDR and HTTPS only
  egress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr_block]
    description = "Allow SSH to resources within VPC"
  }

  egress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr_block]
    description = "Allow HTTPS within VPC"
  }

  # Allow outbound HTTPS for package updates (through NAT Gateway)
  egress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "Allow HTTPS for package updates"
  }

  egress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "Allow HTTP for package updates"
  }

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-bastion-sg"
  })
}

# IAM role for bastion host
resource "aws_iam_role" "bastion" {
  name = "${var.name_prefix}-bastion-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })

  tags = var.tags
}

# IAM policy for bastion host to interact with EKS
resource "aws_iam_policy" "bastion_eks_access" {
  name        = "${var.name_prefix}-bastion-eks-policy"
  description = "IAM policy for bastion host EKS access"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "eks:DescribeCluster",
          "eks:ListClusters",
          "eks:DescribeNodegroup",
          "eks:ListNodegroups",
          "eks:DescribeUpdate",
          "eks:ListUpdates"
        ]
        Resource = "*"
      },
      {
        Effect = "Allow"
        Action = [
          "sts:AssumeRole"
        ]
        Resource = "*"
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "bastion_eks_access" {
  role       = aws_iam_role.bastion.name
  policy_arn = aws_iam_policy.bastion_eks_access.arn
}

# Attach basic SSM policy for session manager access
resource "aws_iam_role_policy_attachment" "bastion_ssm" {
  role       = aws_iam_role.bastion.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
}

resource "aws_iam_instance_profile" "bastion" {
  name = "${var.name_prefix}-bastion-profile"
  role = aws_iam_role.bastion.name
}

# User data script for bastion host
locals {
  user_data = base64encode(templatefile("${path.module}/user_data.sh", {
    cluster_name = var.cluster_name
    region       = var.region
  }))
}

# Bastion host EC2 instance
resource "aws_instance" "bastion" {
  ami                    = var.ami_id != null ? var.ami_id : data.aws_ami.amazon_linux.id
  instance_type          = var.instance_type
  key_name               = var.key_name
  vpc_security_group_ids = [aws_security_group.bastion.id]
  subnet_id              = var.public_subnet_id
  iam_instance_profile   = aws_iam_instance_profile.bastion.name
  user_data              = local.user_data

  associate_public_ip_address = true

  root_block_device {
    volume_type = "gp3"
    volume_size = var.root_volume_size
    encrypted   = true
  }

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-bastion"
    Type = "bastion"
  })

  lifecycle {
    create_before_destroy = true
  }
}

# CloudWatch log group for bastion logs
resource "aws_cloudwatch_log_group" "bastion" {
  name              = "/aws/ec2/bastion/${var.name_prefix}"
  retention_in_days = var.log_retention_in_days

  tags = var.tags
}
</file>

<file path="modules/aws/vpc/main.tf">
data "aws_availability_zones" "available" {
  state = "available"
}

data "aws_caller_identity" "current" {}

data "aws_region" "current" {}

# Data source for existing VPC when create_vpc = false
data "aws_vpc" "existing" {
  count = var.create_vpc ? 0 : 1
  id    = var.vpc_id
}

# VPC resource when create_vpc = true
resource "aws_vpc" "main" {
  count = var.create_vpc ? 1 : 0

  cidr_block           = var.vpc_cidr
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-vpc"
  })
}

# Private subnets when create_vpc = true
resource "aws_subnet" "private" {
  count = var.create_vpc && var.enable_private_subnets ? var.availability_zones_count : 0

  vpc_id            = aws_vpc.main[0].id
  cidr_block        = cidrsubnet(var.vpc_cidr, 8, count.index + 1)
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = merge(var.tags, {
    Name                              = "${var.name_prefix}-private-${count.index + 1}"
    Type                              = "private"
    "kubernetes.io/role/internal-elb" = "1"
  })
}

# Public subnets when create_vpc = true
resource "aws_subnet" "public" {
  count = var.create_vpc && var.enable_public_subnets ? var.availability_zones_count : 0

  vpc_id                  = aws_vpc.main[0].id
  cidr_block              = cidrsubnet(var.vpc_cidr, 8, count.index + 100)
  availability_zone       = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = false

  tags = merge(var.tags, {
    Name                     = "${var.name_prefix}-public-${count.index + 1}"
    Type                     = "public"
    "kubernetes.io/role/elb" = "1"
  })
}

# Internet Gateway when create_vpc = true
resource "aws_internet_gateway" "main" {
  count = var.create_vpc && var.enable_public_subnets ? 1 : 0

  vpc_id = aws_vpc.main[0].id

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-igw"
  })
}

# Elastic IPs for NAT Gateways when create_vpc = true
resource "aws_eip" "nat" {
  count = var.create_vpc && var.enable_nat_gateway && var.enable_private_subnets ? var.availability_zones_count : 0

  domain     = "vpc"
  depends_on = [aws_internet_gateway.main]

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-nat-eip-${count.index + 1}"
  })
}

# NAT Gateways when create_vpc = true
resource "aws_nat_gateway" "main" {
  count = var.create_vpc && var.enable_nat_gateway && var.enable_private_subnets && var.enable_public_subnets ? var.availability_zones_count : 0

  allocation_id = aws_eip.nat[count.index].id
  subnet_id     = aws_subnet.public[count.index].id

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-nat-${count.index + 1}"
  })

  depends_on = [aws_internet_gateway.main]
}

# Public route table when create_vpc = true
resource "aws_route_table" "public" {
  count = var.create_vpc && var.enable_public_subnets ? 1 : 0

  vpc_id = aws_vpc.main[0].id

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-public-rt"
  })
}

# Public route to Internet Gateway
resource "aws_route" "public_internet" {
  count = var.create_vpc && var.enable_public_subnets ? 1 : 0

  route_table_id         = aws_route_table.public[0].id
  destination_cidr_block = "0.0.0.0/0"
  gateway_id             = aws_internet_gateway.main[0].id
}

# Private route tables when create_vpc = true
resource "aws_route_table" "private" {
  count = var.create_vpc && var.enable_private_subnets ? var.availability_zones_count : 0

  vpc_id = aws_vpc.main[0].id

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-private-rt-${count.index + 1}"
  })
}

# Private routes to NAT Gateways
resource "aws_route" "private_nat" {
  count = var.create_vpc && var.enable_private_subnets && var.enable_nat_gateway && var.enable_public_subnets ? var.availability_zones_count : 0

  route_table_id         = aws_route_table.private[count.index].id
  destination_cidr_block = "0.0.0.0/0"
  nat_gateway_id         = aws_nat_gateway.main[count.index].id
}

# Route table associations for public subnets
resource "aws_route_table_association" "public" {
  count = var.create_vpc && var.enable_public_subnets ? var.availability_zones_count : 0

  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public[0].id
}

# Route table associations for private subnets
resource "aws_route_table_association" "private" {
  count = var.create_vpc && var.enable_private_subnets ? var.availability_zones_count : 0

  subnet_id      = aws_subnet.private[count.index].id
  route_table_id = aws_route_table.private[count.index].id
}

# Security group for VPC endpoints - only when create_vpc = true
resource "aws_security_group" "vpc_endpoints" {
  count = var.create_vpc && var.enable_vpc_endpoints ? 1 : 0

  name        = "${var.name_prefix}-vpc-endpoints-sg"
  description = "Security group for VPC endpoints"
  vpc_id      = aws_vpc.main[0].id

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = [var.vpc_cidr]
  }

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-vpc-endpoints-sg"
  })
}

# S3 VPC Endpoint Policy Document
data "aws_iam_policy_document" "s3_endpoint_policy" {
  count = var.create_vpc && var.enable_vpc_endpoints ? 1 : 0

  statement {
    effect = "Allow"
    principals {
      type        = "*"
      identifiers = ["*"]
    }
    actions   = ["s3:*"]
    resources = ["*"]
    condition {
      test     = "StringEquals"
      variable = "aws:PrincipalVpc"
      values   = [aws_vpc.main[0].id]
    }
  }
}

# VPC Endpoints - only when create_vpc = true
resource "aws_vpc_endpoint" "s3" {
  count = var.create_vpc && var.enable_vpc_endpoints ? 1 : 0

  vpc_id            = aws_vpc.main[0].id
  service_name      = "com.amazonaws.${data.aws_region.current.name}.s3"
  vpc_endpoint_type = "Gateway"
  route_table_ids = concat(
    var.enable_private_subnets ? aws_route_table.private[*].id : [],
    var.enable_public_subnets ? [aws_route_table.public[0].id] : []
  )
  policy = data.aws_iam_policy_document.s3_endpoint_policy[0].json

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-s3-endpoint"
  })
}

resource "aws_vpc_endpoint" "ecr_dkr" {
  count = var.create_vpc && var.enable_vpc_endpoints && var.enable_private_subnets ? 1 : 0

  vpc_id              = aws_vpc.main[0].id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.ecr.dkr"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints[0].id]
  private_dns_enabled = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-ecr-dkr-endpoint"
  })
}

resource "aws_vpc_endpoint" "ecr_api" {
  count = var.create_vpc && var.enable_vpc_endpoints && var.enable_private_subnets ? 1 : 0

  vpc_id              = aws_vpc.main[0].id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.ecr.api"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints[0].id]
  private_dns_enabled = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-ecr-api-endpoint"
  })
}

resource "aws_vpc_endpoint" "eks" {
  count = var.create_vpc && var.enable_vpc_endpoints && var.enable_private_subnets ? 1 : 0

  vpc_id              = aws_vpc.main[0].id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.eks"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints[0].id]
  private_dns_enabled = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-eks-endpoint"
  })
}

resource "aws_vpc_endpoint" "ec2" {
  count = var.create_vpc && var.enable_vpc_endpoints && var.enable_private_subnets ? 1 : 0

  vpc_id              = aws_vpc.main[0].id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.ec2"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints[0].id]
  private_dns_enabled = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-ec2-endpoint"
  })
}

resource "aws_vpc_endpoint" "logs" {
  count = var.create_vpc && var.enable_vpc_endpoints && var.enable_private_subnets ? 1 : 0

  vpc_id              = aws_vpc.main[0].id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.logs"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints[0].id]
  private_dns_enabled = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-logs-endpoint"
  })
}

resource "aws_vpc_endpoint" "sts" {
  count = var.create_vpc && var.enable_vpc_endpoints && var.enable_private_subnets ? 1 : 0

  vpc_id              = aws_vpc.main[0].id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.sts"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints[0].id]
  private_dns_enabled = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-sts-endpoint"
  })
}

resource "aws_vpc_endpoint" "elasticloadbalancing" {
  count = var.create_vpc && var.enable_vpc_endpoints && var.enable_private_subnets ? 1 : 0

  vpc_id              = aws_vpc.main[0].id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.elasticloadbalancing"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints[0].id]
  private_dns_enabled = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-elb-endpoint"
  })
}

resource "aws_vpc_endpoint" "autoscaling" {
  count = var.create_vpc && var.enable_vpc_endpoints && var.enable_private_subnets ? 1 : 0

  vpc_id              = aws_vpc.main[0].id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.autoscaling"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints[0].id]
  private_dns_enabled = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-autoscaling-endpoint"
  })
}

resource "aws_vpc_endpoint" "kms" {
  count = var.create_vpc && var.enable_vpc_endpoints && var.enable_private_subnets ? 1 : 0

  vpc_id              = aws_vpc.main[0].id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.kms"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints[0].id]
  private_dns_enabled = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-kms-endpoint"
  })
}

resource "aws_vpc_endpoint" "secretsmanager" {
  count = var.create_vpc && var.enable_vpc_endpoints && var.enable_private_subnets ? 1 : 0

  vpc_id              = aws_vpc.main[0].id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.secretsmanager"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints[0].id]
  private_dns_enabled = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-secretsmanager-endpoint"
  })
}

resource "aws_vpc_endpoint" "ssm" {
  count = var.create_vpc && var.enable_vpc_endpoints && var.enable_private_subnets ? 1 : 0

  vpc_id              = aws_vpc.main[0].id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.ssm"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints[0].id]
  private_dns_enabled = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-ssm-endpoint"
  })
}

resource "aws_vpc_endpoint" "ssmmessages" {
  count = var.create_vpc && var.enable_vpc_endpoints && var.enable_private_subnets ? 1 : 0

  vpc_id              = aws_vpc.main[0].id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.ssmmessages"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints[0].id]
  private_dns_enabled = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-ssmmessages-endpoint"
  })
}

resource "aws_vpc_endpoint" "ec2messages" {
  count = var.create_vpc && var.enable_vpc_endpoints && var.enable_private_subnets ? 1 : 0

  vpc_id              = aws_vpc.main[0].id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.ec2messages"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints[0].id]
  private_dns_enabled = true

  tags = merge(var.tags, {
    Name = "${var.name_prefix}-ec2messages-endpoint"
  })
}
</file>

<file path="modules/aws/README.md">
# AWS Infrastructure Modules

This directory contains modular Terraform configurations for deploying secure AWS infrastructure with EKS, ECS, VPC, and optional bastion host.

## Architecture Overview

The modules provide a highly secure, private-by-default AWS infrastructure:

- **VPC Module**: Creates a VPC with private/public subnets, NAT gateways, and VPC endpoints
- **EKS Module**: Deploys a private EKS cluster with worker nodes in private subnets
- **ECS Module**: Deploys ECS Fargate cluster with secrets management and certificate handling
- **ALB Module**: Application Load Balancer with WAF protection and HTTPS enforcement
- **Bastion Module**: Optional bastion host for secure access to private resources

## Key Security Features

### Private-by-Default
- EKS cluster has **private endpoint access only** (no public access)
- Worker nodes deployed in private subnets only
- No `0.0.0.0/0` access in security groups (except for necessary outbound traffic)

### VPC Endpoints
- Private endpoints for ECR, EKS, CloudWatch, EC2, STS, and S3
- Eliminates need for internet access to AWS APIs
- Reduces data transfer costs and improves security

### Network Security
- Restrictive security groups with principle of least privilege
- Bastion host with controlled SSH access from specific CIDR blocks
- Encrypted EBS volumes and KMS encryption for EKS secrets

## Module Structure

```
modules/aws/
├── main.tf           # Orchestrates all sub-modules
├── variables.tf      # Input variables with validation
├── outputs.tf        # Module outputs
├── vpc/             # VPC and networking resources
├── eks/             # EKS cluster and node groups
├── ecs/             # ECS Fargate cluster and services
├── alb/             # Application Load Balancer
└── bastion/         # Optional bastion host
```

## Usage Examples

### 1. Full Stack (VPC + EKS + Bastion)
```hcl
module "aws_infrastructure" {
  source = "./modules/aws"
  
  cluster_name  = "my-secure-eks"
  region        = "us-west-2"
  environment   = "production"
  
  # Enable all components
  create_vpc     = true
  enable_eks     = true
  enable_bastion = true
  
  # Bastion configuration
  bastion_key_name                = "my-keypair"
  bastion_allowed_ssh_cidr_blocks = ["203.0.113.0/24"]
  
  # EKS configuration
  node_size_config = "medium"
  node_ssh_key_name = "my-keypair"
  
  tags = {
    Environment = "production"
    Project     = "secure-platform"
  }
}
```

### 2. EKS Only (Existing VPC)
```hcl
module "aws_infrastructure" {
  source = "./modules/aws"
  
  cluster_name    = "my-eks"
  region          = "us-west-2"
  environment     = "production"
  
  # Use existing VPC
  create_vpc      = false
  existing_vpc_id = "vpc-0123456789abcdef0"
  
  # Enable only EKS
  enable_eks     = true
  enable_bastion = false
  
  node_size_config = "large"
}
```

### 3. ECS with Application Load Balancer
```hcl
module "aws_infrastructure" {
  source = "./modules/aws"
  
  cluster_name = "my-ecs-app"
  region       = "us-west-2"
  environment  = "production"
  
  # Enable ECS (disable EKS)
  enable_eks = false
  enable_ecs = true
  
  # Container configuration
  ecs_container_image = "my-app:latest"
  ecs_container_port  = 8000
  ecs_desired_count   = 3
  
  # Secrets configuration
  ecs_secrets = {
    database_url = "postgresql://..."
    api_key     = "secret-key"
  }
  
  # Use self-signed certificate (or provide ACM ARN)
  ecs_create_self_signed_cert = true
  ecs_domain_name            = "myapp.example.com"
}
```

### 4. VPC Only (Network Foundation)
```hcl
module "aws_infrastructure" {
  source = "./modules/aws"
  
  cluster_name = "network-foundation"
  region       = "us-west-2"
  environment  = "shared"
  
  # Create VPC only
  create_vpc     = true
  enable_eks     = false
  enable_ecs     = false
  enable_bastion = false
  
  vpc_cidr                 = "10.0.0.0/16"
  availability_zones_count = 3
}
```

## Variables

### Module Control
- `create_vpc`: Whether to create a new VPC (default: true)
- `existing_vpc_id`: ID of existing VPC (required if create_vpc is false)
- `enable_eks`: Whether to create EKS cluster (default: true)
- `enable_ecs`: Whether to create ECS cluster (default: false)
- `enable_bastion`: Whether to create bastion host (default: false)
- `enable_nat_gateway`: Whether to create NAT gateways (default: true)
- `enable_vpc_endpoints`: Whether to create VPC endpoints (default: true)

### VPC Configuration
- `vpc_cidr`: CIDR block for VPC (default: "10.0.0.0/16")
- `availability_zones_count`: Number of AZs to use (default: 2, max: 6)

### EKS Configuration
- `node_size_config`: Node size (small/medium/large, default: small)
- `kubernetes_version`: EKS version (default: "1.28")
- `capacity_type`: ON_DEMAND or SPOT (default: ON_DEMAND)
- `node_ssh_key_name`: EC2 key pair for worker nodes

### ECS Configuration
- `ecs_container_image`: Docker image for application container
- `ecs_container_port`: Port exposed by application (default: 8000)
- `ecs_task_cpu`: CPU units for task (256-4096, default: 512)
- `ecs_task_memory`: Memory in MB (default: 1024)
- `ecs_desired_count`: Number of running tasks (default: 2)
- `ecs_secrets`: Map of secrets for Secrets Manager
- `ecs_acm_certificate_arn`: Existing ACM certificate (optional)
- `ecs_create_self_signed_cert`: Create self-signed cert (default: true)

### Bastion Configuration
- `bastion_key_name`: EC2 key pair for bastion host
- `bastion_allowed_ssh_cidr_blocks`: CIDR blocks allowed SSH access
- `bastion_instance_type`: Instance type (default: t3.micro)

## Outputs

### VPC Outputs
- `vpc_id`: VPC ID
- `private_subnet_ids`: Private subnet IDs
- `public_subnet_ids`: Public subnet IDs

### EKS Outputs
- `cluster_endpoint`: EKS cluster endpoint
- `cluster_name`: EKS cluster name
- `kubeconfig_command`: Command to configure kubectl

### ECS Outputs  
- `ecs_cluster_id`: ECS cluster ID
- `alb_dns_name`: ALB DNS name for accessing application
- `ecs_certificate_arn`: Certificate ARN (provided or self-signed)
- `secrets_endpoint`: Internal endpoint for secrets access

### Bastion Outputs
- `bastion_public_ip`: Bastion host public IP
- `bastion_ssh_command`: SSH command to connect to bastion
- `bastion_session_manager_command`: AWS Session Manager command

### Security Information
- `security_notes`: Important security information about deployment

## Accessing the EKS Cluster

Since the EKS cluster is private-only, you have several options for access:

### Option 1: Bastion Host
1. Enable bastion host: `enable_bastion = true`
2. SSH to bastion: Use output `bastion_ssh_command`
3. Run kubectl from bastion: `./eks-connect.sh`

### Option 2: AWS Session Manager
1. Use output `bastion_session_manager_command`
2. No SSH keys required, uses IAM for authentication

### Option 3: VPN/Direct Connect
1. Set up VPN or Direct Connect to VPC
2. Configure kubectl locally
3. Access cluster directly through private connection

## Security Best Practices

1. **Always use specific CIDR blocks** for bastion SSH access, never `0.0.0.0/0`
2. **Rotate SSH keys regularly** and store them securely
3. **Use AWS Session Manager** instead of SSH when possible
4. **Enable VPC Flow Logs** for network monitoring
5. **Regularly update EKS version** and worker node AMIs
6. **Use IAM roles for service accounts (IRSA)** for pod-level permissions
7. **Enable AWS Config** and **CloudTrail** for compliance monitoring

## Cost Optimization

- Use SPOT instances for non-production workloads (`capacity_type = "SPOT"`)
- Adjust `node_size_config` based on workload requirements
- Consider disabling NAT Gateway if no outbound internet access needed
- Use smaller bastion instance types (`t3.nano` for basic access)

## Troubleshooting

### Common Issues

1. **Cannot access EKS cluster**
   - Ensure bastion host is deployed or VPN is configured
   - Check security group rules
   - Verify kubectl configuration

2. **Worker nodes not joining cluster**
   - Check subnet routing to NAT Gateway
   - Verify VPC endpoints are working
   - Check IAM roles and policies

3. **Bastion host cannot SSH to worker nodes**
   - Ensure `node_ssh_key_name` is set
   - Check security group allows SSH from bastion

4. **High data transfer costs**
   - Ensure VPC endpoints are enabled
   - Check traffic patterns and routing
</file>

<file path=".gitignore">
# Terraform files
*.tfstate
*.tfstate.*

# Terraform directories
.terraform/
.terraform.lock.hcl

# Crash log files
crash.log

# IDE files
.vscode/
.idea/
*.swp
*.swo
*~

# OS files
.DS_Store
Thumbs.db

# Backup files
*.backup

# Local environment files
.env
.env.local
</file>

<file path="examples/aws-eks-only.tfvars">
# Example: EKS cluster only (using existing VPC)
cluster_name = "my-eks-existing-vpc"
region       = "us-west-2"
environment  = "prod"

# Use existing VPC
create_vpc      = false
existing_vpc_id = "vpc-0123456789abcdef0" # Replace with your VPC ID

# Enable only EKS, disable ECS explicitly
enable_eks     = true
enable_ecs     = false
enable_bastion = false

# Networking
enable_nat_gateway   = false # Assuming existing VPC has NAT
enable_vpc_endpoints = true

# EKS Configuration
node_size_config   = "large"
kubernetes_version = "1.28"
capacity_type      = "SPOT"

# Common Configuration
log_retention_in_days = 7

tags = {
  Environment = "prod"
  Project     = "microservices"
  Owner       = "devops-team"
}
</file>

<file path="examples/aws-full-stack.tfvars">
# Example: Full AWS stack with VPC, EKS, and Bastion
cluster_name = "my-secure-eks"
region       = "us-west-2"
environment  = "dev"

# Enable EKS components, disable ECS for isolation
create_vpc     = true
enable_eks     = true
enable_ecs     = false
enable_bastion = true

# VPC Configuration
vpc_cidr                 = "10.0.0.0/16"
availability_zones_count = 2
enable_nat_gateway       = true
enable_vpc_endpoints     = true

# EKS Configuration
node_size_config   = "medium"
kubernetes_version = "1.28"
capacity_type      = "ON_DEMAND"
node_ssh_key_name  = "my-keypair"

# Bastion Configuration
bastion_key_name                = "my-keypair"
bastion_instance_type           = "t3.micro"
bastion_allowed_ssh_cidr_blocks = ["203.0.113.0/24"] # Replace with your IP

# Common Configuration
log_retention_in_days = 14

tags = {
  Environment = "dev"
  Project     = "secure-eks"
  Owner       = "platform-team"
}
</file>

<file path="modules/azure/main.tf">
terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0"
    }
  }
}

# VPC/Network module
module "vpc" {
  source = "./vpc"

  resource_group_name = var.resource_group_name
  location           = var.location
  name_prefix        = var.cluster_name
  vnet_cidr          = var.vnet_cidr
  aks_subnet_cidr    = var.aks_subnet_cidr
  private_subnet_cidr = var.private_subnet_cidr
  bastion_subnet_cidr = var.bastion_subnet_cidr
  enable_nat_gateway  = var.enable_nat_gateway
  enable_bastion      = var.enable_bastion
  create_private_dns_zone = var.create_private_dns_zone
  enable_log_analytics = var.enable_log_analytics
  log_retention_days   = var.log_retention_days
  tags                 = var.tags
}

# AKS module
module "aks" {
  source = "./aks"

  cluster_name                = var.cluster_name
  resource_group_name         = module.vpc.resource_group_name
  location                   = var.location
  kubernetes_version         = var.kubernetes_version
  node_size_config          = var.node_size_config
  subnet_id                 = module.vpc.aks_subnet_id
  private_dns_zone_id       = module.vpc.private_dns_zone_id != null ? module.vpc.private_dns_zone_id : "System"
  network_plugin            = var.network_plugin
  network_policy            = var.network_policy
  dns_service_ip            = var.dns_service_ip
  service_cidr              = var.service_cidr
  key_vault_key_id          = var.key_vault_key_id
  enable_host_encryption    = var.enable_host_encryption
  enable_azure_policy       = var.enable_azure_policy
  enable_microsoft_defender = var.enable_microsoft_defender
  log_analytics_workspace_id = module.vpc.log_analytics_workspace_id
  enable_workload_identity  = var.enable_workload_identity
  container_registry_id     = var.container_registry_id
  workload_node_taints      = var.workload_node_taints
  environment               = var.environment
  tags                      = var.tags
}
</file>

<file path="modules/aws/eks/main.tf">
locals {
  node_size_map = {
    small = {
      instance_types = ["c5.large"]
      desired_size   = 2
      min_size       = 1
      max_size       = 5
      disk_size      = 20
    }
    medium = {
      instance_types = ["c5.xlarge"]
      desired_size   = 3
      min_size       = 2
      max_size       = 10
      disk_size      = 30
    }
    large = {
      instance_types = ["c5.2xlarge"]
      desired_size   = 5
      min_size       = 3
      max_size       = 20
      disk_size      = 50
    }
  }

  node_config = local.node_size_map[var.node_size_config]
}

data "aws_caller_identity" "current" {}

# Security group for EKS cluster control plane
resource "aws_security_group" "cluster" {
  name        = "${var.cluster_name}-cluster-sg"
  description = "Security group for EKS cluster control plane"
  vpc_id      = var.vpc_id

  # Bastion access (if enabled)
  dynamic "ingress" {
    for_each = var.bastion_security_group_id != null ? [1] : []
    content {
      from_port       = 443
      to_port         = 443
      protocol        = "tcp"
      security_groups = [var.bastion_security_group_id]
      description     = "Allow HTTPS from bastion host"
    }
  }

  # Restrict egress to VPC CIDR only
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = [var.vpc_cidr_block]
    description = "Allow all traffic within VPC"
  }

  # Allow HTTPS to VPC endpoints
  egress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr_block]
    description = "Allow HTTPS to VPC endpoints"
  }

  tags = merge(var.tags, {
    Name = "${var.cluster_name}-cluster-sg"
  })
}

# Security group for EKS worker nodes
resource "aws_security_group" "nodes" {
  name        = "${var.cluster_name}-nodes-sg"
  description = "Security group for EKS worker nodes"
  vpc_id      = var.vpc_id

  # Allow all traffic between nodes
  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = "tcp"
    self        = true
    description = "Allow all TCP traffic between worker nodes"
  }

  # Allow traffic from VPC CIDR (including control plane)
  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr_block]
    description = "Allow all TCP traffic from VPC (including control plane)"
  }

  # Allow SSH from bastion if enabled
  dynamic "ingress" {
    for_each = var.bastion_security_group_id != null ? [1] : []
    content {
      from_port       = 22
      to_port         = 22
      protocol        = "tcp"
      security_groups = [var.bastion_security_group_id]
      description     = "Allow SSH from bastion host"
    }
  }

  # Restrict egress to VPC CIDR only
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = [var.vpc_cidr_block]
    description = "Allow all traffic within VPC"
  }

  # Allow HTTPS to VPC endpoints for ECR, etc.
  egress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr_block]
    description = "Allow HTTPS to VPC endpoints"
  }

  tags = merge(var.tags, {
    Name = "${var.cluster_name}-nodes-sg"
  })
}

# IAM role for EKS cluster
resource "aws_iam_role" "cluster" {
  name = "${var.cluster_name}-cluster-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "eks.amazonaws.com"
        }
      }
    ]
  })

  tags = var.tags
}

resource "aws_iam_role_policy_attachment" "cluster_AmazonEKSClusterPolicy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
  role       = aws_iam_role.cluster.name
}

# IAM role for EKS worker nodes
resource "aws_iam_role" "nodes" {
  name = "${var.cluster_name}-nodes-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })

  tags = var.tags
}

resource "aws_iam_role_policy_attachment" "nodes_AmazonEKSWorkerNodePolicy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
  role       = aws_iam_role.nodes.name
}

resource "aws_iam_role_policy_attachment" "nodes_AmazonEKS_CNI_Policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  role       = aws_iam_role.nodes.name
}

resource "aws_iam_role_policy_attachment" "nodes_AmazonEC2ContainerRegistryReadOnly" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
  role       = aws_iam_role.nodes.name
}

# KMS key for EKS cluster encryption - create only if not provided
resource "aws_kms_key" "cluster" {
  count       = var.kms_key_arn == null ? 1 : 0
  description = "EKS Secret Encryption Key for ${var.cluster_name}"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Sid    = "Enable IAM User Permissions"
        Effect = "Allow"
        Principal = {
          AWS = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:root"
        }
        Action   = "kms:*"
        Resource = "*"
      },
      {
        Sid    = "Allow EKS Service"
        Effect = "Allow"
        Principal = {
          Service = "eks.amazonaws.com"
        }
        Action = [
          "kms:Decrypt",
          "kms:GenerateDataKey"
        ]
        Resource = "*"
      }
    ]
  })

  tags = var.tags
}

resource "aws_kms_alias" "cluster" {
  count         = var.kms_key_arn == null ? 1 : 0
  name          = "alias/eks-${var.cluster_name}"
  target_key_id = aws_kms_key.cluster[0].key_id
}

# Local to determine which KMS key to use
locals {
  kms_key_arn = var.kms_key_arn != null ? var.kms_key_arn : aws_kms_key.cluster[0].arn
}

# CloudWatch log group for EKS cluster
resource "aws_cloudwatch_log_group" "cluster" {
  name              = "/aws/eks/${var.cluster_name}/cluster"
  retention_in_days = var.log_retention_in_days
  kms_key_id        = local.kms_key_arn

  tags = var.tags
}

# EKS Cluster - Private endpoint only
resource "aws_eks_cluster" "main" {
  name     = var.cluster_name
  role_arn = aws_iam_role.cluster.arn
  version  = var.kubernetes_version

  vpc_config {
    subnet_ids              = var.private_subnet_ids
    security_group_ids      = [aws_security_group.cluster.id]
    endpoint_private_access = true
    endpoint_public_access  = false # Private only!
  }

  enabled_cluster_log_types = var.enabled_cluster_log_types

  encryption_config {
    provider {
      key_arn = local.kms_key_arn
    }
    resources = ["secrets"]
  }

  tags = var.tags

  depends_on = [
    aws_iam_role_policy_attachment.cluster_AmazonEKSClusterPolicy,
    aws_cloudwatch_log_group.cluster,
  ]
}

# EKS Node Group
resource "aws_eks_node_group" "main" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "${var.cluster_name}-nodes"
  node_role_arn   = aws_iam_role.nodes.arn
  subnet_ids      = var.private_subnet_ids
  instance_types  = local.node_config.instance_types
  disk_size       = local.node_config.disk_size
  ami_type        = var.ami_type
  capacity_type   = var.capacity_type

  scaling_config {
    desired_size = local.node_config.desired_size
    max_size     = local.node_config.max_size
    min_size     = local.node_config.min_size
  }

  update_config {
    max_unavailable = 1
  }

  # Only allow SSH if bastion is provided
  dynamic "remote_access" {
    for_each = var.bastion_security_group_id != null && var.node_ssh_key_name != null ? [1] : []
    content {
      ec2_ssh_key               = var.node_ssh_key_name
      source_security_group_ids = [var.bastion_security_group_id]
    }
  }

  tags = var.tags

  depends_on = [
    aws_iam_role_policy_attachment.nodes_AmazonEKSWorkerNodePolicy,
    aws_iam_role_policy_attachment.nodes_AmazonEKS_CNI_Policy,
    aws_iam_role_policy_attachment.nodes_AmazonEC2ContainerRegistryReadOnly,
  ]
}

# EKS Add-ons
resource "aws_eks_addon" "cni" {
  cluster_name                = aws_eks_cluster.main.name
  addon_name                  = "vpc-cni"
  addon_version               = var.addon_versions.vpc_cni
  resolve_conflicts_on_create = "OVERWRITE"

  tags = var.tags
}

resource "aws_eks_addon" "coredns" {
  cluster_name                = aws_eks_cluster.main.name
  addon_name                  = "coredns"
  addon_version               = var.addon_versions.coredns
  resolve_conflicts_on_create = "OVERWRITE"

  tags = var.tags

  depends_on = [aws_eks_node_group.main]
}

resource "aws_eks_addon" "kube_proxy" {
  cluster_name                = aws_eks_cluster.main.name
  addon_name                  = "kube-proxy"
  addon_version               = var.addon_versions.kube_proxy
  resolve_conflicts_on_create = "OVERWRITE"

  tags = var.tags
}

# Security Group Rule: Allow nodes to communicate with cluster control plane
resource "aws_security_group_rule" "nodes_to_cluster" {
  type                     = "ingress"
  from_port                = 443
  to_port                  = 443
  protocol                 = "tcp"
  source_security_group_id = aws_security_group.nodes.id
  security_group_id        = aws_security_group.cluster.id
  description              = "Allow HTTPS from worker nodes to control plane"
}

resource "aws_eks_addon" "ebs_csi" {
  cluster_name                = aws_eks_cluster.main.name
  addon_name                  = "aws-ebs-csi-driver"
  addon_version               = var.addon_versions.ebs_csi
  resolve_conflicts_on_create = "OVERWRITE"

  tags = var.tags
}
</file>

<file path="modules/aws/outputs.tf">
# VPC Outputs
output "vpc_id" {
  description = "VPC ID"
  value       = local.vpc_id
}

output "vpc_cidr_block" {
  description = "CIDR block of the VPC"
  value       = local.vpc_cidr_block
}

output "private_subnet_ids" {
  description = "IDs of the private subnets"
  value       = local.private_subnet_ids
}

output "public_subnet_ids" {
  description = "IDs of the public subnets"
  value       = local.public_subnet_ids
}

# EKS Outputs
output "cluster_endpoint" {
  description = "EKS cluster endpoint"
  value       = var.enable_eks ? module.eks[0].cluster_endpoint : null
}

output "cluster_ca_certificate" {
  description = "EKS cluster CA certificate"
  value       = var.enable_eks ? module.eks[0].cluster_ca_certificate : null
  sensitive   = true
}

output "cluster_id" {
  description = "EKS cluster ID"
  value       = var.enable_eks ? module.eks[0].cluster_id : null
}

output "cluster_name" {
  description = "EKS cluster name"
  value       = var.enable_eks ? module.eks[0].cluster_name : var.cluster_name
}

output "cluster_arn" {
  description = "EKS cluster ARN"
  value       = var.enable_eks ? module.eks[0].cluster_arn : null
}

output "cluster_version" {
  description = "EKS cluster version"
  value       = var.enable_eks ? module.eks[0].cluster_version : null
}

output "cluster_security_group_id" {
  description = "Security group ID attached to the EKS cluster"
  value       = var.enable_eks ? module.eks[0].cluster_security_group_id : null
}

output "node_security_group_id" {
  description = "Security group ID attached to the EKS worker nodes"
  value       = var.enable_eks ? module.eks[0].node_security_group_id : null
}

output "oidc_issuer_url" {
  description = "The URL on the EKS cluster OIDC Issuer"
  value       = var.enable_eks ? module.eks[0].oidc_issuer_url : null
}

output "kubeconfig_command" {
  description = "Command to get kubeconfig for EKS (requires bastion or VPN access for private cluster)"
  value       = var.enable_eks ? module.eks[0].kubeconfig_command : "EKS cluster not enabled"
}

# Bastion Outputs
output "bastion_instance_id" {
  description = "ID of the bastion host instance"
  value       = var.enable_bastion ? module.bastion[0].bastion_instance_id : null
}

output "bastion_public_ip" {
  description = "Public IP address of the bastion host"
  value       = var.enable_bastion ? module.bastion[0].bastion_public_ip : null
}

output "bastion_private_ip" {
  description = "Private IP address of the bastion host"
  value       = var.enable_bastion ? module.bastion[0].bastion_private_ip : null
}

output "bastion_ssh_command" {
  description = "SSH command to connect to bastion host"
  value       = var.enable_bastion ? module.bastion[0].ssh_command : "Bastion host not enabled"
}

output "bastion_session_manager_command" {
  description = "AWS CLI command to connect via Session Manager"
  value       = var.enable_bastion ? module.bastion[0].session_manager_command : "Bastion host not enabled"
}


# Security Information
output "security_notes" {
  description = "Important security information about the deployment"
  value = {
    eks_private_access_only = var.enable_eks ? "EKS cluster has private endpoint access only" : "EKS not deployed"
    vpc_endpoints_enabled   = var.enable_vpc_endpoints ? "VPC endpoints enabled for private AWS API access" : "VPC endpoints disabled"
    bastion_required        = var.enable_eks && !var.enable_bastion ? "Bastion host or VPN required to access EKS cluster" : "Access configured"
    nat_gateway_enabled     = var.enable_nat_gateway ? "NAT Gateway enabled for outbound internet access" : "No outbound internet access from private subnets"
  }
}
</file>

<file path="modules/aws/variables.tf">
variable "region" {
  description = "AWS region"
  type        = string
}

variable "cluster_name" {
  description = "Name of the EKS cluster"
  type        = string
}

variable "environment" {
  description = "Environment name"
  type        = string
}

# VPC and Networking Variables
variable "vpc_id" {
  description = "VPC ID to use. If not provided, a new VPC will be created"
  type        = string
  default     = ""
}

variable "private_subnet_ids" {
  description = "List of private subnet IDs. If not provided, subnets will be created or discovered"
  type        = list(string)
  default     = []
}

variable "public_subnet_ids" {
  description = "List of public subnet IDs. If not provided, subnets will be created or discovered"
  type        = list(string)
  default     = []
}

variable "enable_eks" {
  description = "Whether to create EKS cluster"
  type        = bool
  default     = true
}


variable "enable_bastion" {
  description = "Whether to create bastion host"
  type        = bool
  default     = false
}

variable "enable_nat_gateway" {
  description = "Whether to create NAT gateways for private subnets"
  type        = bool
  default     = true
}

variable "enable_vpc_endpoints" {
  description = "Whether to create VPC endpoints for AWS services"
  type        = bool
  default     = true
}

# VPC Configuration
variable "vpc_cidr" {
  description = "CIDR block for VPC"
  type        = string
  default     = "10.0.0.0/16"
}

variable "availability_zones_count" {
  description = "Number of availability zones to use"
  type        = number
  default     = 2
  validation {
    condition     = var.availability_zones_count >= 2 && var.availability_zones_count <= 6
    error_message = "Availability zones count must be between 2 and 6."
  }
}

# EKS Configuration
variable "node_size_config" {
  description = "Node size configuration (small, medium, large)"
  type        = string
  default     = "small"
  validation {
    condition     = contains(["small", "medium", "large"], var.node_size_config)
    error_message = "Node size config must be one of: small, medium, large."
  }
}

variable "kubernetes_version" {
  description = "Kubernetes version for EKS cluster"
  type        = string
  default     = "1.28"
}

variable "ami_type" {
  description = "AMI type for EKS worker nodes"
  type        = string
  default     = "AL2_x86_64"
}

variable "capacity_type" {
  description = "Capacity type for EKS worker nodes (ON_DEMAND or SPOT)"
  type        = string
  default     = "ON_DEMAND"
  validation {
    condition     = contains(["ON_DEMAND", "SPOT"], var.capacity_type)
    error_message = "Capacity type must be either ON_DEMAND or SPOT."
  }
}

variable "enabled_cluster_log_types" {
  description = "List of cluster log types to enable"
  type        = list(string)
  default = [
    "api",
    "audit",
    "authenticator",
    "controllerManager",
    "scheduler"
  ]
}

variable "node_ssh_key_name" {
  description = "EC2 Key Pair name for SSH access to worker nodes"
  type        = string
  default     = null
}

variable "addon_versions" {
  description = "Versions for EKS add-ons"
  type = object({
    vpc_cni    = optional(string, null)
    coredns    = optional(string, null)
    kube_proxy = optional(string, null)
    ebs_csi    = optional(string, null)
  })
  default = {}
}

# Bastion Configuration
variable "bastion_key_name" {
  description = "EC2 Key Pair name for SSH access to bastion host"
  type        = string
  default     = null
}

variable "bastion_instance_type" {
  description = "Instance type for bastion host"
  type        = string
  default     = "t3.micro"
}

variable "bastion_allowed_ssh_cidr_blocks" {
  description = "List of CIDR blocks allowed to SSH to bastion host"
  type        = list(string)
  default     = []
  validation {
    condition = length(var.bastion_allowed_ssh_cidr_blocks) == 0 || alltrue([
      for cidr in var.bastion_allowed_ssh_cidr_blocks : can(cidrhost(cidr, 0))
    ])
    error_message = "All provided CIDR blocks must be valid."
  }
}

# Common Configuration
variable "log_retention_in_days" {
  description = "CloudWatch log retention in days"
  type        = number
  default     = 7
}


variable "tags" {
  description = "Tags to apply to resources"
  type        = map(string)
  default     = {}
}
</file>

<file path="main.tf">
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 6.0"
    }
    google = {
      source  = "hashicorp/google"
      version = "~> 5.0"
    }
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
  skip_region_validation = var.cloud_provider != "aws"
}

provider "google" {
  project = var.gcp_project_id
  region  = var.gcp_region
}

provider "azurerm" {
  features {
    resource_group {
      prevent_deletion_if_contains_resources = false
    }
    key_vault {
      purge_soft_delete_on_destroy    = true
      recover_soft_deleted_key_vaults = true
    }
  }
}


# AWS Infrastructure
module "aws_infrastructure" {
  source = "./modules/aws"
  count  = var.cloud_provider == "aws" ? 1 : 0

  region           = var.aws_region
  cluster_name     = var.cluster_name
  environment      = var.environment
  node_size_config = var.node_size_config

  # VPC Configuration
  vpc_id               = var.vpc_id
  private_subnet_ids   = var.private_subnet_ids
  public_subnet_ids    = var.public_subnet_ids
  enable_nat_gateway   = var.enable_nat_gateway
  enable_vpc_endpoints = var.enable_vpc_endpoints
  enable_bastion       = var.enable_bastion

  tags = var.tags
}

# GCP Infrastructure
module "gcp_infrastructure" {
  source = "./modules/gcp"
  count  = var.cloud_provider == "gcp" ? 1 : 0

  # GCP specific variables would go here
  # This module needs to be fully implemented
}

# Azure Infrastructure
module "azure_infrastructure" {
  source = "./modules/azure"
  count  = var.cloud_provider == "azure" ? 1 : 0

  cluster_name         = var.cluster_name
  resource_group_name  = var.azure_resource_group_name
  location            = var.azure_location
  environment         = var.environment
  node_size_config    = var.node_size_config
  kubernetes_version  = var.kubernetes_version
  
  # Encryption
  key_vault_key_id    = var.azure_key_vault_key_id
  
  # Network configuration
  vnet_cidr           = var.azure_vnet_cidr
  enable_nat_gateway  = var.enable_nat_gateway
  enable_bastion      = var.enable_bastion
  
  tags = var.tags
}
</file>

<file path="variables.tf">
variable "cloud_provider" {
  description = "Cloud provider to deploy to (azure, aws, or gcp)"
  type        = string
  validation {
    condition     = contains(["azure", "aws", "gcp"], var.cloud_provider)
    error_message = "Cloud provider must be either 'azure', 'aws', or 'gcp'."
  }
}

variable "cluster_name" {
  description = "Name of the Kubernetes cluster"
  type        = string
}

variable "environment" {
  description = "Environment name (dev, staging, prod, perf)"
  type        = string
}

variable "resource_group_name" {
  description = "Azure resource group name (only used for Azure)"
  type        = string
  default     = ""
}

variable "location" {
  description = "Azure location (only used for Azure)"
  type        = string
  default     = "East US"
}

variable "aws_region" {
  description = "AWS region (only used for AWS)"
  type        = string
  default     = "us-east-1"
}

variable "node_size_config" {
  description = "Node size configuration based on environment"
  type        = string
  validation {
    condition     = contains(["small", "medium", "large"], var.node_size_config)
    error_message = "Node size config must be one of: small, medium, large."
  }
}

variable "tags" {
  description = "Tags to apply to all resources"
  type        = map(string)
  default     = {}
}


# Networking Variables
variable "enable_nat_gateway" {
  description = "Whether to create NAT gateways for private subnets"
  type        = bool
  default     = true
}

variable "enable_vpc_endpoints" {
  description = "Whether to create VPC endpoints for AWS services"
  type        = bool
  default     = true
}

variable "vpc_id" {
  description = "VPC ID to use (AWS only). If not provided, a new VPC will be created"
  type        = string
  default     = ""
}

variable "private_subnet_ids" {
  description = "List of private subnet IDs (AWS only). If not provided, subnets will be created or discovered"
  type        = list(string)
  default     = []
}

variable "public_subnet_ids" {
  description = "List of public subnet IDs (AWS only). If not provided, subnets will be created or discovered"
  type        = list(string)
  default     = []
}

variable "enable_bastion" {
  description = "Whether to create bastion host"
  type        = bool
  default     = false
}

# GCP specific variables
variable "gcp_project_id" {
  description = "GCP Project ID"
  type        = string
  default     = ""
}

variable "gcp_region" {
  description = "GCP region"
  type        = string
  default     = "us-central1"
}

variable "gcp_database_encryption_key_name" {
  description = "Cloud KMS key name for GKE database encryption"
  type        = string
  default     = null
}

# Azure specific variables
variable "azure_resource_group_name" {
  description = "Azure resource group name"
  type        = string
  default     = ""
}

variable "azure_location" {
  description = "Azure location"
  type        = string
  default     = "eastus"
}

variable "azure_key_vault_key_id" {
  description = "Azure Key Vault key ID for AKS encryption"
  type        = string
  default     = null
}

variable "azure_vnet_cidr" {
  description = "Azure VNet CIDR block"
  type        = string
  default     = "10.0.0.0/16"
}

variable "kubernetes_version" {
  description = "Kubernetes version for the cluster"
  type        = string
  default     = "1.28"
}
</file>

<file path="modules/aws/main.tf">
# VPC Module
module "vpc" {
  source = "./vpc"
  count  = var.vpc_id == "" ? 1 : 0

  name_prefix              = var.cluster_name
  vpc_cidr                 = var.vpc_cidr
  availability_zones_count = var.availability_zones_count
  enable_private_subnets   = true
  enable_public_subnets    = var.enable_bastion || var.enable_nat_gateway
  enable_nat_gateway       = var.enable_nat_gateway
  enable_vpc_endpoints     = var.enable_vpc_endpoints

  tags = var.tags
}

# Use existing VPC data if not creating new one
data "aws_vpc" "existing" {
  count = var.vpc_id != "" ? 1 : 0
  id    = var.vpc_id
}

data "aws_subnets" "existing_private" {
  count = var.vpc_id != "" && length(var.private_subnet_ids) == 0 ? 1 : 0

  filter {
    name   = "vpc-id"
    values = [var.vpc_id]
  }

  filter {
    name   = "tag:Type"
    values = ["private"]
  }
}

data "aws_subnets" "existing_public" {
  count = var.vpc_id != "" && length(var.public_subnet_ids) == 0 ? 1 : 0

  filter {
    name   = "vpc-id"
    values = [var.vpc_id]
  }

  filter {
    name   = "tag:Type"
    values = ["public"]
  }
}

# Local values for VPC resources
locals {
  # VPC ID: use provided, or create new
  vpc_id = var.vpc_id != "" ? var.vpc_id : module.vpc[0].vpc_id

  # VPC CIDR: from existing VPC or created VPC
  vpc_cidr_block = var.vpc_id != "" ? data.aws_vpc.existing[0].cidr_block : module.vpc[0].vpc_cidr_block

  # Private subnets: use provided, discover from tags, or create new
  private_subnet_ids = length(var.private_subnet_ids) > 0 ? var.private_subnet_ids : (
    var.vpc_id != "" ? data.aws_subnets.existing_private[0].ids : module.vpc[0].private_subnet_ids
  )

  # Public subnets: use provided, discover from tags, or create new  
  public_subnet_ids = length(var.public_subnet_ids) > 0 ? var.public_subnet_ids : (
    var.vpc_id != "" ? data.aws_subnets.existing_public[0].ids : module.vpc[0].public_subnet_ids
  )
}

# Bastion Host Module (optional)
module "bastion" {
  source = "./bastion"
  count  = var.enable_bastion ? 1 : 0

  name_prefix             = var.cluster_name
  vpc_id                  = local.vpc_id
  vpc_cidr_block          = local.vpc_cidr_block
  public_subnet_id        = local.public_subnet_ids[0]
  cluster_name            = var.cluster_name
  region                  = var.region
  key_name                = var.bastion_key_name
  instance_type           = var.bastion_instance_type
  allowed_ssh_cidr_blocks = var.bastion_allowed_ssh_cidr_blocks
  log_retention_in_days   = var.log_retention_in_days

  tags = var.tags
}

# EKS Cluster Module
module "eks" {
  source = "./eks"
  count  = var.enable_eks ? 1 : 0

  cluster_name              = var.cluster_name
  vpc_id                    = local.vpc_id
  vpc_cidr_block            = local.vpc_cidr_block
  private_subnet_ids        = local.private_subnet_ids
  node_size_config          = var.node_size_config
  kubernetes_version        = var.kubernetes_version
  ami_type                  = var.ami_type
  capacity_type             = var.capacity_type
  enabled_cluster_log_types = var.enabled_cluster_log_types
  log_retention_in_days     = var.log_retention_in_days
  bastion_security_group_id = var.enable_bastion ? module.bastion[0].bastion_security_group_id : null
  node_ssh_key_name         = var.node_ssh_key_name
  addon_versions            = var.addon_versions

  tags = var.tags

  depends_on = [module.vpc, module.bastion]
}
</file>

</files>
